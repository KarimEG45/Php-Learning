{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Rubix ML # A high-level machine learning and deep learning library for the PHP language. Developer-friendly API is delightful to use 40+ supervised and unsupervised learning algorithms Support for ETL, preprocessing, and cross-validation Open Source and free to use commercially What is Rubix ML? # Rubix ML is a free open-source machine learning (ML) library that allows you to build programs that learn from your data using the PHP language. We provide tools for the entire machine learning life cycle from ETL to training, cross-validation, and production with over 40 supervised and unsupervised learning algorithms. In addition, we provide tutorials and other educational content to help you get started using ML in your projects. Getting Started # If you are new to machine learning, we recommend taking a look at the What is Machine Learning? section to get started. If you are already familiar with basic ML concepts, you can browse the basic introduction for a brief look at a typical Rubix ML project. From there, you can browse the official tutorials below which range from beginner to advanced skill level. Tutorials & Example Projects # Check out these example projects using the Rubix ML library. Many come with instructions and a pre-cleaned dataset. CIFAR-10 Image Recognizer Color Clusterer Credit Default Risk Predictor Divorce Predictor DNA Taxonomer Dota 2 Game Outcome Predictor Human Activity Recognizer Housing Price Predictor Iris Flower Classifier MNIST Handwritten Digit Recognizer Text Sentiment Analyzer Interact With The Community # Join Our Telegram Channel Receive Our Newsletter Funding # Rubix ML is funded by donations from the community. You can support this project by making a contribution to one of the core developers listed below. Andrew DalPino Contributing # See CONTRIBUTING.md for guidelines. License # The code is licensed MIT and the documentation is licensed CC BY-NC 4.0 .","title":"Welcome"},{"location":"index.html#rubix-ml","text":"A high-level machine learning and deep learning library for the PHP language. Developer-friendly API is delightful to use 40+ supervised and unsupervised learning algorithms Support for ETL, preprocessing, and cross-validation Open Source and free to use commercially","title":"Rubix ML"},{"location":"index.html#what-is-rubix-ml","text":"Rubix ML is a free open-source machine learning (ML) library that allows you to build programs that learn from your data using the PHP language. We provide tools for the entire machine learning life cycle from ETL to training, cross-validation, and production with over 40 supervised and unsupervised learning algorithms. In addition, we provide tutorials and other educational content to help you get started using ML in your projects.","title":"What is Rubix ML?"},{"location":"index.html#getting-started","text":"If you are new to machine learning, we recommend taking a look at the What is Machine Learning? section to get started. If you are already familiar with basic ML concepts, you can browse the basic introduction for a brief look at a typical Rubix ML project. From there, you can browse the official tutorials below which range from beginner to advanced skill level.","title":"Getting Started"},{"location":"index.html#tutorials-example-projects","text":"Check out these example projects using the Rubix ML library. Many come with instructions and a pre-cleaned dataset. CIFAR-10 Image Recognizer Color Clusterer Credit Default Risk Predictor Divorce Predictor DNA Taxonomer Dota 2 Game Outcome Predictor Human Activity Recognizer Housing Price Predictor Iris Flower Classifier MNIST Handwritten Digit Recognizer Text Sentiment Analyzer","title":"Tutorials &amp; Example Projects"},{"location":"index.html#interact-with-the-community","text":"Join Our Telegram Channel Receive Our Newsletter","title":"Interact With The Community"},{"location":"index.html#funding","text":"Rubix ML is funded by donations from the community. You can support this project by making a contribution to one of the core developers listed below. Andrew DalPino","title":"Funding"},{"location":"index.html#contributing","text":"See CONTRIBUTING.md for guidelines.","title":"Contributing"},{"location":"index.html#license","text":"The code is licensed MIT and the documentation is licensed CC BY-NC 4.0 .","title":"License"},{"location":"basic-introduction.html","text":"Basic Introduction # In this basic introduction to machine learning in Rubix ML, you'll learn how to structure a project, train a learner to predict successful marriages, and then test the model for accuracy. We assume that you already have a basic understanding of the different types of machine learning such as classification and regression. If not, we recommend the section on What is Machine Learning? to start with. Obtaining Data # Machine learning (ML) projects typically begin with a question. For example, you might want to answer the question of \"who of my friends are most likely to stay married to their partner?\" One way to go about answering this question with machine learning would be to go out and ask a bunch of happily married and divorced couples the same set of questions about their partner and then use the answers they gave you to build a model to predict successful relationships. In machine learning terms, the answers you collect are the values of the features that constitute measurements of the phenomena being observed - in this case, the response to a question. The number of features in a sample is called the dimensionality of the sample. For example, a sample with 10 features is said to be 10-dimensional . Suppose that you went out and asked 4 couples (2 married and 2 divorced) to respond to 3 features - their partner's communication skills (between 1 and 5), attractiveness (between 1 and 5), and time spent together per week (hours per week). You would structure the data in PHP like in the example below. You'll notice that the samples are represented in a 2-d array (or matrix ) and the labels are represented as a 1-d array. $samples = [ [ 3 , 4 , 50.5 ], [ 1 , 5 , 24.7 ], [ 4 , 4 , 62.0 ], [ 3 , 2 , 31.1 ], ]; $labels = [ 'married' , 'divorced' , 'married' , 'divorced' ]; Note See the Representing your Data section for an in-depth description of how the library treats various forms of data. The Dataset Object # In Rubix ML, data are passed in specialized containers called Dataset objects . Dataset objects handle selecting, subsampling, splitting, randomizing, and sorting of the samples and labels contained within. In general, there are two types of datasets, Labeled and Unlabeled . Labeled datasets are used for supervised learning and for providing the ground-truth during testing. Unlabeled datasets are used for unsupervised learning and for making predictions. You could construct a Labeled dataset from the data we collected earlier by passing the samples and their labels into the constructor like in the example below. use Rubix\\ML\\Datasets\\Labeled ; $dataset = new Labeled ( $samples , $labels ); Note See the Extracting Data section to learn more about extracting data from different formats and storage mediums. Choosing an Estimator # Estimators make up the core of the Rubix ML library. They provide the predict() API and are responsible for making predictions on unknown samples. Estimators that can be trained with data are called Learners and must be trained before making predictions. In practice, one will experiment with a number of estimators to find the one that works best for their dataset. For our example, we'll focus on an intuitable distance-based supervised learner called K Nearest Neighbors . KNN is a classifier because it takes unknown samples and assigns them a class label. In our example, the output of KNN will either be married or divorced since those are the class labels that we'll train it with. Like most estimators in Rubix, the K Nearest Neighbors classifier requires a set of parameters (called hyper-parameters ) to be chosen up-front by the user. These parameters are defined in the class's constructor and control how the learner behaves during training and inference. Hyper-parameters can be selected based on some prior knowledge or completely at random. The defaults provided are a good place to start for most problems. K Nearest Neighbors works by locating the closest training samples to an unknown sample and choosing the class label that is most common. The hyper-parameter k is the number of nearest points from the training set to compare an unknown sample to in order to infer its class label. For example, if the 3 closest neighbors to a given unknown sample have 2 married and 1 divorced label, then the algorithm will output a prediction of married since its the most common. To instantiate the learner, pass a set of hyper-parameters to the class's constructor. For this example, let's set k to 3 and leave the rest of the hyper-parameters as their default. use Rubix\\ML\\Classifiers\\KNearestNeighbors ; $estimator = new KNearestNeighbors ( 3 ); Note See the Choosing an Estimator section for an in-depth look at the estimators available to you in the library. Training # Training is the process of feeding the learning algorithm data so that it can build an internal representation (or model ) of the task its trying to learn. This representation consists of all of the parameters (except hyper-parameters) that are required to make a prediction. To start training, pass the training dataset as a argument to the train() method on the learner instance. $estimator -> train ( $dataset ); We can verify that the learner has been trained by calling the trained() method. var_dump ( $estimator -> trained ()); bool(true) For our small training set, the training process should only take a matter of microseconds, but larger datasets with more features can take longer. Now that the learner is trained, in the next section we'll show how we can feed in unknown samples to generate predictions. Note See the Training section of the docs for a closer look at training a learner. Making Predictions # Suppose that we went out and collected 4 new data points from different friends using the same questions we asked the couples we interviewed for our training set. We could predict whether or not they will stay married to their spouse by taking their answers and passing them in an Unlabeled dataset to the predict() method on our newly trained estimator. This process of making predictions is called inference because the estimator uses the model constructed during training to infer the label of the unknown samples. Note If you attempt to make predictions using an untrained learner, it will throw an exception. use Rubix\\ML\\Datasets\\Unlabeled ; $samples = [ [ 4 , 3 , 44.2 ], [ 2 , 2 , 16.7 ], [ 2 , 4 , 19.5 ], [ 3 , 3 , 55.0 ], ]; $dataset = new Unlabeled ( $samples ); $predictions = $estimator -> predict ( $dataset ); print_r ( $predictions ); Array ( [ 0 ] => married [ 1 ] => divorced [ 2 ] => divorced [ 3 ] => married ) The output of the estimator are the predicted class labels of the unknown samples. We could either trust these predictions as-is or we could proceed to further evaluate the model. In the next section, we'll learn how to test its accuracy using a process called cross validation. Note Check out the section on Inference for more info on making predictions. Model Evaluation # Let's imagine we went out and collected enough data from our married and divorced friends to build a dataset consisting of 50 samples with their corresponding labels. We could use the entire dataset to train the learner or we could set some of the data aside to use for testing. By setting some data aside we are able to test the model on data it has never seen before. This technique is referred to as cross validation and its goal is to test an estimator's ability to generalize its training. For the purposes of the introduction, we'll use a simple Hold Out validator which takes a portion of the dataset for testing and leaves the rest for training. The Hold Out validator requires the user to set the ratio of testing to training samples as a constructor parameter. Let's choose to use a factor of 0.2 (20%) of the dataset for testing leaving the rest (80%) for training. Note 20% is a good default choice however your mileage may vary. The important thing to note here is the trade off between more data for training and more data to produce better testing results. use Rubix\\ML\\CrossValidation\\HoldOut ; $validator = new HoldOut ( 0.2 ); The test() method on the validator requires a compatible validation Metric to be chosen as the scoring function. One classification metric we could use is the Accuracy metric which is defined as the number of true positives over the total number of predictions. For example, if the estimator returned 10 out of 10 correct predictions then its accuracy would be 1. To return a score from the Hold Out validator using the Accuracy metric, pass an estimator instance along with the samples and their ground-truth labels in a dataset object to the validator like in the example below. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $dataset = new Labeled ( $samples , $labels ); $score = $validator -> test ( $estimator , $dataset , new Accuracy ()); echo $score ; 0.88 The return value is the accuracy score which can be interpreted as the degree to which the learner is able to correctly generalize its training to unseen data. According to the example above, our model is 88% accurate. Nice work! Note More info can be found in the Cross Validation section of the docs. Next Steps # Congratulations! You've completed the basic introduction to machine learning in PHP with Rubix ML. For a more in-depth tutorial using the K Nearest Neighbors classifier and a real dataset, check out the Divorce Predictor tutorial and example project. Have fun!","title":"Basic Introduction"},{"location":"basic-introduction.html#basic-introduction","text":"In this basic introduction to machine learning in Rubix ML, you'll learn how to structure a project, train a learner to predict successful marriages, and then test the model for accuracy. We assume that you already have a basic understanding of the different types of machine learning such as classification and regression. If not, we recommend the section on What is Machine Learning? to start with.","title":"Basic Introduction"},{"location":"basic-introduction.html#obtaining-data","text":"Machine learning (ML) projects typically begin with a question. For example, you might want to answer the question of \"who of my friends are most likely to stay married to their partner?\" One way to go about answering this question with machine learning would be to go out and ask a bunch of happily married and divorced couples the same set of questions about their partner and then use the answers they gave you to build a model to predict successful relationships. In machine learning terms, the answers you collect are the values of the features that constitute measurements of the phenomena being observed - in this case, the response to a question. The number of features in a sample is called the dimensionality of the sample. For example, a sample with 10 features is said to be 10-dimensional . Suppose that you went out and asked 4 couples (2 married and 2 divorced) to respond to 3 features - their partner's communication skills (between 1 and 5), attractiveness (between 1 and 5), and time spent together per week (hours per week). You would structure the data in PHP like in the example below. You'll notice that the samples are represented in a 2-d array (or matrix ) and the labels are represented as a 1-d array. $samples = [ [ 3 , 4 , 50.5 ], [ 1 , 5 , 24.7 ], [ 4 , 4 , 62.0 ], [ 3 , 2 , 31.1 ], ]; $labels = [ 'married' , 'divorced' , 'married' , 'divorced' ]; Note See the Representing your Data section for an in-depth description of how the library treats various forms of data.","title":"Obtaining Data"},{"location":"basic-introduction.html#the-dataset-object","text":"In Rubix ML, data are passed in specialized containers called Dataset objects . Dataset objects handle selecting, subsampling, splitting, randomizing, and sorting of the samples and labels contained within. In general, there are two types of datasets, Labeled and Unlabeled . Labeled datasets are used for supervised learning and for providing the ground-truth during testing. Unlabeled datasets are used for unsupervised learning and for making predictions. You could construct a Labeled dataset from the data we collected earlier by passing the samples and their labels into the constructor like in the example below. use Rubix\\ML\\Datasets\\Labeled ; $dataset = new Labeled ( $samples , $labels ); Note See the Extracting Data section to learn more about extracting data from different formats and storage mediums.","title":"The Dataset Object"},{"location":"basic-introduction.html#choosing-an-estimator","text":"Estimators make up the core of the Rubix ML library. They provide the predict() API and are responsible for making predictions on unknown samples. Estimators that can be trained with data are called Learners and must be trained before making predictions. In practice, one will experiment with a number of estimators to find the one that works best for their dataset. For our example, we'll focus on an intuitable distance-based supervised learner called K Nearest Neighbors . KNN is a classifier because it takes unknown samples and assigns them a class label. In our example, the output of KNN will either be married or divorced since those are the class labels that we'll train it with. Like most estimators in Rubix, the K Nearest Neighbors classifier requires a set of parameters (called hyper-parameters ) to be chosen up-front by the user. These parameters are defined in the class's constructor and control how the learner behaves during training and inference. Hyper-parameters can be selected based on some prior knowledge or completely at random. The defaults provided are a good place to start for most problems. K Nearest Neighbors works by locating the closest training samples to an unknown sample and choosing the class label that is most common. The hyper-parameter k is the number of nearest points from the training set to compare an unknown sample to in order to infer its class label. For example, if the 3 closest neighbors to a given unknown sample have 2 married and 1 divorced label, then the algorithm will output a prediction of married since its the most common. To instantiate the learner, pass a set of hyper-parameters to the class's constructor. For this example, let's set k to 3 and leave the rest of the hyper-parameters as their default. use Rubix\\ML\\Classifiers\\KNearestNeighbors ; $estimator = new KNearestNeighbors ( 3 ); Note See the Choosing an Estimator section for an in-depth look at the estimators available to you in the library.","title":"Choosing an Estimator"},{"location":"basic-introduction.html#training","text":"Training is the process of feeding the learning algorithm data so that it can build an internal representation (or model ) of the task its trying to learn. This representation consists of all of the parameters (except hyper-parameters) that are required to make a prediction. To start training, pass the training dataset as a argument to the train() method on the learner instance. $estimator -> train ( $dataset ); We can verify that the learner has been trained by calling the trained() method. var_dump ( $estimator -> trained ()); bool(true) For our small training set, the training process should only take a matter of microseconds, but larger datasets with more features can take longer. Now that the learner is trained, in the next section we'll show how we can feed in unknown samples to generate predictions. Note See the Training section of the docs for a closer look at training a learner.","title":"Training"},{"location":"basic-introduction.html#making-predictions","text":"Suppose that we went out and collected 4 new data points from different friends using the same questions we asked the couples we interviewed for our training set. We could predict whether or not they will stay married to their spouse by taking their answers and passing them in an Unlabeled dataset to the predict() method on our newly trained estimator. This process of making predictions is called inference because the estimator uses the model constructed during training to infer the label of the unknown samples. Note If you attempt to make predictions using an untrained learner, it will throw an exception. use Rubix\\ML\\Datasets\\Unlabeled ; $samples = [ [ 4 , 3 , 44.2 ], [ 2 , 2 , 16.7 ], [ 2 , 4 , 19.5 ], [ 3 , 3 , 55.0 ], ]; $dataset = new Unlabeled ( $samples ); $predictions = $estimator -> predict ( $dataset ); print_r ( $predictions ); Array ( [ 0 ] => married [ 1 ] => divorced [ 2 ] => divorced [ 3 ] => married ) The output of the estimator are the predicted class labels of the unknown samples. We could either trust these predictions as-is or we could proceed to further evaluate the model. In the next section, we'll learn how to test its accuracy using a process called cross validation. Note Check out the section on Inference for more info on making predictions.","title":"Making Predictions"},{"location":"basic-introduction.html#model-evaluation","text":"Let's imagine we went out and collected enough data from our married and divorced friends to build a dataset consisting of 50 samples with their corresponding labels. We could use the entire dataset to train the learner or we could set some of the data aside to use for testing. By setting some data aside we are able to test the model on data it has never seen before. This technique is referred to as cross validation and its goal is to test an estimator's ability to generalize its training. For the purposes of the introduction, we'll use a simple Hold Out validator which takes a portion of the dataset for testing and leaves the rest for training. The Hold Out validator requires the user to set the ratio of testing to training samples as a constructor parameter. Let's choose to use a factor of 0.2 (20%) of the dataset for testing leaving the rest (80%) for training. Note 20% is a good default choice however your mileage may vary. The important thing to note here is the trade off between more data for training and more data to produce better testing results. use Rubix\\ML\\CrossValidation\\HoldOut ; $validator = new HoldOut ( 0.2 ); The test() method on the validator requires a compatible validation Metric to be chosen as the scoring function. One classification metric we could use is the Accuracy metric which is defined as the number of true positives over the total number of predictions. For example, if the estimator returned 10 out of 10 correct predictions then its accuracy would be 1. To return a score from the Hold Out validator using the Accuracy metric, pass an estimator instance along with the samples and their ground-truth labels in a dataset object to the validator like in the example below. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $dataset = new Labeled ( $samples , $labels ); $score = $validator -> test ( $estimator , $dataset , new Accuracy ()); echo $score ; 0.88 The return value is the accuracy score which can be interpreted as the degree to which the learner is able to correctly generalize its training to unseen data. According to the example above, our model is 88% accurate. Nice work! Note More info can be found in the Cross Validation section of the docs.","title":"Model Evaluation"},{"location":"basic-introduction.html#next-steps","text":"Congratulations! You've completed the basic introduction to machine learning in PHP with Rubix ML. For a more in-depth tutorial using the K Nearest Neighbors classifier and a real dataset, check out the Divorce Predictor tutorial and example project. Have fun!","title":"Next Steps"},{"location":"bootstrap-aggregator.html","text":"[source] Bootstrap Aggregator # Bootstrap Aggregating (or bagging for short) is a model averaging technique designed to improve the stability and performance of a user-specified base estimator by training a number of them on a unique bootstrapped training set sampled at random with replacement. Bagging works especially well with estimators that tend to have high prediction variance by reducing the variance through averaging. Interfaces: Estimator , Learner , Parallel , Persistable Data Type Compatibility: Depends on base learner Parameters # # Name Default Type Description 1 base Learner The base learner. 2 estimators 10 int The number of base learners to train in the ensemble. 3 ratio 0.5 float The ratio of samples from the training set to randomly subsample to train each base learner. Example # use Rubix\\ML\\BootstrapAggregator ; use Rubix\\ML\\Regressors\\RegressionTree ; $estimator = new BootstrapAggregator ( new RegressionTree ( 10 ), 300 , 0.2 ); Additional Methods # This meta estimator does not have any additional methods. References # L. Breiman. (1996). Bagging Predictors. \u21a9","title":"Bootstrap Aggregator"},{"location":"bootstrap-aggregator.html#bootstrap-aggregator","text":"Bootstrap Aggregating (or bagging for short) is a model averaging technique designed to improve the stability and performance of a user-specified base estimator by training a number of them on a unique bootstrapped training set sampled at random with replacement. Bagging works especially well with estimators that tend to have high prediction variance by reducing the variance through averaging. Interfaces: Estimator , Learner , Parallel , Persistable Data Type Compatibility: Depends on base learner","title":"Bootstrap Aggregator"},{"location":"bootstrap-aggregator.html#parameters","text":"# Name Default Type Description 1 base Learner The base learner. 2 estimators 10 int The number of base learners to train in the ensemble. 3 ratio 0.5 float The ratio of samples from the training set to randomly subsample to train each base learner.","title":"Parameters"},{"location":"bootstrap-aggregator.html#example","text":"use Rubix\\ML\\BootstrapAggregator ; use Rubix\\ML\\Regressors\\RegressionTree ; $estimator = new BootstrapAggregator ( new RegressionTree ( 10 ), 300 , 0.2 );","title":"Example"},{"location":"bootstrap-aggregator.html#additional-methods","text":"This meta estimator does not have any additional methods.","title":"Additional Methods"},{"location":"bootstrap-aggregator.html#references","text":"L. Breiman. (1996). Bagging Predictors. \u21a9","title":"References"},{"location":"choosing-an-estimator.html","text":"Choosing an Estimator # Estimators make up the core of the Rubix ML library and include classifiers, regressors, clusterers, anomaly detectors, and meta-estimators organized into their own namespaces. They are responsible for making predictions and are usually trained with data. Most estimators allow tuning by adjusting their user-defined hyper-parameters. Hyper-parameters are arguments to the learning algorithm that effect its behavior during training and inference. The values for the hyper-parameters can be chosen by intuition, tuning , or completely at random. The defaults provided by the library are a good place to start for most problems. To instantiate a new estimator, pass the desired values of the hyper-parameters to the estimator's constructor like in the example below. use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $estimator = new KNearestNeighbors ( 10 , false , new Minkowski ( 2.5 )); Classifiers # Classifiers are supervised learners that predict a categorical class label. They can be used to recognize ( cat , dog , turtle ), differentiate ( spam , not spam ), or describe ( running , walking ) the samples in a dataset based on the labels they were trained on. In addition, classifiers that implement the Probabilistic interface can infer the joint probability distribution of each possible class given an unclassified sample. Name Flexibility Proba Online Ranks Features Verbose Data Compatibility AdaBoost High \u25cf \u25cf Depends on base learner Classification Tree Medium \u25cf \u25cf Categorical, Continuous Extra Tree Classifier Medium \u25cf \u25cf Categorical, Continuous Gaussian Naive Bayes Medium \u25cf \u25cf Continuous K-d Neighbors Medium \u25cf Depends on distance kernel K Nearest Neighbors Medium \u25cf \u25cf Depends on distance kernel Logistic Regression Low \u25cf \u25cf \u25cf \u25cf Continuous Logit Boost High \u25cf \u25cf \u25cf Categorical, Continuous Multilayer Perceptron High \u25cf \u25cf \u25cf Continuous Naive Bayes Medium \u25cf \u25cf Categorical Radius Neighbors Medium \u25cf Depends on distance kernel Random Forest High \u25cf \u25cf Categorical, Continuous Softmax Classifier Low \u25cf \u25cf \u25cf Continuous SVC High Continuous Regressors # Regressors are a type of supervised learner that predict a continuous-valued outcome such as 1.275 or 655 . They can be used to quantify a sample such as its credit score, age, or steering wheel position in units of degrees. Unlike classifiers whose range of predictions is bounded by the number of possible classes in the training set, a regressor's range is unbounded - meaning, the number of possible values a regressor could predict is infinite. Name Flexibility Online Ranks Features Verbose Persistable Data Compatibility Adaline Low \u25cf \u25cf \u25cf \u25cf Continuous Extra Tree Regressor Medium \u25cf \u25cf Categorical, Continuous Gradient Boost High \u25cf \u25cf \u25cf Categorical, Continuous K-d Neighbors Regressor Medium \u25cf Depends on distance kernel KNN Regressor Medium \u25cf \u25cf Depends on distance kernel MLP Regressor High \u25cf \u25cf \u25cf Continuous Radius Neighbors Regressor Medium \u25cf Depends on distance kernerl Regression Tree Medium \u25cf \u25cf Categorical, Continuous Ridge Low \u25cf \u25cf Continuous SVR High Continuous Clusterers # Clusterers are unsupervised learners that predict an integer-valued cluster number such as 0 , 1 , ... , n . They are similar to classifiers, however since they lack a supervised training signal, they cannot be used to recognize or describe samples. Instead, clusterers differentiate and group samples using only the information found within the structure of the samples without their labels. Name Flexibility Proba Online Verbose Persistable Data Compatibility DBSCAN High Depends on distance kernel Fuzzy C Means Low \u25cf \u25cf \u25cf Continuous Gaussian Mixture Medium \u25cf \u25cf \u25cf Continuous K Means Low \u25cf \u25cf \u25cf \u25cf Continuous Mean Shift Medium \u25cf \u25cf \u25cf Continuous Anomaly Detectors # Anomaly Detectors are unsupervised learners that predict whether a sample should be classified as an anomaly or not. We use the value 1 to indicate an outlier and 0 for a regular sample and the predictions can be cast to their boolean equivalent if needed. Anomaly detectors that implement the Scoring interface can output an anomaly score that can be used to sort the samples by their degree of anomalousness. Name Scope Scoring Online Verbose Persistable Data Compatibility Gaussian MLE Global \u25cf \u25cf \u25cf Continuous Isolation Forest Local \u25cf \u25cf Categorical, Continuous Local Outlier Factor Local \u25cf \u25cf Depends on distance kernel Loda Local \u25cf \u25cf \u25cf Continuous One Class SVM Global \u25cf Continuous Robust Z-Score Global \u25cf \u25cf Continuous Model Flexibility Tradeoff # A characteristic of most estimator types is the notion of flexibility . Flexibility can be expressed in different ways but greater flexibility usually comes with the capacity to handle more complex tasks. The tradeoff for flexibility is increased computational complexity, reduced model interpretability, and greater susceptibility to overfitting . In contrast, low flexibility models tend to be easier to interpret and quicker to train but are more prone to underfitting . In general, we recommend choosing the simplest model that does not underfit the training data for your project. No Free Lunch Theorem # At some point you may ask yourself \"Why do we need so many different learning algorithms?\" The answer to that question can be understood by the No Free Lunch Theorem which states that, when averaged over the space of all possible problems, no algorithm performs any better than the next. Perhaps a more useful way of stating NFL is that certain learners perform better at certain tasks and worse in others. This is explained by the fact that all learning algorithms have some prior knowledge inherent in them whether it be via the choice of hyper-parameters or the design of the algorithm itself. Another consequence of No Free Lunch is that there exists no single estimator that performs better for all problems.","title":"Choosing an Estimator"},{"location":"choosing-an-estimator.html#choosing-an-estimator","text":"Estimators make up the core of the Rubix ML library and include classifiers, regressors, clusterers, anomaly detectors, and meta-estimators organized into their own namespaces. They are responsible for making predictions and are usually trained with data. Most estimators allow tuning by adjusting their user-defined hyper-parameters. Hyper-parameters are arguments to the learning algorithm that effect its behavior during training and inference. The values for the hyper-parameters can be chosen by intuition, tuning , or completely at random. The defaults provided by the library are a good place to start for most problems. To instantiate a new estimator, pass the desired values of the hyper-parameters to the estimator's constructor like in the example below. use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $estimator = new KNearestNeighbors ( 10 , false , new Minkowski ( 2.5 ));","title":"Choosing an Estimator"},{"location":"choosing-an-estimator.html#classifiers","text":"Classifiers are supervised learners that predict a categorical class label. They can be used to recognize ( cat , dog , turtle ), differentiate ( spam , not spam ), or describe ( running , walking ) the samples in a dataset based on the labels they were trained on. In addition, classifiers that implement the Probabilistic interface can infer the joint probability distribution of each possible class given an unclassified sample. Name Flexibility Proba Online Ranks Features Verbose Data Compatibility AdaBoost High \u25cf \u25cf Depends on base learner Classification Tree Medium \u25cf \u25cf Categorical, Continuous Extra Tree Classifier Medium \u25cf \u25cf Categorical, Continuous Gaussian Naive Bayes Medium \u25cf \u25cf Continuous K-d Neighbors Medium \u25cf Depends on distance kernel K Nearest Neighbors Medium \u25cf \u25cf Depends on distance kernel Logistic Regression Low \u25cf \u25cf \u25cf \u25cf Continuous Logit Boost High \u25cf \u25cf \u25cf Categorical, Continuous Multilayer Perceptron High \u25cf \u25cf \u25cf Continuous Naive Bayes Medium \u25cf \u25cf Categorical Radius Neighbors Medium \u25cf Depends on distance kernel Random Forest High \u25cf \u25cf Categorical, Continuous Softmax Classifier Low \u25cf \u25cf \u25cf Continuous SVC High Continuous","title":"Classifiers"},{"location":"choosing-an-estimator.html#regressors","text":"Regressors are a type of supervised learner that predict a continuous-valued outcome such as 1.275 or 655 . They can be used to quantify a sample such as its credit score, age, or steering wheel position in units of degrees. Unlike classifiers whose range of predictions is bounded by the number of possible classes in the training set, a regressor's range is unbounded - meaning, the number of possible values a regressor could predict is infinite. Name Flexibility Online Ranks Features Verbose Persistable Data Compatibility Adaline Low \u25cf \u25cf \u25cf \u25cf Continuous Extra Tree Regressor Medium \u25cf \u25cf Categorical, Continuous Gradient Boost High \u25cf \u25cf \u25cf Categorical, Continuous K-d Neighbors Regressor Medium \u25cf Depends on distance kernel KNN Regressor Medium \u25cf \u25cf Depends on distance kernel MLP Regressor High \u25cf \u25cf \u25cf Continuous Radius Neighbors Regressor Medium \u25cf Depends on distance kernerl Regression Tree Medium \u25cf \u25cf Categorical, Continuous Ridge Low \u25cf \u25cf Continuous SVR High Continuous","title":"Regressors"},{"location":"choosing-an-estimator.html#clusterers","text":"Clusterers are unsupervised learners that predict an integer-valued cluster number such as 0 , 1 , ... , n . They are similar to classifiers, however since they lack a supervised training signal, they cannot be used to recognize or describe samples. Instead, clusterers differentiate and group samples using only the information found within the structure of the samples without their labels. Name Flexibility Proba Online Verbose Persistable Data Compatibility DBSCAN High Depends on distance kernel Fuzzy C Means Low \u25cf \u25cf \u25cf Continuous Gaussian Mixture Medium \u25cf \u25cf \u25cf Continuous K Means Low \u25cf \u25cf \u25cf \u25cf Continuous Mean Shift Medium \u25cf \u25cf \u25cf Continuous","title":"Clusterers"},{"location":"choosing-an-estimator.html#anomaly-detectors","text":"Anomaly Detectors are unsupervised learners that predict whether a sample should be classified as an anomaly or not. We use the value 1 to indicate an outlier and 0 for a regular sample and the predictions can be cast to their boolean equivalent if needed. Anomaly detectors that implement the Scoring interface can output an anomaly score that can be used to sort the samples by their degree of anomalousness. Name Scope Scoring Online Verbose Persistable Data Compatibility Gaussian MLE Global \u25cf \u25cf \u25cf Continuous Isolation Forest Local \u25cf \u25cf Categorical, Continuous Local Outlier Factor Local \u25cf \u25cf Depends on distance kernel Loda Local \u25cf \u25cf \u25cf Continuous One Class SVM Global \u25cf Continuous Robust Z-Score Global \u25cf \u25cf Continuous","title":"Anomaly Detectors"},{"location":"choosing-an-estimator.html#model-flexibility-tradeoff","text":"A characteristic of most estimator types is the notion of flexibility . Flexibility can be expressed in different ways but greater flexibility usually comes with the capacity to handle more complex tasks. The tradeoff for flexibility is increased computational complexity, reduced model interpretability, and greater susceptibility to overfitting . In contrast, low flexibility models tend to be easier to interpret and quicker to train but are more prone to underfitting . In general, we recommend choosing the simplest model that does not underfit the training data for your project.","title":"Model Flexibility Tradeoff"},{"location":"choosing-an-estimator.html#no-free-lunch-theorem","text":"At some point you may ask yourself \"Why do we need so many different learning algorithms?\" The answer to that question can be understood by the No Free Lunch Theorem which states that, when averaged over the space of all possible problems, no algorithm performs any better than the next. Perhaps a more useful way of stating NFL is that certain learners perform better at certain tasks and worse in others. This is explained by the fact that all learning algorithms have some prior knowledge inherent in them whether it be via the choice of hyper-parameters or the design of the algorithm itself. Another consequence of No Free Lunch is that there exists no single estimator that performs better for all problems.","title":"No Free Lunch Theorem"},{"location":"committee-machine.html","text":"[source] Committee Machine # A voting ensemble that aggregates the predictions of a committee of heterogeneous learners (referred to as experts ). The committee employs a user-specified influence scheme to weight the final predictions. Note Influence values can be on any arbitrary scale as they are automatically normalized upon instantiation. Interfaces: Estimator , Learner , Parallel , Persistable Data Type Compatibility: Depends on the base learners Parameters # # Name Default Type Description 1 experts array An array of learner instances that will comprise the committee. 2 influences null array The influence values for each expert in the committee. If null, each expert will be weighted equally. Example # use Rubix\\ML\\CommitteeMachine ; use Rubix\\ML\\Classifiers\\GaussianNB ; use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ClassificationTree ; use Rubix\\ML\\Classifiers\\KDNeighbors ; use Rubix\\ML\\Classifiers\\SoftmaxClassifier ; $estimator = new CommitteeMachine ([ new GaussianNB (), new RandomForest ( new ClassificationTree ( 4 ), 100 , 0.3 ), new KDNeighbors ( 3 ), new SoftmaxClassifier ( 100 ), ], [ 0.2 , 0.4 , 0.3 , 0.1 , ]); Additional Methods # Return the learner instances of the committee: public experts () : array Return the normalized influence scores of each expert in the committee: public influences () : array References # H. Drucker. (1997). Fast Committee Machines for Regression and Classification. \u21a9","title":"Committee Machine"},{"location":"committee-machine.html#committee-machine","text":"A voting ensemble that aggregates the predictions of a committee of heterogeneous learners (referred to as experts ). The committee employs a user-specified influence scheme to weight the final predictions. Note Influence values can be on any arbitrary scale as they are automatically normalized upon instantiation. Interfaces: Estimator , Learner , Parallel , Persistable Data Type Compatibility: Depends on the base learners","title":"Committee Machine"},{"location":"committee-machine.html#parameters","text":"# Name Default Type Description 1 experts array An array of learner instances that will comprise the committee. 2 influences null array The influence values for each expert in the committee. If null, each expert will be weighted equally.","title":"Parameters"},{"location":"committee-machine.html#example","text":"use Rubix\\ML\\CommitteeMachine ; use Rubix\\ML\\Classifiers\\GaussianNB ; use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ClassificationTree ; use Rubix\\ML\\Classifiers\\KDNeighbors ; use Rubix\\ML\\Classifiers\\SoftmaxClassifier ; $estimator = new CommitteeMachine ([ new GaussianNB (), new RandomForest ( new ClassificationTree ( 4 ), 100 , 0.3 ), new KDNeighbors ( 3 ), new SoftmaxClassifier ( 100 ), ], [ 0.2 , 0.4 , 0.3 , 0.1 , ]);","title":"Example"},{"location":"committee-machine.html#additional-methods","text":"Return the learner instances of the committee: public experts () : array Return the normalized influence scores of each expert in the committee: public influences () : array","title":"Additional Methods"},{"location":"committee-machine.html#references","text":"H. Drucker. (1997). Fast Committee Machines for Regression and Classification. \u21a9","title":"References"},{"location":"cross-validation.html","text":"Cross Validation # Cross Validation (CV) is a technique for assessing the generalization performance of a model using data it has never seen before. The validation score gives us a sense for how well the model will perform in the real world. In addition, it allows the user to identify problems such as underfitting, overfitting, and selection bias which are discussed in the last section. Creating a Testing Set # For some projects we'll create a dedicated testing set, but in others we can separate some of the samples from our master dataset to be used for testing on the fly. To ensure that both the training and testing sets contain samples that accurately represent the master set we have a number of methods on the Dataset object we can employ. Randomized Split # The first method of creating a training and testing set that works for all datasets is to randomize and then split the dataset into two subsets of varying proportions. In the example below we'll create a training set with 80% of the samples and a testing set with the remaining 20% using the randomize() and split() methods on the Dataset object. [ $training , $testing ] = $dataset -> randomize () -> split ( 0.8 ); You can also use the take() method to extract a testing set while leaving the remaining samples in the training set. $testing = $training -> randomize () -> take ( 1000 ); Stratified Split # If we have a Labeled dataset containing class labels, we can split the dataset in such a way that samples belonging to each class are represented fairly in both sets. This stratified method helps to reduce selection bias by ensuring that each subset remains balanced. [ $training , $testing ] = $dataset -> stratifiedSplit ( 0.8 ); Metrics # Cross validation Metrics are used to score the predictions made by an Estimator with respect to their known ground-truth labels. There are different metrics for different types of problems. To return a validation score from a Metric pass the predictions and labels to the score() method like in the example below. use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $predictions = $estimator -> predict ( $testing ); $metric = new Accuracy (); $score = $metric -> score ( $predictions , $testing -> labels ()); echo $score ; 0.85 Note All metrics follow the schema that higher scores are better - thus, common loss functions such as Mean Squared Error and RMSE are given as their negative to conform to this schema. Classification and Anomaly Detection # Metrics for classification and anomaly detection (a special case of binary classification) compare class predictions to other categorical labels. Their scores are calculated from the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) counts derived from the confusion matrix between the set of predictions and their ground-truth labels. Name Range Formula Notes Accuracy [0, 1] \\(\\frac{TP}{TP + FP}\\) Not suited for imbalanced datasets F Beta [0, 1] \\((1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}\\) Informedness [-1, 1] \\({\\frac {\\text{TP}}{{\\text{TP}}+{\\text{FN}}}}+{\\frac {\\text{TP}}{{\\text{TN}}+{\\text{FP}}}}-1\\) MCC [-1, 1] \\({\\frac {\\mathrm {TP} \\times \\mathrm {TN} -\\mathrm {FP} \\times \\mathrm {FN} }{\\sqrt {(\\mathrm {TP} +\\mathrm {FP} )(\\mathrm {TP} +\\mathrm {FN} )(\\mathrm {TN} +\\mathrm {FP} )(\\mathrm {TN} +\\mathrm {FN} )}}}\\) Probabilistic Classification # Instead of their class predictions, these metrics calculate validation scores from the estimated probabilities of a Probabilistic classifier. Name Range Formula Notes Brier Score [-2, 0] \\(\\frac{1}{n}\\sum\\limits _{i=1}^{n}\\sum\\limits _{j=1}^{p}(Y_{ij}-{\\hat {Y_{ij}}})^2\\) Probabilistic Accuracy [0, 1] Regression # Regression metrics output a score based on the error achieved by comparing continuous-valued predictions and their ground-truth labels. Name Range Formula Notes Mean Absolute Error [-\u221e, 0] \\({\\frac {1}{n}}{\\sum _{i=1}^{n}\\left | Y_{i}-\\hat {Y_{i}}\\right | }\\) Output in same units as predictions Mean Squared Error [-\u221e, 0] \\({\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}\\) Sensitive to outliers Median Absolute Error [-\u221e, 0] \\({\\operatorname {median} (| Y_{i}-{\\tilde {Y}} |)}\\) Robust to outliers R Squared [-\u221e, 1] \\(1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}\\) RMSE [-\u221e, 0] \\({\\sqrt{ \\frac {1}{n} \\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}}}\\) Output in same units as predictions SMAPE [-100, 0] \\({\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)/2}}\\) Clustering # Clustering metrics derive their scores from a contingency table which can be thought of as a confusion matrix where the class names of the predictions are unknown. Name Range Formula Notes Completeness [0, 1] \\(1-\\frac{H(K, C)}{H(K)}\\) Not suited for hyper-parameter tuning Homogeneity [0, 1] \\(1-\\frac{H(C, K)}{H(C)}\\) Not suited for hyper-parameter tuning Rand Index [-1, 1] \\({\\frac {\\left.\\sum _{ij}{\\binom {n_{ij}}{2}}-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}{\\left.{\\frac {1}{2}}\\left[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}\\right]-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}}\\) V Measure [0, 1] \\(\\frac{(1+\\beta)hc}{\\beta h + c}\\) Reports # Cross validation reports give you a deeper sense for how well a particular model performs with fine-grained information. The generate() method on the Report Generator interface takes a set of predictions and their corresponding ground-truth labels and returns a Report object filled with useful statistics that can be printed directly to the terminal or saved to a file. Report Usage Confusion Matrix Classification or Anomaly Detection Contingency Table Clustering Error Analysis Regression Multiclass Breakdown Classification or Anomaly Detection Generating a Report # To generate the report, pass the predictions made by an estimator and their ground-truth labels to the generate() method on the report generator instance. use Rubix\\ML\\CrossValidation\\Reports\\ErrorAnalysis ; $report = new ErrorAnalysis (); $results = $report -> generate ( $predictions , $labels ); Printing a Report # The results of the report are returned in a Report object. Report objects implement the Stringable interface which means they can be cast to strings to output the human-readable form of the report. echo $results ; { \"mean absolute error\" : 0.8 , \"median absolute error\" : 1 , \"mean squared error\" : 1 , \"mean absolute percentage error\" : 14.02077497665733 , \"rms error\" : 1 , \"mean squared log error\" : 0.019107097505647368 , \"r squared\" : 0.9958930551562692 , \"error mean\" : -0.2 , \"error standard deviation\" : 0.9898464007663 , \"error skewness\" : -0.22963966338592326 , \"error kurtosis\" : -1.0520833333333324 , \"error min\" : -2 , \"error 25%\" : -1.0 , \"error median\" : 0.0 , \"error 75%\" : 0.75 , \"error max\" : 1 , \"cardinality\" : 10 } Accessing Report Attributes # You can access individual report attributes by treating the report object as an associative array. $mae = $results [ 'mean absolute error' ]; Saving a Report # Report objects can be cast to JSON encodings which are persistable using a Persister object. To save a report, call the toJSON() method on the report to return an encoding object and then pass a persister to its saveTo() method like in the example below. use Rubix\\ML\\Persisters\\Filesystem ; $results -> toJSON () -> saveTo ( new Filesystem ( 'error.report' )); Validators # Metrics can be used stand-alone or they can be used within a Validator object as the scoring function. Validators automate the cross validation process by training and testing a learner on different subsets of a master dataset. The way in which subsets are chosen depends on the algorithm employed under the hood. Most validators implement the Parallel interface which allows multiple tests to be run at the same time in parallel. Validator Test Coverage Parallel Hold Out Partial K Fold Full \u25cf Leave P Out Full \u25cf Monte Carlo Asymptotically Full \u25cf For example, the K Fold validator automatically selects one of k subsets referred to as a fold as a validation set and then uses the rest of the folds to train the learner. It does this until the learner is trained and tested on every sample in the dataset at least once. The final score is then an average of the k validation scores returned by each test. To begin, pass an untrained Learner , a Labeled dataset, and your chosen validation metric to the validator's test() method. use Rubix\\ML\\CrossValidation\\KFold ; use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $validator = new KFold ( 5 ); $score = $validator -> test ( $estimator , $dataset , new FBeta ()); echo $score ; 0.9175 Common Problems # Poor generalization performance can be explained by one or more of these common problems. Underfitting # A poorly performing model can sometimes be explained as underfitting the training data - a condition in which the learner is unable to capture the underlying pattern or trend given the model constraints. The result of an underfit model is an estimator with high bias error. Underfitting usually occurs when a simple model is chosen to represent data that is complex and non-linear. Adding more features to the dataset can help, however if the problem is too severe, a more flexible learning algorithm can be chosen for the task instead. Overfitting # When a model performs well on training data but poorly during cross-validation it could be that the model has overfit the training data. Overfitting occurs when the model conforms too closely to the training data and therefore fails to generalize well to new data or make predictions reliably. Flexible models are more prone to overfitting due to their ability to memorize individual samples. Most learners employ strategies such as regularization, early stopping, and/or tree pruning to control overfitting, however if overfitting is still a problem, adding more unique samples to the dataset can also help. Selection Bias # When a model performs well on certain samples but poorly on others it could be that the learner was trained with a dataset that exhibits selection bias. Selection bias is the bias introduced when a population is disproportionally represented in a dataset. For example, if a learner is trained to classify pictures of cats and dogs but mostly (say 90%) cats are represented in the dataset, the model will likely have difficulty making real-world predictions where cats and dogs are more equally represented. To correct selection bias, either obtain more unique training samples or up-sample the class of the underrepresented type.","title":"Cross-validation"},{"location":"cross-validation.html#cross-validation","text":"Cross Validation (CV) is a technique for assessing the generalization performance of a model using data it has never seen before. The validation score gives us a sense for how well the model will perform in the real world. In addition, it allows the user to identify problems such as underfitting, overfitting, and selection bias which are discussed in the last section.","title":"Cross Validation"},{"location":"cross-validation.html#creating-a-testing-set","text":"For some projects we'll create a dedicated testing set, but in others we can separate some of the samples from our master dataset to be used for testing on the fly. To ensure that both the training and testing sets contain samples that accurately represent the master set we have a number of methods on the Dataset object we can employ.","title":"Creating a Testing Set"},{"location":"cross-validation.html#randomized-split","text":"The first method of creating a training and testing set that works for all datasets is to randomize and then split the dataset into two subsets of varying proportions. In the example below we'll create a training set with 80% of the samples and a testing set with the remaining 20% using the randomize() and split() methods on the Dataset object. [ $training , $testing ] = $dataset -> randomize () -> split ( 0.8 ); You can also use the take() method to extract a testing set while leaving the remaining samples in the training set. $testing = $training -> randomize () -> take ( 1000 );","title":"Randomized Split"},{"location":"cross-validation.html#stratified-split","text":"If we have a Labeled dataset containing class labels, we can split the dataset in such a way that samples belonging to each class are represented fairly in both sets. This stratified method helps to reduce selection bias by ensuring that each subset remains balanced. [ $training , $testing ] = $dataset -> stratifiedSplit ( 0.8 );","title":"Stratified Split"},{"location":"cross-validation.html#metrics","text":"Cross validation Metrics are used to score the predictions made by an Estimator with respect to their known ground-truth labels. There are different metrics for different types of problems. To return a validation score from a Metric pass the predictions and labels to the score() method like in the example below. use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $predictions = $estimator -> predict ( $testing ); $metric = new Accuracy (); $score = $metric -> score ( $predictions , $testing -> labels ()); echo $score ; 0.85 Note All metrics follow the schema that higher scores are better - thus, common loss functions such as Mean Squared Error and RMSE are given as their negative to conform to this schema.","title":"Metrics"},{"location":"cross-validation.html#classification-and-anomaly-detection","text":"Metrics for classification and anomaly detection (a special case of binary classification) compare class predictions to other categorical labels. Their scores are calculated from the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) counts derived from the confusion matrix between the set of predictions and their ground-truth labels. Name Range Formula Notes Accuracy [0, 1] \\(\\frac{TP}{TP + FP}\\) Not suited for imbalanced datasets F Beta [0, 1] \\((1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}\\) Informedness [-1, 1] \\({\\frac {\\text{TP}}{{\\text{TP}}+{\\text{FN}}}}+{\\frac {\\text{TP}}{{\\text{TN}}+{\\text{FP}}}}-1\\) MCC [-1, 1] \\({\\frac {\\mathrm {TP} \\times \\mathrm {TN} -\\mathrm {FP} \\times \\mathrm {FN} }{\\sqrt {(\\mathrm {TP} +\\mathrm {FP} )(\\mathrm {TP} +\\mathrm {FN} )(\\mathrm {TN} +\\mathrm {FP} )(\\mathrm {TN} +\\mathrm {FN} )}}}\\)","title":"Classification and Anomaly Detection"},{"location":"cross-validation.html#probabilistic-classification","text":"Instead of their class predictions, these metrics calculate validation scores from the estimated probabilities of a Probabilistic classifier. Name Range Formula Notes Brier Score [-2, 0] \\(\\frac{1}{n}\\sum\\limits _{i=1}^{n}\\sum\\limits _{j=1}^{p}(Y_{ij}-{\\hat {Y_{ij}}})^2\\) Probabilistic Accuracy [0, 1]","title":"Probabilistic Classification"},{"location":"cross-validation.html#regression","text":"Regression metrics output a score based on the error achieved by comparing continuous-valued predictions and their ground-truth labels. Name Range Formula Notes Mean Absolute Error [-\u221e, 0] \\({\\frac {1}{n}}{\\sum _{i=1}^{n}\\left | Y_{i}-\\hat {Y_{i}}\\right | }\\) Output in same units as predictions Mean Squared Error [-\u221e, 0] \\({\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}\\) Sensitive to outliers Median Absolute Error [-\u221e, 0] \\({\\operatorname {median} (| Y_{i}-{\\tilde {Y}} |)}\\) Robust to outliers R Squared [-\u221e, 1] \\(1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}\\) RMSE [-\u221e, 0] \\({\\sqrt{ \\frac {1}{n} \\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}}}\\) Output in same units as predictions SMAPE [-100, 0] \\({\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)/2}}\\)","title":"Regression"},{"location":"cross-validation.html#clustering","text":"Clustering metrics derive their scores from a contingency table which can be thought of as a confusion matrix where the class names of the predictions are unknown. Name Range Formula Notes Completeness [0, 1] \\(1-\\frac{H(K, C)}{H(K)}\\) Not suited for hyper-parameter tuning Homogeneity [0, 1] \\(1-\\frac{H(C, K)}{H(C)}\\) Not suited for hyper-parameter tuning Rand Index [-1, 1] \\({\\frac {\\left.\\sum _{ij}{\\binom {n_{ij}}{2}}-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}{\\left.{\\frac {1}{2}}\\left[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}\\right]-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}}\\) V Measure [0, 1] \\(\\frac{(1+\\beta)hc}{\\beta h + c}\\)","title":"Clustering"},{"location":"cross-validation.html#reports","text":"Cross validation reports give you a deeper sense for how well a particular model performs with fine-grained information. The generate() method on the Report Generator interface takes a set of predictions and their corresponding ground-truth labels and returns a Report object filled with useful statistics that can be printed directly to the terminal or saved to a file. Report Usage Confusion Matrix Classification or Anomaly Detection Contingency Table Clustering Error Analysis Regression Multiclass Breakdown Classification or Anomaly Detection","title":"Reports"},{"location":"cross-validation.html#generating-a-report","text":"To generate the report, pass the predictions made by an estimator and their ground-truth labels to the generate() method on the report generator instance. use Rubix\\ML\\CrossValidation\\Reports\\ErrorAnalysis ; $report = new ErrorAnalysis (); $results = $report -> generate ( $predictions , $labels );","title":"Generating a Report"},{"location":"cross-validation.html#printing-a-report","text":"The results of the report are returned in a Report object. Report objects implement the Stringable interface which means they can be cast to strings to output the human-readable form of the report. echo $results ; { \"mean absolute error\" : 0.8 , \"median absolute error\" : 1 , \"mean squared error\" : 1 , \"mean absolute percentage error\" : 14.02077497665733 , \"rms error\" : 1 , \"mean squared log error\" : 0.019107097505647368 , \"r squared\" : 0.9958930551562692 , \"error mean\" : -0.2 , \"error standard deviation\" : 0.9898464007663 , \"error skewness\" : -0.22963966338592326 , \"error kurtosis\" : -1.0520833333333324 , \"error min\" : -2 , \"error 25%\" : -1.0 , \"error median\" : 0.0 , \"error 75%\" : 0.75 , \"error max\" : 1 , \"cardinality\" : 10 }","title":"Printing a Report"},{"location":"cross-validation.html#accessing-report-attributes","text":"You can access individual report attributes by treating the report object as an associative array. $mae = $results [ 'mean absolute error' ];","title":"Accessing Report Attributes"},{"location":"cross-validation.html#saving-a-report","text":"Report objects can be cast to JSON encodings which are persistable using a Persister object. To save a report, call the toJSON() method on the report to return an encoding object and then pass a persister to its saveTo() method like in the example below. use Rubix\\ML\\Persisters\\Filesystem ; $results -> toJSON () -> saveTo ( new Filesystem ( 'error.report' ));","title":"Saving a Report"},{"location":"cross-validation.html#validators","text":"Metrics can be used stand-alone or they can be used within a Validator object as the scoring function. Validators automate the cross validation process by training and testing a learner on different subsets of a master dataset. The way in which subsets are chosen depends on the algorithm employed under the hood. Most validators implement the Parallel interface which allows multiple tests to be run at the same time in parallel. Validator Test Coverage Parallel Hold Out Partial K Fold Full \u25cf Leave P Out Full \u25cf Monte Carlo Asymptotically Full \u25cf For example, the K Fold validator automatically selects one of k subsets referred to as a fold as a validation set and then uses the rest of the folds to train the learner. It does this until the learner is trained and tested on every sample in the dataset at least once. The final score is then an average of the k validation scores returned by each test. To begin, pass an untrained Learner , a Labeled dataset, and your chosen validation metric to the validator's test() method. use Rubix\\ML\\CrossValidation\\KFold ; use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $validator = new KFold ( 5 ); $score = $validator -> test ( $estimator , $dataset , new FBeta ()); echo $score ; 0.9175","title":"Validators"},{"location":"cross-validation.html#common-problems","text":"Poor generalization performance can be explained by one or more of these common problems.","title":"Common Problems"},{"location":"cross-validation.html#underfitting","text":"A poorly performing model can sometimes be explained as underfitting the training data - a condition in which the learner is unable to capture the underlying pattern or trend given the model constraints. The result of an underfit model is an estimator with high bias error. Underfitting usually occurs when a simple model is chosen to represent data that is complex and non-linear. Adding more features to the dataset can help, however if the problem is too severe, a more flexible learning algorithm can be chosen for the task instead.","title":"Underfitting"},{"location":"cross-validation.html#overfitting","text":"When a model performs well on training data but poorly during cross-validation it could be that the model has overfit the training data. Overfitting occurs when the model conforms too closely to the training data and therefore fails to generalize well to new data or make predictions reliably. Flexible models are more prone to overfitting due to their ability to memorize individual samples. Most learners employ strategies such as regularization, early stopping, and/or tree pruning to control overfitting, however if overfitting is still a problem, adding more unique samples to the dataset can also help.","title":"Overfitting"},{"location":"cross-validation.html#selection-bias","text":"When a model performs well on certain samples but poorly on others it could be that the learner was trained with a dataset that exhibits selection bias. Selection bias is the bias introduced when a population is disproportionally represented in a dataset. For example, if a learner is trained to classify pictures of cats and dogs but mostly (say 90%) cats are represented in the dataset, the model will likely have difficulty making real-world predictions where cats and dogs are more equally represented. To correct selection bias, either obtain more unique training samples or up-sample the class of the underrepresented type.","title":"Selection Bias"},{"location":"estimator.html","text":"Estimator # The Estimator interface is implemented by all learners in Rubix ML. It provides basic inference functionality through the predict() method which returns a set of predictions from a dataset. Additionally, it provides methods for returning estimator type and data type compatibility declarations. Make Predictions # Return the predictions from a dataset containing unknown samples in an array: public predict ( Dataset $dataset ) : array $predictions = $estimator -> predict ( $dataset ); print_r ( $predictions ); Array ( [ 0 ] => married [ 1 ] => divorced [ 2 ] => divorced [ 3 ] => married ) Note The return value of predict() is an array containing the predictions in the same order that they were indexed in the dataset.","title":"Estimator"},{"location":"estimator.html#estimator","text":"The Estimator interface is implemented by all learners in Rubix ML. It provides basic inference functionality through the predict() method which returns a set of predictions from a dataset. Additionally, it provides methods for returning estimator type and data type compatibility declarations.","title":"Estimator"},{"location":"estimator.html#make-predictions","text":"Return the predictions from a dataset containing unknown samples in an array: public predict ( Dataset $dataset ) : array $predictions = $estimator -> predict ( $dataset ); print_r ( $predictions ); Array ( [ 0 ] => married [ 1 ] => divorced [ 2 ] => divorced [ 3 ] => married ) Note The return value of predict() is an array containing the predictions in the same order that they were indexed in the dataset.","title":"Make Predictions"},{"location":"exploring-data.html","text":"Exploring Data # Exploratory Data Analysis (EDA) is an approach to modeling data that produces insights into the characteristics of a dataset. EDA is useful for feature engineering as well as model selection and can save time and lead to better modes when included in your machine learning lifecycle. In general, there are two types of Exploratory Data Analysis - quantitative and graphical. Quantitative data analysis summarizes the data using statistical or probabilistic methods. Graphical analysis uses techniques such as scatterplots and histograms to glean information from the structure and shape of the data and can incorporate Manifold Learning to reduce the dimensionality of the samples. Describe a Dataset # The Dataset API has a handy method named describe() that computes statistics for each continuous feature of the dataset such as the column median, standard deviation, and skewness. In addition, it provides the probabilities of each category for categorical feature columns. In the example below, we'll echo the Report object returned by the describe() method to get a better understanding for how the values of our features are distributed. $report = $dataset -> describe (); echo $report ; [ { \"offset\" : 0 , \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"friendly\" : 0.6666666666666666 , \"loner\" : 0.3333333333333333 } }, { \"offset\" : 1 , \"type\" : \"continuous\" , \"mean\" : 0.3333333333333333 , \"standard deviation\" : 3.129252661934191 , \"skewness\" : -0.4481030843690633 , \"kurtosis\" : -1.1330702741786107 , \"min\" : -5 , \"25%\" : -1.375 , \"median\" : 0.8 , \"75%\" : 2.825 , \"max\" : 4 } ] We can also save the report by passing a Persister to the saveTo() method on the Encoding object returned by calling toJSON() on the Report object. use Rubix\\ML\\Persisters\\Filesystem ; $report -> toJSON () -> saveTo ( new Filesystem ( 'report.json' )); Describe by Label # You can also describe the dataset in terms of the classes each sample belongs to by calling the describeByLabel() method on a Labeled dataset object with categorical labels. $report = $dataset -> describeByLabel (); Visualization # Another technique used in data analysis is plotting one or more of its dimensions in a chart such as a scatterplot or histogram. Visualizing the data gives us an understanding as to the shape of the data and can aid in discovering outliers or for choosing features to train our model with. Since the library works with common data formats, you are free to use your favorite 3rd party plotting software to visualize the data copied from Rubix ML. If you are looking for a place to start, the free Plotly online Chart Studio or a modern spreadsheet application should work well for most visualization tasks. Exporting Data # Before importing a dataset into your plotting software, you may need to export it in a format that can be recognized. For this, the library provides the Writable Extractor API to handle exporting dataset objects to various formats including CSV and NDJSON . For example, to export a dataset in CSV format pass the CSV extractor to the exportTo() method on the dataset object. use Rubix\\ML\\Extractors\\CSV ; $dataset -> exportTo ( new CSV ( 'dataset.csv' )); Converting Formats # You may want to convert a dataset stored in one format to another format. To convert formats, pass an extractor object to the export() method on a target extractor that implements the Writable interface. In the example below, we'll convert a data table from CSV format to NDJSON, saving it to a new file. use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new NDJSON ( 'dataset.ndjson' ); $extractor -> export ( new CSV ( 'dataset.csv' )); 1D Histogram # One way to visualize the categorical features of a dataset is to put each sample into a bin corresponding to the particular category it belongs to. We can then count the number of samples and display them in a histogram so they can be visually compared. In the following example, we'll bin the samples of the Housing dataset according to building type. 2D Scatterplot # A common way to visualize the continuous features of a dataset is to plot two features as X and Y axis of a scatterplot. In the example below, we'll plot the petal width and petal length features of the Iris dataset. Notice that we can distinguish 3 clusters corresponding to each class label - therefore, these features will do a pretty good job of informing the learner at training time. Manifold Learning # Manifold Learning is a type of dimensionality reduction that aims to produce a faithful low-dimensional (1 - 3) representation of a whole dataset for visualization. Unlike the example above in which we isolated a fixed number of features, Manifold Learning allows us to plot a representation of all the features. This representation is referred to as an embedding because the high-dimensional features are embedded into a lower-dimensional manifold. In the first example, we'll use a dimensionality reduction method called Truncated SVD to project the Iris dataset down into 2 dimensions and then export the data to a CSV file using the exportTo() method so we can import it into our plotting software. use Rubix\\ML\\Transformers\\TruncatedSVD ; use Rubix\\ML\\Extractors\\CSV ; $dataset -> apply ( new TruncatedSVD ( 2 )) -> exportTo ( new CSV ( 'embedding.csv' )); When we visualize the embedding, again we see the formation of clusters, however, notice that the X and Y axis no longer correspond to individual features but rather to arbitrary axis of the 2-dimensional embedding of all the features. Another algorithm often used for manifold learning is T-distributed Stochastic Neighbor Embedding or t-SNE. Unlike Truncated SVD which is a linear dimensionality reducer, t-SNE is able to find non-linear manifolds of the dataset and therefore can sometimes produce more faithful representations of the data in low dimensions. In the example below, we'll use the t-SNE transformer to embed the 4-dimensional Iris dataset into 2 dimensions so we can visualize it. use Rubix\\ML\\Transformers\\TSNE ; use Rubix\\ML\\Extractors\\CSV ; $dataset -> apply ( new TSNE ( 2 , 100.0 , 10.0 )) -> exportTo ( new CSV ( 'embedding.csv' )); Here is what a t-SNE embedding looks like when it is plotted. Notice that although the clusters are sparser and more gaussian-like, the structure and distances between samples is roughly preserved.","title":"Exploring Data"},{"location":"exploring-data.html#exploring-data","text":"Exploratory Data Analysis (EDA) is an approach to modeling data that produces insights into the characteristics of a dataset. EDA is useful for feature engineering as well as model selection and can save time and lead to better modes when included in your machine learning lifecycle. In general, there are two types of Exploratory Data Analysis - quantitative and graphical. Quantitative data analysis summarizes the data using statistical or probabilistic methods. Graphical analysis uses techniques such as scatterplots and histograms to glean information from the structure and shape of the data and can incorporate Manifold Learning to reduce the dimensionality of the samples.","title":"Exploring Data"},{"location":"exploring-data.html#describe-a-dataset","text":"The Dataset API has a handy method named describe() that computes statistics for each continuous feature of the dataset such as the column median, standard deviation, and skewness. In addition, it provides the probabilities of each category for categorical feature columns. In the example below, we'll echo the Report object returned by the describe() method to get a better understanding for how the values of our features are distributed. $report = $dataset -> describe (); echo $report ; [ { \"offset\" : 0 , \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"friendly\" : 0.6666666666666666 , \"loner\" : 0.3333333333333333 } }, { \"offset\" : 1 , \"type\" : \"continuous\" , \"mean\" : 0.3333333333333333 , \"standard deviation\" : 3.129252661934191 , \"skewness\" : -0.4481030843690633 , \"kurtosis\" : -1.1330702741786107 , \"min\" : -5 , \"25%\" : -1.375 , \"median\" : 0.8 , \"75%\" : 2.825 , \"max\" : 4 } ] We can also save the report by passing a Persister to the saveTo() method on the Encoding object returned by calling toJSON() on the Report object. use Rubix\\ML\\Persisters\\Filesystem ; $report -> toJSON () -> saveTo ( new Filesystem ( 'report.json' ));","title":"Describe a Dataset"},{"location":"exploring-data.html#describe-by-label","text":"You can also describe the dataset in terms of the classes each sample belongs to by calling the describeByLabel() method on a Labeled dataset object with categorical labels. $report = $dataset -> describeByLabel ();","title":"Describe by Label"},{"location":"exploring-data.html#visualization","text":"Another technique used in data analysis is plotting one or more of its dimensions in a chart such as a scatterplot or histogram. Visualizing the data gives us an understanding as to the shape of the data and can aid in discovering outliers or for choosing features to train our model with. Since the library works with common data formats, you are free to use your favorite 3rd party plotting software to visualize the data copied from Rubix ML. If you are looking for a place to start, the free Plotly online Chart Studio or a modern spreadsheet application should work well for most visualization tasks.","title":"Visualization"},{"location":"exploring-data.html#exporting-data","text":"Before importing a dataset into your plotting software, you may need to export it in a format that can be recognized. For this, the library provides the Writable Extractor API to handle exporting dataset objects to various formats including CSV and NDJSON . For example, to export a dataset in CSV format pass the CSV extractor to the exportTo() method on the dataset object. use Rubix\\ML\\Extractors\\CSV ; $dataset -> exportTo ( new CSV ( 'dataset.csv' ));","title":"Exporting Data"},{"location":"exploring-data.html#converting-formats","text":"You may want to convert a dataset stored in one format to another format. To convert formats, pass an extractor object to the export() method on a target extractor that implements the Writable interface. In the example below, we'll convert a data table from CSV format to NDJSON, saving it to a new file. use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new NDJSON ( 'dataset.ndjson' ); $extractor -> export ( new CSV ( 'dataset.csv' ));","title":"Converting Formats"},{"location":"exploring-data.html#1d-histogram","text":"One way to visualize the categorical features of a dataset is to put each sample into a bin corresponding to the particular category it belongs to. We can then count the number of samples and display them in a histogram so they can be visually compared. In the following example, we'll bin the samples of the Housing dataset according to building type.","title":"1D Histogram"},{"location":"exploring-data.html#2d-scatterplot","text":"A common way to visualize the continuous features of a dataset is to plot two features as X and Y axis of a scatterplot. In the example below, we'll plot the petal width and petal length features of the Iris dataset. Notice that we can distinguish 3 clusters corresponding to each class label - therefore, these features will do a pretty good job of informing the learner at training time.","title":"2D Scatterplot"},{"location":"exploring-data.html#manifold-learning","text":"Manifold Learning is a type of dimensionality reduction that aims to produce a faithful low-dimensional (1 - 3) representation of a whole dataset for visualization. Unlike the example above in which we isolated a fixed number of features, Manifold Learning allows us to plot a representation of all the features. This representation is referred to as an embedding because the high-dimensional features are embedded into a lower-dimensional manifold. In the first example, we'll use a dimensionality reduction method called Truncated SVD to project the Iris dataset down into 2 dimensions and then export the data to a CSV file using the exportTo() method so we can import it into our plotting software. use Rubix\\ML\\Transformers\\TruncatedSVD ; use Rubix\\ML\\Extractors\\CSV ; $dataset -> apply ( new TruncatedSVD ( 2 )) -> exportTo ( new CSV ( 'embedding.csv' )); When we visualize the embedding, again we see the formation of clusters, however, notice that the X and Y axis no longer correspond to individual features but rather to arbitrary axis of the 2-dimensional embedding of all the features. Another algorithm often used for manifold learning is T-distributed Stochastic Neighbor Embedding or t-SNE. Unlike Truncated SVD which is a linear dimensionality reducer, t-SNE is able to find non-linear manifolds of the dataset and therefore can sometimes produce more faithful representations of the data in low dimensions. In the example below, we'll use the t-SNE transformer to embed the 4-dimensional Iris dataset into 2 dimensions so we can visualize it. use Rubix\\ML\\Transformers\\TSNE ; use Rubix\\ML\\Extractors\\CSV ; $dataset -> apply ( new TSNE ( 2 , 100.0 , 10.0 )) -> exportTo ( new CSV ( 'embedding.csv' )); Here is what a t-SNE embedding looks like when it is plotted. Notice that although the clusters are sparser and more gaussian-like, the structure and distances between samples is roughly preserved.","title":"Manifold Learning"},{"location":"extracting-data.html","text":"Extracting Data # There are a number of ways to instantiate a new Dataset object, but all of them require the data to be loaded into memory first. Some common formats you'll find data in are structured plain-text such as CSV or NDJSON, or in a queryable database such as MySQL or MongoDB. No matter how your data are stored, you have the freedom and flexibility to implement the data source to fit your current infrastructure. To help make extraction simple for more common use cases, the library provides a number of Extractor objects. Extractors are iterators that let you loop over the records of a dataset in storage and can be used to instantiate a dataset object using the fromIterator() method. CSV # A common plain-text format for small to medium-sized datasets is comma-separated values or CSV for short. A CSV file contains a table with individual samples indicated by rows and the values of the features stored in each column. Columns are separated by a delimiter such as the , or ; character and may be enclosed on both ends with an optional enclosure such as \" . The file can sometimes contain a header as the first row. CSV files have the advantage of being able to be processed line by line, however, their disadvantage is that type information cannot be inferred from the format. Thus, all CSV data are imported as categorical type (strings) by default. Example attitude,texture,sociability,rating,class nice,furry,friendly,4,not monster mean,furry,loner,-1.5,monster The library provides the CSV Extractor to help import data from the CSV format. We'll use it in conjunction with the fromIterator() method to instantiate a new dataset object. In the example below, In addition, we'll apply the Numeric String Converter to the newly instantiated dataset object to convert the numeric data to the proper format immediately after instantiation. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Extractors\\CSV ; use Rubix\\ML\\Transformers\\NumericStringConverter ; $dataset = Labeled :: fromIterator ( new CSV ( 'example.csv' , true )) -> apply ( new NumericStringConverter ()); We can check the number of records that were imported by calling the numSamples() method on the dataset object. echo $dataset -> numSamples (); 5000 NDJSON # Another plain-text format called NDJSON or Newline Delimited Javascript Object Notation (JSON) can be considered a hybrid of both CSV and JSON. It contains rows of JSON arrays or objects delineated by a newline character ( \\n or \\r\\n ). It has the advantage of retaining type information like JSON and can also be read into memory efficiently like CSV. Example { \"attitude\" : \"nice\" , \"texture\" : \"furry\" , \"sociability\" : \"friendly\" , \"rating\" : 4 , \"class\" : \"not monster\" } { \"attitude\" : \"mean\" , \"texture\" : \"furry\" , \"sociability\" : \"loner\" , \"rating\" : -1.5 , \"class\" : \"monster\" } { \"attitude\" : \"nice\" , \"texture\" : \"rough\" , \"sociability\" : \"friendly\" , \"rating\" : 2.6 , \"class\" : \"not monster\" } The NDJSON extractor can be used to instantiate a new dataset object from a NDJSON file. Optionally, it can be combined with the standard PHP library's Limit Iterator to only load a portion of the data into memory. In the example below, we load the first 1,000 rows of data from an NDJSON file into an Unlabeled dataset. use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Datasets\\Unlabeled ; use LimitIterator ; $extractor = new NDJSON ( 'example.ndjson' ); $iterator = new LimitIterator ( $extractor -> getIterator (), 0 , 1000 ); $dataset = Unlabeled :: fromIterator ( $iterator ); SQL # Medium to large datasets will often be stored in an RDBMS (relational database management system) such as MySQL , PostgreSQL or Sqlite . Relational databases allow you to query large amounts of data on-the-fly and can be very flexible. PHP comes with robust relational database support through its PDO interface. To iterate over the rows of an SQL table we provide an SQL Table extractor uses the PDO interface under the hood. In the example below we'll wrap our SQL Table extractor in a Column Picker to instantiate a new Unlabeled dataset object from a particular set of columns of the table. use Rubix\\ML\\Extractors\\SQLTable ; use Rubix\\ML\\Extractors\\ColumnPicker ; use Rubix\\ML\\Datasets\\Unlabeled ; use PDO ; $connection = new PDO ( 'sqlite:/example.sqlite' ); $extractor = new ColumnPicker ( new SQLTable ( $connection , 'patients' ), [ 'age' , 'gender' , 'height' , 'diagnosis' , ]); $dataset = Unlabeled :: fromIterator ( $extractor ); If you need more control over your data pipeline then we recommend writing your own custom queries. The following example uses the PDO interface to execute a user-defined SQL query and instantiate a dataset object containing the same data as the example above. However, this method may be more efficient because it avoids querying for more data than you need. use Rubix\\ML\\Datasets\\Unlabeled ; use PDO ; $pdo = new PDO ( 'sqlite:/example.sqlite' ); $query = $pdo -> prepare ( 'SELECT age, gender, height, diagnosis FROM patients' ); $query -> execute (); $samples = $query -> fetchAll ( PDO :: FETCH_NUM ); $dataset = new Unlabeled ( $samples ); Images # PHP offers a number of functions to import images as PHP resources such as imagecreatefromjpeg() and imagecreatefrompng() that come with the GD extension. The example below imports the .png images in the train folder and labels them using part of their filename. The samples and labels are then put into a Labeled dataset using the build() factory method and then converted into raw color channel data by applying the Image Vectorizer . use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Transformers\\ImageVectorizer ; $samples = $labels = []; foreach ( glob ( 'train/*.png' ) as $file ) { $samples [] = [ imagecreatefrompng ( $file )]; $labels [] = preg_replace ( '/[0-9]+_(.*).png/' , '$1' , basename ( $file )); } $dataset = Labeled :: build ( $samples , $labels ) -> apply ( new ImageVectorizer ()); Synthetic Datasets # Synthetic datasets are those that can be generated by one or more predefined formulas. In Rubix ML, we can generate synthetic datasets using Generator objects. Generators are useful in educational settings and for supplementing a small dataset with more samples. To generate a labeled dataset using the Half Moon generator pass the number of records you wish to generate to the generate() method. use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; $generator = new HalfMoon (); $dataset = $generator -> generate ( 1000 ); Now we can write the dataset to a CSV file and import it into our favorite plotting software. use Rubix\\ML\\Extractors\\CSV ; $dataset -> exportTo ( new CSV ( 'half-moon.csv' ));","title":"Extracting Data"},{"location":"extracting-data.html#extracting-data","text":"There are a number of ways to instantiate a new Dataset object, but all of them require the data to be loaded into memory first. Some common formats you'll find data in are structured plain-text such as CSV or NDJSON, or in a queryable database such as MySQL or MongoDB. No matter how your data are stored, you have the freedom and flexibility to implement the data source to fit your current infrastructure. To help make extraction simple for more common use cases, the library provides a number of Extractor objects. Extractors are iterators that let you loop over the records of a dataset in storage and can be used to instantiate a dataset object using the fromIterator() method.","title":"Extracting Data"},{"location":"extracting-data.html#csv","text":"A common plain-text format for small to medium-sized datasets is comma-separated values or CSV for short. A CSV file contains a table with individual samples indicated by rows and the values of the features stored in each column. Columns are separated by a delimiter such as the , or ; character and may be enclosed on both ends with an optional enclosure such as \" . The file can sometimes contain a header as the first row. CSV files have the advantage of being able to be processed line by line, however, their disadvantage is that type information cannot be inferred from the format. Thus, all CSV data are imported as categorical type (strings) by default. Example attitude,texture,sociability,rating,class nice,furry,friendly,4,not monster mean,furry,loner,-1.5,monster The library provides the CSV Extractor to help import data from the CSV format. We'll use it in conjunction with the fromIterator() method to instantiate a new dataset object. In the example below, In addition, we'll apply the Numeric String Converter to the newly instantiated dataset object to convert the numeric data to the proper format immediately after instantiation. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Extractors\\CSV ; use Rubix\\ML\\Transformers\\NumericStringConverter ; $dataset = Labeled :: fromIterator ( new CSV ( 'example.csv' , true )) -> apply ( new NumericStringConverter ()); We can check the number of records that were imported by calling the numSamples() method on the dataset object. echo $dataset -> numSamples (); 5000","title":"CSV"},{"location":"extracting-data.html#ndjson","text":"Another plain-text format called NDJSON or Newline Delimited Javascript Object Notation (JSON) can be considered a hybrid of both CSV and JSON. It contains rows of JSON arrays or objects delineated by a newline character ( \\n or \\r\\n ). It has the advantage of retaining type information like JSON and can also be read into memory efficiently like CSV. Example { \"attitude\" : \"nice\" , \"texture\" : \"furry\" , \"sociability\" : \"friendly\" , \"rating\" : 4 , \"class\" : \"not monster\" } { \"attitude\" : \"mean\" , \"texture\" : \"furry\" , \"sociability\" : \"loner\" , \"rating\" : -1.5 , \"class\" : \"monster\" } { \"attitude\" : \"nice\" , \"texture\" : \"rough\" , \"sociability\" : \"friendly\" , \"rating\" : 2.6 , \"class\" : \"not monster\" } The NDJSON extractor can be used to instantiate a new dataset object from a NDJSON file. Optionally, it can be combined with the standard PHP library's Limit Iterator to only load a portion of the data into memory. In the example below, we load the first 1,000 rows of data from an NDJSON file into an Unlabeled dataset. use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Datasets\\Unlabeled ; use LimitIterator ; $extractor = new NDJSON ( 'example.ndjson' ); $iterator = new LimitIterator ( $extractor -> getIterator (), 0 , 1000 ); $dataset = Unlabeled :: fromIterator ( $iterator );","title":"NDJSON"},{"location":"extracting-data.html#sql","text":"Medium to large datasets will often be stored in an RDBMS (relational database management system) such as MySQL , PostgreSQL or Sqlite . Relational databases allow you to query large amounts of data on-the-fly and can be very flexible. PHP comes with robust relational database support through its PDO interface. To iterate over the rows of an SQL table we provide an SQL Table extractor uses the PDO interface under the hood. In the example below we'll wrap our SQL Table extractor in a Column Picker to instantiate a new Unlabeled dataset object from a particular set of columns of the table. use Rubix\\ML\\Extractors\\SQLTable ; use Rubix\\ML\\Extractors\\ColumnPicker ; use Rubix\\ML\\Datasets\\Unlabeled ; use PDO ; $connection = new PDO ( 'sqlite:/example.sqlite' ); $extractor = new ColumnPicker ( new SQLTable ( $connection , 'patients' ), [ 'age' , 'gender' , 'height' , 'diagnosis' , ]); $dataset = Unlabeled :: fromIterator ( $extractor ); If you need more control over your data pipeline then we recommend writing your own custom queries. The following example uses the PDO interface to execute a user-defined SQL query and instantiate a dataset object containing the same data as the example above. However, this method may be more efficient because it avoids querying for more data than you need. use Rubix\\ML\\Datasets\\Unlabeled ; use PDO ; $pdo = new PDO ( 'sqlite:/example.sqlite' ); $query = $pdo -> prepare ( 'SELECT age, gender, height, diagnosis FROM patients' ); $query -> execute (); $samples = $query -> fetchAll ( PDO :: FETCH_NUM ); $dataset = new Unlabeled ( $samples );","title":"SQL"},{"location":"extracting-data.html#images","text":"PHP offers a number of functions to import images as PHP resources such as imagecreatefromjpeg() and imagecreatefrompng() that come with the GD extension. The example below imports the .png images in the train folder and labels them using part of their filename. The samples and labels are then put into a Labeled dataset using the build() factory method and then converted into raw color channel data by applying the Image Vectorizer . use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Transformers\\ImageVectorizer ; $samples = $labels = []; foreach ( glob ( 'train/*.png' ) as $file ) { $samples [] = [ imagecreatefrompng ( $file )]; $labels [] = preg_replace ( '/[0-9]+_(.*).png/' , '$1' , basename ( $file )); } $dataset = Labeled :: build ( $samples , $labels ) -> apply ( new ImageVectorizer ());","title":"Images"},{"location":"extracting-data.html#synthetic-datasets","text":"Synthetic datasets are those that can be generated by one or more predefined formulas. In Rubix ML, we can generate synthetic datasets using Generator objects. Generators are useful in educational settings and for supplementing a small dataset with more samples. To generate a labeled dataset using the Half Moon generator pass the number of records you wish to generate to the generate() method. use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; $generator = new HalfMoon (); $dataset = $generator -> generate ( 1000 ); Now we can write the dataset to a CSV file and import it into our favorite plotting software. use Rubix\\ML\\Extractors\\CSV ; $dataset -> exportTo ( new CSV ( 'half-moon.csv' ));","title":"Synthetic Datasets"},{"location":"faq.html","text":"FAQ # Here you will find answers to the most frequently asked questions. Is Machine Learning the same thing as AI? # Machine Learning is a subset of Artificial Intelligence (AI) that focuses on using data to train a computer to perform tasks. While machine learning (ML) has contributed substantially to the field of AI, other non-learning techniques such as rule-based or symbolic systems are also forms of artificial intelligence. What types of problems is ML good for? # Machine Learning is a good fit for problems in which it would be infeasible for software developers and domain experts to design a system that could encode all the necessary rulesets to obtain accurate predictions. In other words, if your problem can be solved with a few if/then statements, it is probably not a good fit for machine learning due to unnecessary complexity. What environment (SAPI) should I run Rubix ML in? # All Rubix ML projects are designed to run from the PHP command line interface ( CLI ). The reason almost always boils down to performance and memory consumption. If you would like to serve your models in production, the preferred method is to use the Server library to spin up a high-performance standalone model server from the command line. If you plan to implement your own model server, we recommend using an asynchronous event loop such as React PHP or Swoole to prevent the model from having to be loaded into memory on each request. To run a PHP script using the command line interface (CLI), open a terminal window and enter: $ php example.php Note The PHP interpreter must be installed and in your default PATH for the above syntax to work correctly. I'm getting out of memory errors. # Try adjusting the memory_limit option in your php.ini file to something more reasonable. We recommend setting this to -1 (no limit) or slightly below your device's memory supply for best results. You can temporarily set the memory_limit in your script by using the ini_set() function. ini_set ( 'memory_limit' , '-1' ); Note Training can require a lot of memory. The amount necessary will depend on the amount of training data and the size of your model. If you have more data than you can hold in memory, some learners allow you to train in batches. See the Online Learning section of the Training docs for more information. Training is slower than usual. # Training time depends on a number of factors including size of the dataset and complexity of the model. If you believe that training is taking unusually long then check the following factors. Xdebug or other debuggers are not enabled. You have enough RAM to hold the dataset and model in memory without swapping to disk. Does Rubix ML support multiprocessing/multithreading? # Yes, learners that support parallel processing (multiprocessing or multithreading) do so by utilizing a pluggable parallel computing backend such as Amp or extension such as Tensor under the hood. Does Rubix ML support Deep Learning? # Yes, a number of learners in the library support Deep Learning including the Multilayer Perceptron classifier and MLP Regressor . Does Rubix ML support Reinforcement Learning? # Not currently, but we may in the future. Does Rubix ML support time series data? # Yes and no. Currently, the library treats time series data like any other continuous feature. In the future, we may add algorithms that work specifically with a separate time component. How can I contribute to the project? # Anyone is welcome to contribute to Rubix ML. See the CONTRIBUTING guide in the project root for more info. How can I become a sponsor? # Check out our funding sources here and consider donating to the cause.","title":"FAQ"},{"location":"faq.html#faq","text":"Here you will find answers to the most frequently asked questions.","title":"FAQ"},{"location":"faq.html#is-machine-learning-the-same-thing-as-ai","text":"Machine Learning is a subset of Artificial Intelligence (AI) that focuses on using data to train a computer to perform tasks. While machine learning (ML) has contributed substantially to the field of AI, other non-learning techniques such as rule-based or symbolic systems are also forms of artificial intelligence.","title":"Is Machine Learning the same thing as AI?"},{"location":"faq.html#what-types-of-problems-is-ml-good-for","text":"Machine Learning is a good fit for problems in which it would be infeasible for software developers and domain experts to design a system that could encode all the necessary rulesets to obtain accurate predictions. In other words, if your problem can be solved with a few if/then statements, it is probably not a good fit for machine learning due to unnecessary complexity.","title":"What types of problems is ML good for?"},{"location":"faq.html#what-environment-sapi-should-i-run-rubix-ml-in","text":"All Rubix ML projects are designed to run from the PHP command line interface ( CLI ). The reason almost always boils down to performance and memory consumption. If you would like to serve your models in production, the preferred method is to use the Server library to spin up a high-performance standalone model server from the command line. If you plan to implement your own model server, we recommend using an asynchronous event loop such as React PHP or Swoole to prevent the model from having to be loaded into memory on each request. To run a PHP script using the command line interface (CLI), open a terminal window and enter: $ php example.php Note The PHP interpreter must be installed and in your default PATH for the above syntax to work correctly.","title":"What environment (SAPI) should I run Rubix ML in?"},{"location":"faq.html#im-getting-out-of-memory-errors","text":"Try adjusting the memory_limit option in your php.ini file to something more reasonable. We recommend setting this to -1 (no limit) or slightly below your device's memory supply for best results. You can temporarily set the memory_limit in your script by using the ini_set() function. ini_set ( 'memory_limit' , '-1' ); Note Training can require a lot of memory. The amount necessary will depend on the amount of training data and the size of your model. If you have more data than you can hold in memory, some learners allow you to train in batches. See the Online Learning section of the Training docs for more information.","title":"I'm getting out of memory errors."},{"location":"faq.html#training-is-slower-than-usual","text":"Training time depends on a number of factors including size of the dataset and complexity of the model. If you believe that training is taking unusually long then check the following factors. Xdebug or other debuggers are not enabled. You have enough RAM to hold the dataset and model in memory without swapping to disk.","title":"Training is slower than usual."},{"location":"faq.html#does-rubix-ml-support-multiprocessingmultithreading","text":"Yes, learners that support parallel processing (multiprocessing or multithreading) do so by utilizing a pluggable parallel computing backend such as Amp or extension such as Tensor under the hood.","title":"Does Rubix ML support multiprocessing/multithreading?"},{"location":"faq.html#does-rubix-ml-support-deep-learning","text":"Yes, a number of learners in the library support Deep Learning including the Multilayer Perceptron classifier and MLP Regressor .","title":"Does Rubix ML support Deep Learning?"},{"location":"faq.html#does-rubix-ml-support-reinforcement-learning","text":"Not currently, but we may in the future.","title":"Does Rubix ML support Reinforcement Learning?"},{"location":"faq.html#does-rubix-ml-support-time-series-data","text":"Yes and no. Currently, the library treats time series data like any other continuous feature. In the future, we may add algorithms that work specifically with a separate time component.","title":"Does Rubix ML support time series data?"},{"location":"faq.html#how-can-i-contribute-to-the-project","text":"Anyone is welcome to contribute to Rubix ML. See the CONTRIBUTING guide in the project root for more info.","title":"How can I contribute to the project?"},{"location":"faq.html#how-can-i-become-a-sponsor","text":"Check out our funding sources here and consider donating to the cause.","title":"How can I become a sponsor?"},{"location":"grid-search.html","text":"[source] Grid Search # Grid Search is an algorithm that optimizes hyper-parameter selection. From the user's perspective, the process of training and predicting is the same, however, under the hood Grid Search trains a model for each combination of possible parameters and the best model is selected as the base estimator. Interfaces: Estimator , Learner , Parallel , Persistable , Verbose Data Type Compatibility: Depends on base learner Parameters # # Name Default Type Description 1 base string The class name of the base learner. 2 params array An array of lists containing the possible values for each of the base learner's constructor parameters. 3 metric auto Metric The validation metric used to score each set of hyper-parameters. 4 validator KFold Validator The validator used to test and score the model. Example # use Rubix\\ML\\GridSearch ; use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; use Rubix\\ML\\CrossValidation\\KFold ; $params = [ [ 1 , 3 , 5 , 10 ], [ true , false ], [ new Euclidean (), new Manhattan ()], ]; $estimator = new GridSearch ( KNearestNeighbors :: class , $params , new FBeta (), new KFold ( 5 )); Additional Methods # Return the base learner instance: public base () : ? \\Rubix\\ML\\Learner","title":"Grid Search"},{"location":"grid-search.html#grid-search","text":"Grid Search is an algorithm that optimizes hyper-parameter selection. From the user's perspective, the process of training and predicting is the same, however, under the hood Grid Search trains a model for each combination of possible parameters and the best model is selected as the base estimator. Interfaces: Estimator , Learner , Parallel , Persistable , Verbose Data Type Compatibility: Depends on base learner","title":"Grid Search"},{"location":"grid-search.html#parameters","text":"# Name Default Type Description 1 base string The class name of the base learner. 2 params array An array of lists containing the possible values for each of the base learner's constructor parameters. 3 metric auto Metric The validation metric used to score each set of hyper-parameters. 4 validator KFold Validator The validator used to test and score the model.","title":"Parameters"},{"location":"grid-search.html#example","text":"use Rubix\\ML\\GridSearch ; use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; use Rubix\\ML\\CrossValidation\\KFold ; $params = [ [ 1 , 3 , 5 , 10 ], [ true , false ], [ new Euclidean (), new Manhattan ()], ]; $estimator = new GridSearch ( KNearestNeighbors :: class , $params , new FBeta (), new KFold ( 5 ));","title":"Example"},{"location":"grid-search.html#additional-methods","text":"Return the base learner instance: public base () : ? \\Rubix\\ML\\Learner","title":"Additional Methods"},{"location":"hyper-parameter-tuning.html","text":"Hyper-parameter Tuning # Hyper-parameter tuning is an experimental process that incorporates cross-validation to guide hyper-parameter selection. When choosing an estimator for your project it often helps to fine-tune its hyper-parameters in order to get the best accuracy and performance from the model. Manual Tuning # When actively tuning a model, we will train an estimator with one set of hyper-parameters, obtain a validation score, and then use that as a baseline to make future adjustments. The goal at each iteration is to determine whether the adjustments improve accuracy or cause it to decrease. We can consider a model to be fully tuned when adjustments to the hyper-parameters can no longer make improvements to the validation score. With practice, we'll develop an intuition for which parameters need adjusting. Refer to the API documentation for each learner for a description of each hyper-parameter. In the example below, we'll tune the radius parameter of Radius Neighbors Regressor by iterating over the following block of code with a different setting each time. At first, we can start by choosing radius from a set of values and then honing in on the best value once we have obtained the settings with the highest SMAPE score. use Rubix\\ML\\Regressors\\RadiusNeighborsRegressor ; use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE ; [ $training , $testing ] = $dataset -> randomize () -> split ( 0.8 ); $estimator = new RadiusNeighborsRegressor ( 0.5 ); // 0.1, 0.5, 1.0, 2.0, 5.0 $estimator -> train ( $training ); $predictions = $estimator -> predict ( $testing ); $metric = new SMAPE (); $score = $metric -> score ( $predictions , $testing -> labels ()); echo $score ; -4.75 Deterministic Training # When the algorithm that trains a Learner is stochastic or randomized, it may be desirable for the sake of hyper-parameter tuning to isolate the effect of randomness on training. Fortunately, PHP makes it easy to seed the pseudo-random number generator (PRNG) with a known constant so your training sessions are repeatable. To seed the random number generator call the srand() function at the start of your training script passing any integer constant. After that point the PRNG will generate the same series of random numbers each time the training script is run. srand ( 42 ) Hyper-parameter Optimization # In distinction to manual tuning, Hyper-parameter optimization is an AutoML technique that employs search and meta-learning strategies to explore various algorithm configurations. In Rubix ML, hyper-parameter optimizers are implemented as meta-estimators that wrap a base learner whose hyper-parameters we wish to optimize. Grid Search # Grid Search is a meta-estimator that aims to find the combination of hyper-parameters that maximizes a particular cross-validation Metric . It works by training and testing a unique model for each combination of possible hyper-parameters and then picking the combination that returns the highest validation score. Since Grid Search implements the Parallel interface, we can greatly reduce the search time by training many models in parallel. As an example, we could attempt to find the best setting for the hyper-parameter k in K Nearest Neighbors from a list of possible values 1 , 3 , 5 , and 10 . In addition, we could try each value of k with distance weighting turned on or off. We might also want to know if the data is sensitive to the underlying distance kernel so we'll try the standard Euclidean as well as the Manhattan distances. The order in which the sets of possible parameters are given to Grid Search is the same order they are given in the constructor of the learner. use Rubix\\ML\\GridSearch ; use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $params = [ [ 1 , 3 , 5 , 10 ], [ true , false ], [ new Euclidean (), new Manhattan ()] ]; $estimator = new GridSearch ( KNearestNeighbors :: class , $params ); $estimator -> train ( $dataset ); Once training is complete, Grid Search automatically trains the base learner with the best hyper-parameters on the full dataset and can perform inference like a normal estimator. $predictions = $estimator -> predict ( $dataset ); We can also dump the selected hyper-parameters by calling the params() method on the base learner. To return the base learner trained by Grid Search, call the base() method like in the example below. print_r ( $estimator -> base () -> params ()); Array ( [ k ] => 3 [ weighted ] => true [ kernel ] => Rubix\\ML\\Kernels\\Distance\\Euclidean Object () ) Grid Search vs. Random Search # When the possible values of the continuous hyper-parameters are selected such that they are evenly spaced out in a grid, we call that grid search . You can use the static grid() method on the Params helper to generate an array of evenly-spaced values automatically. use Rubix\\ML\\Helpers\\Params ; $params = [ Params :: grid ( 1 , 10 , 4 ), [ true , false ], // ... ]; When the list of possible continuous-valued hyper-parameters is randomly chosen from a distribution, we call that random search . In the absence of a good manual strategy, random search has the advantage of being able to search the hyper-parameter space more effectively by testing combinations of parameters that might not have been considered otherwise. To generate a list of random values from a uniform distribution you can use either the ints() or floats() method on the Params helper. use Rubix\\ML\\Helpers\\Params ; $params = [ Params :: ints ( 1 , 10 , 4 ), [ true , false ], // ... ];","title":"Hyper-parameter Tuning"},{"location":"hyper-parameter-tuning.html#hyper-parameter-tuning","text":"Hyper-parameter tuning is an experimental process that incorporates cross-validation to guide hyper-parameter selection. When choosing an estimator for your project it often helps to fine-tune its hyper-parameters in order to get the best accuracy and performance from the model.","title":"Hyper-parameter Tuning"},{"location":"hyper-parameter-tuning.html#manual-tuning","text":"When actively tuning a model, we will train an estimator with one set of hyper-parameters, obtain a validation score, and then use that as a baseline to make future adjustments. The goal at each iteration is to determine whether the adjustments improve accuracy or cause it to decrease. We can consider a model to be fully tuned when adjustments to the hyper-parameters can no longer make improvements to the validation score. With practice, we'll develop an intuition for which parameters need adjusting. Refer to the API documentation for each learner for a description of each hyper-parameter. In the example below, we'll tune the radius parameter of Radius Neighbors Regressor by iterating over the following block of code with a different setting each time. At first, we can start by choosing radius from a set of values and then honing in on the best value once we have obtained the settings with the highest SMAPE score. use Rubix\\ML\\Regressors\\RadiusNeighborsRegressor ; use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE ; [ $training , $testing ] = $dataset -> randomize () -> split ( 0.8 ); $estimator = new RadiusNeighborsRegressor ( 0.5 ); // 0.1, 0.5, 1.0, 2.0, 5.0 $estimator -> train ( $training ); $predictions = $estimator -> predict ( $testing ); $metric = new SMAPE (); $score = $metric -> score ( $predictions , $testing -> labels ()); echo $score ; -4.75","title":"Manual Tuning"},{"location":"hyper-parameter-tuning.html#deterministic-training","text":"When the algorithm that trains a Learner is stochastic or randomized, it may be desirable for the sake of hyper-parameter tuning to isolate the effect of randomness on training. Fortunately, PHP makes it easy to seed the pseudo-random number generator (PRNG) with a known constant so your training sessions are repeatable. To seed the random number generator call the srand() function at the start of your training script passing any integer constant. After that point the PRNG will generate the same series of random numbers each time the training script is run. srand ( 42 )","title":"Deterministic Training"},{"location":"hyper-parameter-tuning.html#hyper-parameter-optimization","text":"In distinction to manual tuning, Hyper-parameter optimization is an AutoML technique that employs search and meta-learning strategies to explore various algorithm configurations. In Rubix ML, hyper-parameter optimizers are implemented as meta-estimators that wrap a base learner whose hyper-parameters we wish to optimize.","title":"Hyper-parameter Optimization"},{"location":"hyper-parameter-tuning.html#grid-search","text":"Grid Search is a meta-estimator that aims to find the combination of hyper-parameters that maximizes a particular cross-validation Metric . It works by training and testing a unique model for each combination of possible hyper-parameters and then picking the combination that returns the highest validation score. Since Grid Search implements the Parallel interface, we can greatly reduce the search time by training many models in parallel. As an example, we could attempt to find the best setting for the hyper-parameter k in K Nearest Neighbors from a list of possible values 1 , 3 , 5 , and 10 . In addition, we could try each value of k with distance weighting turned on or off. We might also want to know if the data is sensitive to the underlying distance kernel so we'll try the standard Euclidean as well as the Manhattan distances. The order in which the sets of possible parameters are given to Grid Search is the same order they are given in the constructor of the learner. use Rubix\\ML\\GridSearch ; use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $params = [ [ 1 , 3 , 5 , 10 ], [ true , false ], [ new Euclidean (), new Manhattan ()] ]; $estimator = new GridSearch ( KNearestNeighbors :: class , $params ); $estimator -> train ( $dataset ); Once training is complete, Grid Search automatically trains the base learner with the best hyper-parameters on the full dataset and can perform inference like a normal estimator. $predictions = $estimator -> predict ( $dataset ); We can also dump the selected hyper-parameters by calling the params() method on the base learner. To return the base learner trained by Grid Search, call the base() method like in the example below. print_r ( $estimator -> base () -> params ()); Array ( [ k ] => 3 [ weighted ] => true [ kernel ] => Rubix\\ML\\Kernels\\Distance\\Euclidean Object () )","title":"Grid Search"},{"location":"hyper-parameter-tuning.html#grid-search-vs-random-search","text":"When the possible values of the continuous hyper-parameters are selected such that they are evenly spaced out in a grid, we call that grid search . You can use the static grid() method on the Params helper to generate an array of evenly-spaced values automatically. use Rubix\\ML\\Helpers\\Params ; $params = [ Params :: grid ( 1 , 10 , 4 ), [ true , false ], // ... ]; When the list of possible continuous-valued hyper-parameters is randomly chosen from a distribution, we call that random search . In the absence of a good manual strategy, random search has the advantage of being able to search the hyper-parameter space more effectively by testing combinations of parameters that might not have been considered otherwise. To generate a list of random values from a uniform distribution you can use either the ints() or floats() method on the Params helper. use Rubix\\ML\\Helpers\\Params ; $params = [ Params :: ints ( 1 , 10 , 4 ), [ true , false ], // ... ];","title":"Grid Search vs. Random Search"},{"location":"inference.html","text":"Inference # Inference is the process of making predictions using an Estimator . You can think of an estimator inferring the outcome of a sample given the input features and the estimator's hidden state obtained during training. Once a learner has been trained it can perform inference on any number of samples. Estimator Types # There are 4 base estimator types to consider in Rubix ML and each type outputs a prediction specific to its type. Meta-estimators are polymorphic in the sense that they take on the type of the base estimator they wrap. Estimator Type Prediction Data Type Example Classifier Class label String 'cat' Regressor Number Integer or Float 1.348957 Clusterer Cluster number Integer 6 Anomaly Detector 1 for an anomaly or 0 otherwise Integer 0 Making Predictions # All estimators implement the Estimator interface which provides the predict() method. The predict() method takes a dataset of unknown samples and returns their predictions from the model in an array. Note The inference samples must contain the same number and order of feature columns as the samples used to train the learner. $predictions = $estimator -> predict ( $dataset ); print_r ( $predictions ); Array ( [ 0 ] => cat [ 1 ] => dog [ 2 ] => frog ) Estimation of Probabilities # Sometimes, you may want to know how certain the model is about a particular outcome. Classifiers and clusterers that implement the Probabilistic interface have the proba() method that computes the joint probability estimates for each class or cluster number as shown in the example below. $probabilities = $estimator -> proba ( $dataset ); print_r ( $probabilities ); Array ( [ 0 ] => Array ( [ cat ] => 0.6 [ dog ] => 0.4 [ frog ] => 0.0 ) [ 1 ] => Array ( [ cat ] => 0.3 [ dog ] => 0.6 [ frog ] => 0.1 ) [ 2 ] => Array ( [ cat ] => 0.0 [ dog ] => 0.0 [ frog ] => 1.0 ) ) Anomaly Scores # Anomaly detectors that implement the Scoring interface can output the anomaly scores assigned to the samples in a dataset. Anomaly scores are useful for attaining the degree of anomalousness for a sample relative to other samples. Higher anomaly scores equate to greater abnormality whereas low scores are typical of normal samples. $scores = $estimator -> score ( $dataset ); print_r ( $scores ); Array ( [ 0 ] => 0.35033 [ 1 ] => 0.40992 [ 2 ] => 1.68153 )","title":"Inference"},{"location":"inference.html#inference","text":"Inference is the process of making predictions using an Estimator . You can think of an estimator inferring the outcome of a sample given the input features and the estimator's hidden state obtained during training. Once a learner has been trained it can perform inference on any number of samples.","title":"Inference"},{"location":"inference.html#estimator-types","text":"There are 4 base estimator types to consider in Rubix ML and each type outputs a prediction specific to its type. Meta-estimators are polymorphic in the sense that they take on the type of the base estimator they wrap. Estimator Type Prediction Data Type Example Classifier Class label String 'cat' Regressor Number Integer or Float 1.348957 Clusterer Cluster number Integer 6 Anomaly Detector 1 for an anomaly or 0 otherwise Integer 0","title":"Estimator Types"},{"location":"inference.html#making-predictions","text":"All estimators implement the Estimator interface which provides the predict() method. The predict() method takes a dataset of unknown samples and returns their predictions from the model in an array. Note The inference samples must contain the same number and order of feature columns as the samples used to train the learner. $predictions = $estimator -> predict ( $dataset ); print_r ( $predictions ); Array ( [ 0 ] => cat [ 1 ] => dog [ 2 ] => frog )","title":"Making Predictions"},{"location":"inference.html#estimation-of-probabilities","text":"Sometimes, you may want to know how certain the model is about a particular outcome. Classifiers and clusterers that implement the Probabilistic interface have the proba() method that computes the joint probability estimates for each class or cluster number as shown in the example below. $probabilities = $estimator -> proba ( $dataset ); print_r ( $probabilities ); Array ( [ 0 ] => Array ( [ cat ] => 0.6 [ dog ] => 0.4 [ frog ] => 0.0 ) [ 1 ] => Array ( [ cat ] => 0.3 [ dog ] => 0.6 [ frog ] => 0.1 ) [ 2 ] => Array ( [ cat ] => 0.0 [ dog ] => 0.0 [ frog ] => 1.0 ) )","title":"Estimation of Probabilities"},{"location":"inference.html#anomaly-scores","text":"Anomaly detectors that implement the Scoring interface can output the anomaly scores assigned to the samples in a dataset. Anomaly scores are useful for attaining the degree of anomalousness for a sample relative to other samples. Higher anomaly scores equate to greater abnormality whereas low scores are typical of normal samples. $scores = $estimator -> score ( $dataset ); print_r ( $scores ); Array ( [ 0 ] => 0.35033 [ 1 ] => 0.40992 [ 2 ] => 1.68153 )","title":"Anomaly Scores"},{"location":"installation.html","text":"Installation # Install Rubix ML into your project using Composer : $ composer require rubix/ml Requirements # PHP 7.4 or above Recommended Tensor extension for fast Matrix/Vector computing Optional Extras Package for experimental features GD extension for image support Mbstring extension for fast multibyte string manipulation SVM extension for Support Vector Machine engine (libsvm) PDO extension for relational database support","title":"Installation"},{"location":"installation.html#installation","text":"Install Rubix ML into your project using Composer : $ composer require rubix/ml","title":"Installation"},{"location":"installation.html#requirements","text":"PHP 7.4 or above Recommended Tensor extension for fast Matrix/Vector computing Optional Extras Package for experimental features GD extension for image support Mbstring extension for fast multibyte string manipulation SVM extension for Support Vector Machine engine (libsvm) PDO extension for relational database support","title":"Requirements"},{"location":"learner.html","text":"Learner # Most estimators have the ability to be trained with data. These estimators are called Learners and require training before they are can make predictions. Training is the process of feeding data to the learner so that it can form a generalized representation or model of the dataset. Train a Learner # To train a learner pass a training dataset as argument to the train() method: public train ( Dataset $training ) : void $estimator -> train ( $dataset ); Note Calling the train() method on an already trained learner will erase its previous training. If you would like to train a model incrementally, you can do so with learners implementing the Online interface. Is the Learner Trained? # Return whether or not the learner has been trained: public trained () : bool var_dump ( $estimator -> trained ()); bool(true)","title":"Learner"},{"location":"learner.html#learner","text":"Most estimators have the ability to be trained with data. These estimators are called Learners and require training before they are can make predictions. Training is the process of feeding data to the learner so that it can form a generalized representation or model of the dataset.","title":"Learner"},{"location":"learner.html#train-a-learner","text":"To train a learner pass a training dataset as argument to the train() method: public train ( Dataset $training ) : void $estimator -> train ( $dataset ); Note Calling the train() method on an already trained learner will erase its previous training. If you would like to train a model incrementally, you can do so with learners implementing the Online interface.","title":"Train a Learner"},{"location":"learner.html#is-the-learner-trained","text":"Return whether or not the learner has been trained: public trained () : bool var_dump ( $estimator -> trained ()); bool(true)","title":"Is the Learner Trained?"},{"location":"model-ensembles.html","text":"Model Ensembles # Ensemble learning is when multiple estimators are used together to form the prediction of a sample. Model ensembles can consist of multiple variations of the same estimator, a heterogeneous mix of estimators of the same type, or even a mix of different estimator types. Bootstrap Aggregator # Bootstrap Aggregation or bagging is a technique that trains multiple clones of the same estimator that each specialize on a subset of the training set known as a bootstrap set. The final prediction is the averaged prediction returned by the ensemble. By averaging, we can often achieve greater accuracy than a single estimator at the cost of training more models. In the example below, we'll wrap a Regression Tree in the Bootstrap Aggregator meta-estimator to form a forest of 1,000 trees. Calling the train() method will train the ensemble and afterward the meta-estimator can be used to make predictions like a regular estimator. use Rubix\\ML\\BootstrapAggregator ; use Rubix\\ML\\Regressors\\RegressionTree ; $estimator = new BootstrapAggregator ( new RegressionTree ( 5 ), 1000 ); $estimator -> train ( $dataset ); Committee Machine # Committee Machine is another ensemble learner that works by the principal of averaging. It is a meta-estimator consisting of a heterogeneous mix of estimators (referred to as experts ) with user-programmable influences . Each expert is trained on the same dataset and the final prediction is based on the contribution of each expert weighted by their influence in the committee. By varying the influences of the experts, we can control which estimators contribute more or less to the final prediction. use Rubix\\ML\\CommitteeMachine ; use Rubix\\ML\\RandomForest ; new Rubix\\ML\\SoftmaxClassifier ; use Rubix\\ML\\AdaBoost ; $estimator = new CommitteeMachine ([ new RandomForest (), new SoftmaxClassifier ( 128 ), new AdaBoost (), ], [ 3.0 , 1.7 , 2.5 , ]); Model Chaining # Model chaining is a form of ensemble learning that uses the predictions of one or more estimators as the input features to other downstream estimators. In this simple example, let's say we want to predict if we should give a customer a loan or not. One thing we could do is we could first predict the customer's credit score and then add it to the dataset with the original features for the loan classifier to train and infer on. We'll write a custom callback function to add the new feature to the training set after their values have been predicted using the Lambda Function transformer. The callback accepts three arguments - the current sample passed by reference, the current row offset, and a context variable which we'll use to store the predicted credit scores. use Rubix\\ML\\Regressors\\KDNeighborsRegressor ; use Rubix\\ML\\Transformers\\LambdaFunction ; use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ExtraTreeClassifier ; $creditScoreEstimator = new KDNeighborsRegressor ( 10 ); $creditScoreEstimator -> train ( $dataset ); $creditScores = $creditScoreEstimator -> predict ( $dataset ); $addFeature = function ( & $sample , $offset , $context ) { $sample [] = $context [ $offset ]; } $dataset -> apply ( new LambdaFunction ( $addFeature , $creditScores )); $loanApprovalEstimator = new RandomForest ( new ExtraTreeClassifier ( 8 ), 300 ); $loanApprovalEstimator -> train ( $dataset ); Model Orchestra # When you combine chaining with model averaging you get a technique referred to as stacking . Unlike Committee Machine , which relies on a priori knowledge of the estimator influences, stacking aims to learn the influence scores automatically by using another model. We introduce the orchestra pattern for implementing a stacked model ensemble. The complete model consists of three Probabilistic classifiers referred to as the orchestra and a conductor that makes the final prediction by training on the class probabilities outputted by the orchestra. A key step to this process is to separate the training set into two sets so that we can do a second optimization to determine the model influences. We can vary the amount of data used to train each layer of the model by changing the proportion argument to the stratifiedSplit() method. For this example, we'll choose to use half of the data to train the orchestra and half to train the conductor. use Rubix\\ML\\Classifiers\\KDNeighbors ; use Rubix\\ML\\Classifiers\\RadiusNeighbors ; use Rubix\\ML\\Classifiers\\AdaBoost ; use Rubix\\ML\\Classifiers\\MultilayerPerceptron ; use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; use Rubix\\ML\\Datasets\\Labeled ; [ $dataset1 , $dataset2 ] = $training -> stratifiedSplit ( 0.5 ); $orchestra = [ new KDNeighbors ( 10 ), new RadiusNeighbors ( 2.0 ), new AdaBoost (), ]; $samples = []; foreach ( $orchestra as $estimator ) { $estimator -> train ( $dataset1 ); $probabilities = $estimator -> proba ( $dataset2 ); foreach ( $probabilities as $offset => $dist ) { $sample = & $sample [ $offset ]; $sample = array_merge ( $sample , array_values ( $dist )); } } $dataset = new Labeled ( $samples , $dataset2 -> labels ()); $conductor = new MultilayerPerceptron ([ new Dense ( 100 ), new Activation ( new ReLU ()), ]); $conductor -> train ( $dataset );","title":"Model Ensembles"},{"location":"model-ensembles.html#model-ensembles","text":"Ensemble learning is when multiple estimators are used together to form the prediction of a sample. Model ensembles can consist of multiple variations of the same estimator, a heterogeneous mix of estimators of the same type, or even a mix of different estimator types.","title":"Model Ensembles"},{"location":"model-ensembles.html#bootstrap-aggregator","text":"Bootstrap Aggregation or bagging is a technique that trains multiple clones of the same estimator that each specialize on a subset of the training set known as a bootstrap set. The final prediction is the averaged prediction returned by the ensemble. By averaging, we can often achieve greater accuracy than a single estimator at the cost of training more models. In the example below, we'll wrap a Regression Tree in the Bootstrap Aggregator meta-estimator to form a forest of 1,000 trees. Calling the train() method will train the ensemble and afterward the meta-estimator can be used to make predictions like a regular estimator. use Rubix\\ML\\BootstrapAggregator ; use Rubix\\ML\\Regressors\\RegressionTree ; $estimator = new BootstrapAggregator ( new RegressionTree ( 5 ), 1000 ); $estimator -> train ( $dataset );","title":"Bootstrap Aggregator"},{"location":"model-ensembles.html#committee-machine","text":"Committee Machine is another ensemble learner that works by the principal of averaging. It is a meta-estimator consisting of a heterogeneous mix of estimators (referred to as experts ) with user-programmable influences . Each expert is trained on the same dataset and the final prediction is based on the contribution of each expert weighted by their influence in the committee. By varying the influences of the experts, we can control which estimators contribute more or less to the final prediction. use Rubix\\ML\\CommitteeMachine ; use Rubix\\ML\\RandomForest ; new Rubix\\ML\\SoftmaxClassifier ; use Rubix\\ML\\AdaBoost ; $estimator = new CommitteeMachine ([ new RandomForest (), new SoftmaxClassifier ( 128 ), new AdaBoost (), ], [ 3.0 , 1.7 , 2.5 , ]);","title":"Committee Machine"},{"location":"model-ensembles.html#model-chaining","text":"Model chaining is a form of ensemble learning that uses the predictions of one or more estimators as the input features to other downstream estimators. In this simple example, let's say we want to predict if we should give a customer a loan or not. One thing we could do is we could first predict the customer's credit score and then add it to the dataset with the original features for the loan classifier to train and infer on. We'll write a custom callback function to add the new feature to the training set after their values have been predicted using the Lambda Function transformer. The callback accepts three arguments - the current sample passed by reference, the current row offset, and a context variable which we'll use to store the predicted credit scores. use Rubix\\ML\\Regressors\\KDNeighborsRegressor ; use Rubix\\ML\\Transformers\\LambdaFunction ; use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ExtraTreeClassifier ; $creditScoreEstimator = new KDNeighborsRegressor ( 10 ); $creditScoreEstimator -> train ( $dataset ); $creditScores = $creditScoreEstimator -> predict ( $dataset ); $addFeature = function ( & $sample , $offset , $context ) { $sample [] = $context [ $offset ]; } $dataset -> apply ( new LambdaFunction ( $addFeature , $creditScores )); $loanApprovalEstimator = new RandomForest ( new ExtraTreeClassifier ( 8 ), 300 ); $loanApprovalEstimator -> train ( $dataset );","title":"Model Chaining"},{"location":"model-ensembles.html#model-orchestra","text":"When you combine chaining with model averaging you get a technique referred to as stacking . Unlike Committee Machine , which relies on a priori knowledge of the estimator influences, stacking aims to learn the influence scores automatically by using another model. We introduce the orchestra pattern for implementing a stacked model ensemble. The complete model consists of three Probabilistic classifiers referred to as the orchestra and a conductor that makes the final prediction by training on the class probabilities outputted by the orchestra. A key step to this process is to separate the training set into two sets so that we can do a second optimization to determine the model influences. We can vary the amount of data used to train each layer of the model by changing the proportion argument to the stratifiedSplit() method. For this example, we'll choose to use half of the data to train the orchestra and half to train the conductor. use Rubix\\ML\\Classifiers\\KDNeighbors ; use Rubix\\ML\\Classifiers\\RadiusNeighbors ; use Rubix\\ML\\Classifiers\\AdaBoost ; use Rubix\\ML\\Classifiers\\MultilayerPerceptron ; use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; use Rubix\\ML\\Datasets\\Labeled ; [ $dataset1 , $dataset2 ] = $training -> stratifiedSplit ( 0.5 ); $orchestra = [ new KDNeighbors ( 10 ), new RadiusNeighbors ( 2.0 ), new AdaBoost (), ]; $samples = []; foreach ( $orchestra as $estimator ) { $estimator -> train ( $dataset1 ); $probabilities = $estimator -> proba ( $dataset2 ); foreach ( $probabilities as $offset => $dist ) { $sample = & $sample [ $offset ]; $sample = array_merge ( $sample , array_values ( $dist )); } } $dataset = new Labeled ( $samples , $dataset2 -> labels ()); $conductor = new MultilayerPerceptron ([ new Dense ( 100 ), new Activation ( new ReLU ()), ]); $conductor -> train ( $dataset );","title":"Model Orchestra"},{"location":"model-persistence.html","text":"Model Persistence # Model persistence is the ability to save and subsequently load a learner's state in another process. Trained estimators can be used for real-time inference by loading the model onto a server or they can be saved to make predictions in batches offline at a later time. Estimators that implement the Persistable interface are able to have their internal state captured between processes. In addition, the library provides the Persistent Model meta-estimator that acts as a wrapper for persistable estimators. Serialization # Serialization occurs in between saving and loading a model and can be thought of as packaging the model's parameters. The data can be in a lightweight format such as with PHP's Native serializer or in a robust format such as RBX . In the this example, we'll demonstrate how to encode a Persistable learner using the compressed RBX format, save the encoding with a Persister , and then how to deserialize the encoding. use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Serializers\\RBX ; use Rubix\\ML\\Persisters\\Filesystem ; $estimator = new RandomForest ( 100 ); $serializer = new RBX (); $encoding = $serializer -> serialize ( $estimator ); $estimator = $serializer -> deserialize ( $encoding ); Note Due to a limitation in PHP, anonymous classes and functions ( closures ) are not able to be deserialized. Therefore, avoid anonymous classes or functions if you intend to persist the model. Persistent Model Meta-estimator # The persistence subsystem can be interfaced at a low level with Serializer and Persister objects or it can be interacted with at a higher level using the Persistent Model meta-estimator. It is a decorator that provides save() and load() methods giving the estimator the ability to save and load itself. use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Clusterers\\KMeans ; use Rubix\\ML\\Persisters\\Filesystem ; $estimator = new PersistentModel ( new KMeans ( 5 ), new Filesystem ( 'example.rbx' )); $estimator -> train ( $dataset ); $estimator -> save (); use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Persisters\\Filesystem ; $estimator = PersistentModel :: load ( new Filesystem ( 'example.rbx' )); Persisting Transformers # In addition to Learners, the persistence subsystem can be used to individually save and load any Stateful transformer that implements the Persistable interface. In the example below we'll fit a transformer to a dataset and then save it to the Filesystem . use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Serializers\\RBX ; use Rubix\\ML\\Persisters\\Filesystem ; $transformer = new OneHotEncoder (); $serializer = new RBX (); $transformer -> fit ( $dataset ); $serializer -> serialize ( $transformer ) -> saveTo ( new Filesystem ( 'example.rbx' )); Then, to load the transformer in another process call the deserialize() method on the encoding returned by the persister's load() method. use Rubix\\ML\\Serializers\\RBX ; use Rubix\\ML\\Persisters\\Filesystem ; $transformer = $persister -> load () -> deserializeWith ( new RBX ()); Caveats # Since model data are exported with the learner's current class definition in mind, problems may occur when loading a model using a different version of the library than the one it was trained and saved on. For example, when upgrading to a new version, there is a small chance that a previously saved learner may not be able to be deserialized if the model is not compatible with the learner's new class definition. For maximum interoperability, ensure that each system is running the same version of the library.","title":"Model Persistence"},{"location":"model-persistence.html#model-persistence","text":"Model persistence is the ability to save and subsequently load a learner's state in another process. Trained estimators can be used for real-time inference by loading the model onto a server or they can be saved to make predictions in batches offline at a later time. Estimators that implement the Persistable interface are able to have their internal state captured between processes. In addition, the library provides the Persistent Model meta-estimator that acts as a wrapper for persistable estimators.","title":"Model Persistence"},{"location":"model-persistence.html#serialization","text":"Serialization occurs in between saving and loading a model and can be thought of as packaging the model's parameters. The data can be in a lightweight format such as with PHP's Native serializer or in a robust format such as RBX . In the this example, we'll demonstrate how to encode a Persistable learner using the compressed RBX format, save the encoding with a Persister , and then how to deserialize the encoding. use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Serializers\\RBX ; use Rubix\\ML\\Persisters\\Filesystem ; $estimator = new RandomForest ( 100 ); $serializer = new RBX (); $encoding = $serializer -> serialize ( $estimator ); $estimator = $serializer -> deserialize ( $encoding ); Note Due to a limitation in PHP, anonymous classes and functions ( closures ) are not able to be deserialized. Therefore, avoid anonymous classes or functions if you intend to persist the model.","title":"Serialization"},{"location":"model-persistence.html#persistent-model-meta-estimator","text":"The persistence subsystem can be interfaced at a low level with Serializer and Persister objects or it can be interacted with at a higher level using the Persistent Model meta-estimator. It is a decorator that provides save() and load() methods giving the estimator the ability to save and load itself. use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Clusterers\\KMeans ; use Rubix\\ML\\Persisters\\Filesystem ; $estimator = new PersistentModel ( new KMeans ( 5 ), new Filesystem ( 'example.rbx' )); $estimator -> train ( $dataset ); $estimator -> save (); use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Persisters\\Filesystem ; $estimator = PersistentModel :: load ( new Filesystem ( 'example.rbx' ));","title":"Persistent Model Meta-estimator"},{"location":"model-persistence.html#persisting-transformers","text":"In addition to Learners, the persistence subsystem can be used to individually save and load any Stateful transformer that implements the Persistable interface. In the example below we'll fit a transformer to a dataset and then save it to the Filesystem . use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Serializers\\RBX ; use Rubix\\ML\\Persisters\\Filesystem ; $transformer = new OneHotEncoder (); $serializer = new RBX (); $transformer -> fit ( $dataset ); $serializer -> serialize ( $transformer ) -> saveTo ( new Filesystem ( 'example.rbx' )); Then, to load the transformer in another process call the deserialize() method on the encoding returned by the persister's load() method. use Rubix\\ML\\Serializers\\RBX ; use Rubix\\ML\\Persisters\\Filesystem ; $transformer = $persister -> load () -> deserializeWith ( new RBX ());","title":"Persisting Transformers"},{"location":"model-persistence.html#caveats","text":"Since model data are exported with the learner's current class definition in mind, problems may occur when loading a model using a different version of the library than the one it was trained and saved on. For example, when upgrading to a new version, there is a small chance that a previously saved learner may not be able to be deserialized if the model is not compatible with the learner's new class definition. For maximum interoperability, ensure that each system is running the same version of the library.","title":"Caveats"},{"location":"online.html","text":"Online # Learners that implement the Online interface can be trained in batches. Learners of this type are great for when you either have a continuous stream of data or a dataset that is too large to fit into memory. In addition, partial training allows the model to evolve over time. Partially Train # To partially train an Online learner pass it a training set to its partial() method: public partial ( Dataset $dataset ) : void $folds = $dataset -> fold ( 3 ); $estimator -> train ( $folds [ 0 ]); $estimator -> partial ( $folds [ 1 ]); $estimator -> partial ( $folds [ 2 ]); Note Learner will continue to train as long as you are using the partial() method, however, calling train() on a trained or partially trained learner will reset it back to baseline first.","title":"Online"},{"location":"online.html#online","text":"Learners that implement the Online interface can be trained in batches. Learners of this type are great for when you either have a continuous stream of data or a dataset that is too large to fit into memory. In addition, partial training allows the model to evolve over time.","title":"Online"},{"location":"online.html#partially-train","text":"To partially train an Online learner pass it a training set to its partial() method: public partial ( Dataset $dataset ) : void $folds = $dataset -> fold ( 3 ); $estimator -> train ( $folds [ 0 ]); $estimator -> partial ( $folds [ 1 ]); $estimator -> partial ( $folds [ 2 ]); Note Learner will continue to train as long as you are using the partial() method, however, calling train() on a trained or partially trained learner will reset it back to baseline first.","title":"Partially Train"},{"location":"parallel.html","text":"Parallel # Multiprocessing is the use of two or more processes that execute in parallel. Objects that implement the Parallel interface can take advantage of multicore processors by executing parts or all of the algorithm in parallel. Choose a number of processes equal to the number of CPU cores in order to take advantage of a system's full processing capability. Note Most parallel learners are configured to use the Serial backend by default. Set a Backend # Parallelizable objects can utilize a parallel processing Backend by passing it to the setBackend() method. To set the backend processing engine: public setBackend ( Backend $backend ) : void use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Backends\\Amp ; $estimator = new RandomForest (); $estimator -> setBackend ( new Amp ( 16 ));","title":"Parallel"},{"location":"parallel.html#parallel","text":"Multiprocessing is the use of two or more processes that execute in parallel. Objects that implement the Parallel interface can take advantage of multicore processors by executing parts or all of the algorithm in parallel. Choose a number of processes equal to the number of CPU cores in order to take advantage of a system's full processing capability. Note Most parallel learners are configured to use the Serial backend by default.","title":"Parallel"},{"location":"parallel.html#set-a-backend","text":"Parallelizable objects can utilize a parallel processing Backend by passing it to the setBackend() method. To set the backend processing engine: public setBackend ( Backend $backend ) : void use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Backends\\Amp ; $estimator = new RandomForest (); $estimator -> setBackend ( new Amp ( 16 ));","title":"Set a Backend"},{"location":"persistable.html","text":"Persistable # An estimator that implements the Persistable interface can be serialized by a Serializer or save and loaded using the Persistent Model meta-estimator. To return the current class revision hash: public revision () : string echo $persistable -> revision (); e7eeec9a","title":"Persistable"},{"location":"persistable.html#persistable","text":"An estimator that implements the Persistable interface can be serialized by a Serializer or save and loaded using the Persistent Model meta-estimator. To return the current class revision hash: public revision () : string echo $persistable -> revision (); e7eeec9a","title":"Persistable"},{"location":"persistent-model.html","text":"[source] Persistent Model # The Persistent Model meta-estimator wraps a Persistable learner with additional functionality for saving and loading the model. It uses Persister objects to interface with various storage backends such as the Filesystem . Interfaces: Estimator , Learner , Probabilistic , Scoring Data Type Compatibility: Depends on base learner Parameters # # Name Default Type Description 1 base Persistable The persistable base learner. 2 persister Persister The persister used to interface with the storage system. 3 serializer RBX Serializer The object serializer. Examples # use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Clusterers\\KMeans ; use Rubix\\ML\\Persisters\\Filesystem ; use Rubix\\ML\\Serializers\\RBX ; $estimator = new PersistentModel ( new KMeans ( 10 ), new Filesystem ( 'example.model' ), new RBX ()); Additional Methods # Load the model from storage: public static load ( Persister $persister , ? Serializer $serializer = null ) : self use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Persisters\\Filesystem ; use Rubix\\ML\\Serializers\\RBX ; $estimator = PersistentModel :: load ( new Filesystem ( 'example.model' ), new RBX ()); Save the model to storage: public save () : void $estimator -> save ();","title":"Persistent Model"},{"location":"persistent-model.html#persistent-model","text":"The Persistent Model meta-estimator wraps a Persistable learner with additional functionality for saving and loading the model. It uses Persister objects to interface with various storage backends such as the Filesystem . Interfaces: Estimator , Learner , Probabilistic , Scoring Data Type Compatibility: Depends on base learner","title":"Persistent Model"},{"location":"persistent-model.html#parameters","text":"# Name Default Type Description 1 base Persistable The persistable base learner. 2 persister Persister The persister used to interface with the storage system. 3 serializer RBX Serializer The object serializer.","title":"Parameters"},{"location":"persistent-model.html#examples","text":"use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Clusterers\\KMeans ; use Rubix\\ML\\Persisters\\Filesystem ; use Rubix\\ML\\Serializers\\RBX ; $estimator = new PersistentModel ( new KMeans ( 10 ), new Filesystem ( 'example.model' ), new RBX ());","title":"Examples"},{"location":"persistent-model.html#additional-methods","text":"Load the model from storage: public static load ( Persister $persister , ? Serializer $serializer = null ) : self use Rubix\\ML\\PersistentModel ; use Rubix\\ML\\Persisters\\Filesystem ; use Rubix\\ML\\Serializers\\RBX ; $estimator = PersistentModel :: load ( new Filesystem ( 'example.model' ), new RBX ()); Save the model to storage: public save () : void $estimator -> save ();","title":"Additional Methods"},{"location":"pipeline.html","text":"[source] Pipeline # Pipeline is a meta-estimator capable of transforming an input dataset by applying a series of Transformer middleware . Under the hood, Pipeline will automatically fit the training set and transform any Dataset object supplied as an argument to one of the base estimator's methods before reaching the method context. With elastic mode enabled, Pipeline will update the fitting of Elastic transformers during partial training. Note Pipeline modifies the input dataset during fitting. If you need to keep a clean dataset in memory, you can clone the dataset object before calling the method that consumes it. Interfaces: Estimator , Learner , Online , Probabilistic , Scoring , Persistable Data Type Compatibility: Depends on base learner and transformers Parameters # # Name Default Type Description 1 transformers array A list of transformers to be applied in order. 2 estimator Estimator An instance of a base estimator to receive the transformed data. 3 elastic true bool Should we update the elastic transformers during partial training? Example # use Rubix\\ML\\Pipeline ; use Rubix\\ML\\Transformers\\MissingDataImputer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\PrincipalComponentAnalysis ; use Rubix\\ML\\Classifiers\\SoftmaxClassifier ; $estimator = new Pipeline ([ new MissingDataImputer (), new OneHotEncoder (), new PrincipalComponentAnalysis ( 20 ), ], new SoftmaxClassifier ( 128 ), true ); Additional Methods # This meta-estimator does not have any additional methods.","title":"Pipeline"},{"location":"pipeline.html#pipeline","text":"Pipeline is a meta-estimator capable of transforming an input dataset by applying a series of Transformer middleware . Under the hood, Pipeline will automatically fit the training set and transform any Dataset object supplied as an argument to one of the base estimator's methods before reaching the method context. With elastic mode enabled, Pipeline will update the fitting of Elastic transformers during partial training. Note Pipeline modifies the input dataset during fitting. If you need to keep a clean dataset in memory, you can clone the dataset object before calling the method that consumes it. Interfaces: Estimator , Learner , Online , Probabilistic , Scoring , Persistable Data Type Compatibility: Depends on base learner and transformers","title":"Pipeline"},{"location":"pipeline.html#parameters","text":"# Name Default Type Description 1 transformers array A list of transformers to be applied in order. 2 estimator Estimator An instance of a base estimator to receive the transformed data. 3 elastic true bool Should we update the elastic transformers during partial training?","title":"Parameters"},{"location":"pipeline.html#example","text":"use Rubix\\ML\\Pipeline ; use Rubix\\ML\\Transformers\\MissingDataImputer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\PrincipalComponentAnalysis ; use Rubix\\ML\\Classifiers\\SoftmaxClassifier ; $estimator = new Pipeline ([ new MissingDataImputer (), new OneHotEncoder (), new PrincipalComponentAnalysis ( 20 ), ], new SoftmaxClassifier ( 128 ), true );","title":"Example"},{"location":"pipeline.html#additional-methods","text":"This meta-estimator does not have any additional methods.","title":"Additional Methods"},{"location":"preprocessing.html","text":"Preprocessing # Sometimes, one or more preprocessing steps may need to be taken before handing a dataset off to a Learner. In some cases, data may not be in the correct format and in others you may want to process the data to aid in training. Transformers # Transformers are objects that perform various preprocessing steps to the samples in a dataset. They take a dataset object as input and transform it in place. Stateful transformers are a type of transformer that must be fitted to a dataset. Fitting a dataset to a transformer is much like training a learner but in the context of preprocessing rather than inference. After fitting a stateful transformer, it will expect the features to be present in the same order when transforming subsequent datasets. A few transformers are supervised meaning they must be fitted with a Labeled dataset. Elastic transformers can have their fittings updated with new data after an initial fitting. Transform a Dataset # An example of a transformation is one that converts the categorical features of a dataset to continuous ones using a one hot encoding. To accomplish this with the library, pass a One Hot Encoder instance as an argument to the Dataset object's apply() method. Note that the apply() method also handles fitting a Stateful transformer automatically. use Rubix\\ML\\Transformers\\OneHotEncoder ; $dataset -> apply ( new OneHotEncoder ()); Transformations can be chained by calling the apply() method fluently. use Rubix\\ML\\Transformers\\HotDeckImputer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\MinMaxNormalizer ; $dataset -> apply ( new HotDeckImputer ( 5 )) -> apply ( new OneHotEncoder ()) -> apply ( new MinMaxNormalizer ()); Transforming the Labels # Transformers do not alter the labels in a dataset. For that we can pass a callback function to the transformLabels() method on a Labeled dataset instance. The callback accepts a single argument that is the value of the label to be transformed. In this example, we'll convert the categorical labels of a dataset to integer ordinals. $dataset -> transformLabels ( 'intval' ); Manually Fitting # If you need to fit a Stateful transformer to a dataset other than the one it was meant to transform, you can fit the transformer manually by calling the fit() method before applying the transformation. use Rubix\\ML\\Transformers\\WordCountVectorizer ; $transformer = new WordCountVectorizer ( 5000 ); $transformer -> fit ( $dataset1 ); $dataset2 -> apply ( $transformer ); Update Fitting # To update the fitting of an Elastic transformer call the update() method with a new dataset. $transformer -> update ( $dataset ); Types of Preprocessing # Here we dive into the different types of data preprocessing that Transformers are capable of. Standardization and Normalization # Oftentimes, the continuous features of a dataset will be on different scales because they were measured by different methods. For example, age (0 - 100) and income (0 - 9,999,999) are on two widely different scales. Standardization is the processes of transforming a dataset such that the features are all on one common scale. Normalization is the special case where the transformed features have a range between 0 and 1. Depending on the transformer, it may operate on the columns or the rows of the dataset. Transformer Operates Output Range Stateful Elastic L1 Normalizer Row-wise [0, 1] L2 Normalizer Row-wise [0, 1] Max Absolute Scaler Column-wise [-1, 1] \u25cf \u25cf Min Max Normalizer Column-wise [min, max] \u25cf \u25cf Robust Standardizer Column-wise [-\u221e, \u221e] \u25cf Z Scale Standardizer Column-wise [-\u221e, \u221e] \u25cf \u25cf Feature Conversion # Feature converters are transformers that convert feature columns of one data type to another by changing their representation. Transformer From To Stateful Elastic Interval Discretizer Continuous Categorical \u25cf One Hot Encoder Categorical Continuous \u25cf Numeric String Converter Categorical Continuous Boolean Converter Other Categorical or Continuous Dimensionality Reduction # Dimensionality reduction is a preprocessing technique for projecting a dataset onto a lower dimensional vector space. It allows a learner to train and infer quicker by producing a training set with fewer but more informative features. Dimensionality reducers can also be used to visualize datasets by outputting low (1 - 3) dimensionality embeddings for use in plotting software. Transformer Supervised Stateful Elastic Gaussian Random Projector \u25cf Linear Discriminant Analysis \u25cf \u25cf Principal Component Analysis \u25cf Sparse Random Projector \u25cf Truncated SVD \u25cf t-SNE Feature Expansion # Feature expansion aims to add flexibility to a model by deriving additional features from a dataset. It can be thought of as the opposite of dimensionality reduction. Transformer Supervised Stateful Elastic Polynomial Expander Imputation # Imputation is a technique for handling missing values in a dataset by replacing them with a pretty good guess. Transformer Compatibility Supervised Stateful Elastic KNN Imputer Depends on distance kernel \u25cf Missing Data Imputer Categorical, Continuous \u25cf Hot Deck Imputer Depends on distance kernel \u25cf Natural Language # The library provides a number of transformers for Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as text cleaning, feature extraction, and term weighting. Transformer Supervised Stateful Elastic Regex Filter Text Normalizer Multibyte Text Normalizer Stop Word Filter TF-IDF Transformer \u25cf \u25cf Token Hashing Vectorizer Word Count Vectorizer \u25cf Images # These transformers operate on the high-level image data type. Transformer Supervised Stateful Elastic Image Resizer Image Vectorizer \u25cf Custom Transformations # In additional to providing specialized Transformers for common preprocessing tasks, the library includes a Lambda Function transformer that allows you to apply custom data transformations using a callback. The callback function accepts a sample passed by reference so that the transformation occurs in-place. In the following example, let's write a callback to binarize the continuous features just at column offset 3 of the dataset. use Rubix\\ML\\Transformers\\LambdaFunction ; $binarize = function ( & $sample ) { $sample [ 3 ] = $sample [ 3 ] > 182 ? 'tall' : 'not tall' ; } $dataset -> apply ( new LambdaFunction ( $binarize )); Another technique we can employ using the Lambda Function transformer is to perform a categorical feature cross between two feature columns of a dataset. A cross feature is a higher-order feature that represents the presence of two or more features simultaneously. For example, we may want to represent the combination of someone's gender and education level as it's own feature. We'll choose to represent the new feature as a CRC32 hash to save on memory and storage but you could just concatenate both categories to represent the new feature as well. use Rubix\\ML\\Transformers\\LambdaFunction ; use function hash ; $crossFeatures = function ( & $sample ) { $sample [] = hash ( 'crc32b' , \" { $sample [ 6 ] } and { $sample [ 7 ] } \" ); }; $dataset -> apply ( new LambdaFunction ( $crossFeatures )); Advanced Preprocessing # In some cases, certain features of a dataset may require a different set of preprocessing steps than the others. In such a case, we can extract a certain set of features, preprocess them, and then join them with the rest of the dataset later. In the example below, we'll extract just the text reviews and their sentiment labels into a dataset object and put the sample's category, number of clicks, and ratings into another one using two Column Pickers . Then, we can apply a separate set of transformations to each set of features and use the join() method to combine them into a single dataset. We can even apply another set of transformations to the joined dataset after that. use Rubix\\ML\\Dataset\\Labeled ; use Rubix\\ML\\Extractors\\ColumnPicker ; use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Dataset\\Unlabeled ; use Rubix\\ML\\Transformers\\TextNormalizer ; use Rubix\\ML\\Transformers\\WordCountVectorizer ; use Rubix\\ML\\Transformers\\TfIdfTransformer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\ZScaleStandardizer ; $extractor1 = new ColumnPicker ( new NDJSON ( 'example.ndjson' ), [ 'review' , 'sentiment' , ]); $extractor2 = new ColumnPicker ( new NDJSON ( 'example.ndjson' ), [ 'category' , 'clicks' , 'rating' , ]); $dataset1 = Labeled :: fromIterator ( $extractor1 ) -> apply ( new TextNormalizer ()) -> apply ( new WordCountVectorizer ( 5000 )) -> apply ( new TfIdfTransformer ()); $dataset2 = Unlabeled :: fromIterator ( $extractor2 ) -> apply ( new OneHotEncoder ()); $dataset = $dataset1 -> join ( $dataset2 ) -> apply ( new ZScaleStandardizer ()); Transformer Pipelines # The Pipeline meta-estimator helps you automate a series of transformations applied to the input dataset to an estimator. With a Pipeline, any dataset object passed to will automatically be fitted and/or transformed before it arrives in the estimator's context. In addition, transformer fittings can be saved alongside the model data when the Pipeline is persisted. use Rubix\\ML\\Pipeline ; use Rubix\\ML\\Transformers\\HotDeckImputer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\ZScaleStandardizer ; use Rubix\\ML\\Clusterers\\KMeans ; $estimator = new Pipeline ([ new HotDeckImputer ( 5 ), new OneHotEncoder (), new ZScaleStandardizer (), ], new KMeans ( 10 )); Calling train() or partial() will result in the transformers being fitted or updated before being passed to the Softmax Classifier. $estimator -> train ( $dataset ); // Transformers fitted and applied $estimator -> partial ( $dataset ); // Transformers updated and applied Any time a dataset is passed to the Pipeline it will automatically be transformed before being handed to the underlying estimator. $predictions = $estimator -> predict ( $dataset ); // Dataset transformed automatically Filtering Records # In some cases, you may want to remove entire rows from the dataset. For example, you may want to remove records that contain features with abnormally low/high values as these samples can be interpreted as noise. The filter() method on the dataset object uses a callback function to determine if a row should be included in the return dataset. In this example, we'll filter all the samples whose value for feature at offset 3 is greater than some amount. $tallPeople = function ( $record ) { return $record [ 3 ] > 178.5 ; }; $dataset = $dataset -> filter ( $tallPeople ); Let's say we wanted to train a classifier with our Labeled dataset but only on a subset of the possible class outcomes. We could filter the samples that correspond to undesirable outcomes by targetting the label with our callback. use function in_array ; $dogsAndCats = function ( $record ) { return in_array ( end ( $record ), [ 'dog' , 'cat' ]); } $training = $dataset -> filter ( $dogsAndCats ); Note For Labeled datasets the label column is always the last column of the record. In the next example, we'll filter all the records that have missing feature values. We can detect missing continuous variables by calling the custom library function iterator_contains_nan() on each record. Additionally, we can filter records with missing categorical values by looking for a special placeholder category, in this case we'll use the value '?' , to denote missing categorical variables. use function Rubix\\ML\\iterator_contains_nan ; use function in_array ; $noMissingValues = function ( $record ) { return ! iterator_contains_nan ( $record ) and ! in_array ( '?' , $record ); }; $complete = $dataset -> filter ( $noMissingValues ); Note The standard PHP library function in_array() does not handle NAN comparisons. De-duplication # When it is undesirable for a dataset to contain duplicate records, you can remove all duplicates by calling the deduplicate() method on the dataset object. $dataset -> deduplicate (); Note The O(N^2) time complexity of de-duplication may be prohibitive for large datasets.","title":"Preprocessing"},{"location":"preprocessing.html#preprocessing","text":"Sometimes, one or more preprocessing steps may need to be taken before handing a dataset off to a Learner. In some cases, data may not be in the correct format and in others you may want to process the data to aid in training.","title":"Preprocessing"},{"location":"preprocessing.html#transformers","text":"Transformers are objects that perform various preprocessing steps to the samples in a dataset. They take a dataset object as input and transform it in place. Stateful transformers are a type of transformer that must be fitted to a dataset. Fitting a dataset to a transformer is much like training a learner but in the context of preprocessing rather than inference. After fitting a stateful transformer, it will expect the features to be present in the same order when transforming subsequent datasets. A few transformers are supervised meaning they must be fitted with a Labeled dataset. Elastic transformers can have their fittings updated with new data after an initial fitting.","title":"Transformers"},{"location":"preprocessing.html#transform-a-dataset","text":"An example of a transformation is one that converts the categorical features of a dataset to continuous ones using a one hot encoding. To accomplish this with the library, pass a One Hot Encoder instance as an argument to the Dataset object's apply() method. Note that the apply() method also handles fitting a Stateful transformer automatically. use Rubix\\ML\\Transformers\\OneHotEncoder ; $dataset -> apply ( new OneHotEncoder ()); Transformations can be chained by calling the apply() method fluently. use Rubix\\ML\\Transformers\\HotDeckImputer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\MinMaxNormalizer ; $dataset -> apply ( new HotDeckImputer ( 5 )) -> apply ( new OneHotEncoder ()) -> apply ( new MinMaxNormalizer ());","title":"Transform a Dataset"},{"location":"preprocessing.html#transforming-the-labels","text":"Transformers do not alter the labels in a dataset. For that we can pass a callback function to the transformLabels() method on a Labeled dataset instance. The callback accepts a single argument that is the value of the label to be transformed. In this example, we'll convert the categorical labels of a dataset to integer ordinals. $dataset -> transformLabels ( 'intval' );","title":"Transforming the Labels"},{"location":"preprocessing.html#manually-fitting","text":"If you need to fit a Stateful transformer to a dataset other than the one it was meant to transform, you can fit the transformer manually by calling the fit() method before applying the transformation. use Rubix\\ML\\Transformers\\WordCountVectorizer ; $transformer = new WordCountVectorizer ( 5000 ); $transformer -> fit ( $dataset1 ); $dataset2 -> apply ( $transformer );","title":"Manually Fitting"},{"location":"preprocessing.html#update-fitting","text":"To update the fitting of an Elastic transformer call the update() method with a new dataset. $transformer -> update ( $dataset );","title":"Update Fitting"},{"location":"preprocessing.html#types-of-preprocessing","text":"Here we dive into the different types of data preprocessing that Transformers are capable of.","title":"Types of Preprocessing"},{"location":"preprocessing.html#standardization-and-normalization","text":"Oftentimes, the continuous features of a dataset will be on different scales because they were measured by different methods. For example, age (0 - 100) and income (0 - 9,999,999) are on two widely different scales. Standardization is the processes of transforming a dataset such that the features are all on one common scale. Normalization is the special case where the transformed features have a range between 0 and 1. Depending on the transformer, it may operate on the columns or the rows of the dataset. Transformer Operates Output Range Stateful Elastic L1 Normalizer Row-wise [0, 1] L2 Normalizer Row-wise [0, 1] Max Absolute Scaler Column-wise [-1, 1] \u25cf \u25cf Min Max Normalizer Column-wise [min, max] \u25cf \u25cf Robust Standardizer Column-wise [-\u221e, \u221e] \u25cf Z Scale Standardizer Column-wise [-\u221e, \u221e] \u25cf \u25cf","title":"Standardization and Normalization"},{"location":"preprocessing.html#feature-conversion","text":"Feature converters are transformers that convert feature columns of one data type to another by changing their representation. Transformer From To Stateful Elastic Interval Discretizer Continuous Categorical \u25cf One Hot Encoder Categorical Continuous \u25cf Numeric String Converter Categorical Continuous Boolean Converter Other Categorical or Continuous","title":"Feature Conversion"},{"location":"preprocessing.html#dimensionality-reduction","text":"Dimensionality reduction is a preprocessing technique for projecting a dataset onto a lower dimensional vector space. It allows a learner to train and infer quicker by producing a training set with fewer but more informative features. Dimensionality reducers can also be used to visualize datasets by outputting low (1 - 3) dimensionality embeddings for use in plotting software. Transformer Supervised Stateful Elastic Gaussian Random Projector \u25cf Linear Discriminant Analysis \u25cf \u25cf Principal Component Analysis \u25cf Sparse Random Projector \u25cf Truncated SVD \u25cf t-SNE","title":"Dimensionality Reduction"},{"location":"preprocessing.html#feature-expansion","text":"Feature expansion aims to add flexibility to a model by deriving additional features from a dataset. It can be thought of as the opposite of dimensionality reduction. Transformer Supervised Stateful Elastic Polynomial Expander","title":"Feature Expansion"},{"location":"preprocessing.html#imputation","text":"Imputation is a technique for handling missing values in a dataset by replacing them with a pretty good guess. Transformer Compatibility Supervised Stateful Elastic KNN Imputer Depends on distance kernel \u25cf Missing Data Imputer Categorical, Continuous \u25cf Hot Deck Imputer Depends on distance kernel \u25cf","title":"Imputation"},{"location":"preprocessing.html#natural-language","text":"The library provides a number of transformers for Natural Language Processing (NLP) and Information Retrieval (IR) tasks such as text cleaning, feature extraction, and term weighting. Transformer Supervised Stateful Elastic Regex Filter Text Normalizer Multibyte Text Normalizer Stop Word Filter TF-IDF Transformer \u25cf \u25cf Token Hashing Vectorizer Word Count Vectorizer \u25cf","title":"Natural Language"},{"location":"preprocessing.html#images","text":"These transformers operate on the high-level image data type. Transformer Supervised Stateful Elastic Image Resizer Image Vectorizer \u25cf","title":"Images"},{"location":"preprocessing.html#custom-transformations","text":"In additional to providing specialized Transformers for common preprocessing tasks, the library includes a Lambda Function transformer that allows you to apply custom data transformations using a callback. The callback function accepts a sample passed by reference so that the transformation occurs in-place. In the following example, let's write a callback to binarize the continuous features just at column offset 3 of the dataset. use Rubix\\ML\\Transformers\\LambdaFunction ; $binarize = function ( & $sample ) { $sample [ 3 ] = $sample [ 3 ] > 182 ? 'tall' : 'not tall' ; } $dataset -> apply ( new LambdaFunction ( $binarize )); Another technique we can employ using the Lambda Function transformer is to perform a categorical feature cross between two feature columns of a dataset. A cross feature is a higher-order feature that represents the presence of two or more features simultaneously. For example, we may want to represent the combination of someone's gender and education level as it's own feature. We'll choose to represent the new feature as a CRC32 hash to save on memory and storage but you could just concatenate both categories to represent the new feature as well. use Rubix\\ML\\Transformers\\LambdaFunction ; use function hash ; $crossFeatures = function ( & $sample ) { $sample [] = hash ( 'crc32b' , \" { $sample [ 6 ] } and { $sample [ 7 ] } \" ); }; $dataset -> apply ( new LambdaFunction ( $crossFeatures ));","title":"Custom Transformations"},{"location":"preprocessing.html#advanced-preprocessing","text":"In some cases, certain features of a dataset may require a different set of preprocessing steps than the others. In such a case, we can extract a certain set of features, preprocess them, and then join them with the rest of the dataset later. In the example below, we'll extract just the text reviews and their sentiment labels into a dataset object and put the sample's category, number of clicks, and ratings into another one using two Column Pickers . Then, we can apply a separate set of transformations to each set of features and use the join() method to combine them into a single dataset. We can even apply another set of transformations to the joined dataset after that. use Rubix\\ML\\Dataset\\Labeled ; use Rubix\\ML\\Extractors\\ColumnPicker ; use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Dataset\\Unlabeled ; use Rubix\\ML\\Transformers\\TextNormalizer ; use Rubix\\ML\\Transformers\\WordCountVectorizer ; use Rubix\\ML\\Transformers\\TfIdfTransformer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\ZScaleStandardizer ; $extractor1 = new ColumnPicker ( new NDJSON ( 'example.ndjson' ), [ 'review' , 'sentiment' , ]); $extractor2 = new ColumnPicker ( new NDJSON ( 'example.ndjson' ), [ 'category' , 'clicks' , 'rating' , ]); $dataset1 = Labeled :: fromIterator ( $extractor1 ) -> apply ( new TextNormalizer ()) -> apply ( new WordCountVectorizer ( 5000 )) -> apply ( new TfIdfTransformer ()); $dataset2 = Unlabeled :: fromIterator ( $extractor2 ) -> apply ( new OneHotEncoder ()); $dataset = $dataset1 -> join ( $dataset2 ) -> apply ( new ZScaleStandardizer ());","title":"Advanced Preprocessing"},{"location":"preprocessing.html#transformer-pipelines","text":"The Pipeline meta-estimator helps you automate a series of transformations applied to the input dataset to an estimator. With a Pipeline, any dataset object passed to will automatically be fitted and/or transformed before it arrives in the estimator's context. In addition, transformer fittings can be saved alongside the model data when the Pipeline is persisted. use Rubix\\ML\\Pipeline ; use Rubix\\ML\\Transformers\\HotDeckImputer ; use Rubix\\ML\\Transformers\\OneHotEncoder ; use Rubix\\ML\\Transformers\\ZScaleStandardizer ; use Rubix\\ML\\Clusterers\\KMeans ; $estimator = new Pipeline ([ new HotDeckImputer ( 5 ), new OneHotEncoder (), new ZScaleStandardizer (), ], new KMeans ( 10 )); Calling train() or partial() will result in the transformers being fitted or updated before being passed to the Softmax Classifier. $estimator -> train ( $dataset ); // Transformers fitted and applied $estimator -> partial ( $dataset ); // Transformers updated and applied Any time a dataset is passed to the Pipeline it will automatically be transformed before being handed to the underlying estimator. $predictions = $estimator -> predict ( $dataset ); // Dataset transformed automatically","title":"Transformer Pipelines"},{"location":"preprocessing.html#filtering-records","text":"In some cases, you may want to remove entire rows from the dataset. For example, you may want to remove records that contain features with abnormally low/high values as these samples can be interpreted as noise. The filter() method on the dataset object uses a callback function to determine if a row should be included in the return dataset. In this example, we'll filter all the samples whose value for feature at offset 3 is greater than some amount. $tallPeople = function ( $record ) { return $record [ 3 ] > 178.5 ; }; $dataset = $dataset -> filter ( $tallPeople ); Let's say we wanted to train a classifier with our Labeled dataset but only on a subset of the possible class outcomes. We could filter the samples that correspond to undesirable outcomes by targetting the label with our callback. use function in_array ; $dogsAndCats = function ( $record ) { return in_array ( end ( $record ), [ 'dog' , 'cat' ]); } $training = $dataset -> filter ( $dogsAndCats ); Note For Labeled datasets the label column is always the last column of the record. In the next example, we'll filter all the records that have missing feature values. We can detect missing continuous variables by calling the custom library function iterator_contains_nan() on each record. Additionally, we can filter records with missing categorical values by looking for a special placeholder category, in this case we'll use the value '?' , to denote missing categorical variables. use function Rubix\\ML\\iterator_contains_nan ; use function in_array ; $noMissingValues = function ( $record ) { return ! iterator_contains_nan ( $record ) and ! in_array ( '?' , $record ); }; $complete = $dataset -> filter ( $noMissingValues ); Note The standard PHP library function in_array() does not handle NAN comparisons.","title":"Filtering Records"},{"location":"preprocessing.html#de-duplication","text":"When it is undesirable for a dataset to contain duplicate records, you can remove all duplicates by calling the deduplicate() method on the dataset object. $dataset -> deduplicate (); Note The O(N^2) time complexity of de-duplication may be prohibitive for large datasets.","title":"De-duplication"},{"location":"probabilistic.html","text":"Probabilistic # Estimators that implement the Probabilistic interface have the proba() method that returns an array of joint probability estimates for every possible class or cluster number. Probabilities are useful for ascertaining the degree to which the estimator is certain about a particular outcome. A value of 1 indicates that the estimator is 100% certain about a particular class or cluster number. Conversely, a value of 0 means that the estimator is 100% certain that it's not that class or cluster number. When the probabilities are considered together they are called a joint distribution and always sum to 1. Predict Probabilities # Return the joint probability estimates from a dataset: public proba ( Dataset $dataset ) : array $probabilities = $estimator -> proba ( $dataset ); print_r ( $probabilities ); Array ( [ 0 ] => Array ( [ monster ] => 0.6 [ not monster ] => 0.4 ) [ 1 ] => Array ( [ monster ] => 0.5 [ not monster ] => 0.5 ) [ 2 ] => Array ( [ monster ] => 0.2 [ not monster ] => 0.8 ) )","title":"Probabilistic"},{"location":"probabilistic.html#probabilistic","text":"Estimators that implement the Probabilistic interface have the proba() method that returns an array of joint probability estimates for every possible class or cluster number. Probabilities are useful for ascertaining the degree to which the estimator is certain about a particular outcome. A value of 1 indicates that the estimator is 100% certain about a particular class or cluster number. Conversely, a value of 0 means that the estimator is 100% certain that it's not that class or cluster number. When the probabilities are considered together they are called a joint distribution and always sum to 1.","title":"Probabilistic"},{"location":"probabilistic.html#predict-probabilities","text":"Return the joint probability estimates from a dataset: public proba ( Dataset $dataset ) : array $probabilities = $estimator -> proba ( $dataset ); print_r ( $probabilities ); Array ( [ 0 ] => Array ( [ monster ] => 0.6 [ not monster ] => 0.4 ) [ 1 ] => Array ( [ monster ] => 0.5 [ not monster ] => 0.5 ) [ 2 ] => Array ( [ monster ] => 0.2 [ not monster ] => 0.8 ) )","title":"Predict Probabilities"},{"location":"ranks-features.html","text":"Ranks Features # The Ranks Features interface is for learners that can determine the importances of the features used to train them. Low importance is given to feature columns that do not contribute significantly in the model whereas high importance indicates that the feature is more influential. Feature importances can help explain the predictions derived from a model and can also be used to identify informative features for feature selection. Feature Importances # Return the importance scores of each feature column of the training set: public featureImportances () : array $estimator -> train ( $dataset ); $importances = $estimator -> featureImportances (); print_r ( $importances ); Array ( [ 0 ] => 0.04757 [ 1 ] => 0.37948 [ 2 ] => 0.53170 [ 3 ] => 0.04123 )","title":"Ranks Features"},{"location":"ranks-features.html#ranks-features","text":"The Ranks Features interface is for learners that can determine the importances of the features used to train them. Low importance is given to feature columns that do not contribute significantly in the model whereas high importance indicates that the feature is more influential. Feature importances can help explain the predictions derived from a model and can also be used to identify informative features for feature selection.","title":"Ranks Features"},{"location":"ranks-features.html#feature-importances","text":"Return the importance scores of each feature column of the training set: public featureImportances () : array $estimator -> train ( $dataset ); $importances = $estimator -> featureImportances (); print_r ( $importances ); Array ( [ 0 ] => 0.04757 [ 1 ] => 0.37948 [ 2 ] => 0.53170 [ 3 ] => 0.04123 )","title":"Feature Importances"},{"location":"representing-your-data.html","text":"Representing Your Data # The library makes it easy to work with your data via the Dataset object, which is a specialized data container that every learner can recognize. A dataset is made up of a matrix of samples comprised of features which are usually scalar values. Each sample is a sequentially-ordered array with exactly the same number of elements as every other sample. The columns of the matrix contain the values for a particular feature represented by that column. The dimensionality of a sample is equal to the number of features it has. For example, the samples below are said to be 3-dimensional because they contain 3 feature columns. You'll notice that samples can be made up of a heterogeneous mix of data types. $samples = [ [ 0.1 , 21.5 , 'furry' ], [ 2.0 , - 5 , 'rough' ], [ 0.001 , - 10 , 'rough' ], ]; High-level Data Types # The library comes with a higher-order type system that distinguishes types that are continuous, categorical (discrete), or some other data type. The distinction between types is important for determining the operations that can be performed on a particular feature. Library Type PHP Type Continuous Integer or floating point number Categorical String Image GD Image object or resource Continuous Features # Continuous features represent some quantitative property of the sample and are represented as natural, integer, or real (floating point) numbers. They can be broken down into intervals, ratios, and counts each with their own properties and constraints. One property they all share, however, is that the distances between adjacent values are equal and consistent. Intervals # Intervals are the most general form of continuous measurement and can take on any value within the set of real numbers. Some examples of interval data include temperature in Celsius or Fahrenheit, income, and scores on a personality test. Ratios # Ratios are lower bounded at a fixed zero point. Due to this extra constraint, ratio variables are able to say something about the relative differences between samples by comparing numbers on the scale to absolute zero. Examples of ratio data include height, distance, and temperature in Kelvin. Counts # Count variables are limited to the set of natural (or counting ) numbers and therefore are always non-negative. Categorical Features # Categories are discrete values that describe a qualitative property of a sample such as texture, genre, or political party. They are represented as strings and, unlike continuous features, have no numerical relationship between the values. Categories # Categorical or nominal variables specify which category a sample belongs to among a finite set of choices. For example, a texture feature might include categories such as rough , furry , or smooth . Ordinals # Numeric strings such as '1' and '2' are considered categorical variables in our high-level type system. This conveniently allows you to represent ordinals as ordered categories in which the distances between the levels could be arbitrary. Booleans # A boolean (or binary ) variable is a special case of a categorical variable in which the number of possible categories is strictly two. For example, to denote if a subject is tall or not you can use the tall and not tall categories respectively. Text # Text data are a product of language communication and can be viewed as a dense encoding of many sparse features. Initially, text blobs are imported as categorical features, however, they have little meaning as a category because the features are still encoded. Thus, import text blobs and use a preprocessing step to extract features such as word counts, weighted term frequencies, or word embeddings. Images # Images are represented as either the GD resource type or a GdImage object. An image type is a special type that holds a reference to the data stored within the image file. For this reason, images must eventually be converted to a scalar type, such as the RGB color intensity values of each pixel, before they can be the input to a learning algorithm. Date/Time # There are a number of ways that date/time features can be represented in a dataset. One way is to discretize the value into days, months, and years using categories like june , july , august , and 2020 , 2021 . Date/times can also be represented as continuous features by converting them to a numerical timestamp. What about NULL? # Null values are often used to indicate the absence of a value, however since they do not give any information as to the type of variable that is missing, they cannot be used in a dataset. Instead, represent missing values as either the standard PHP math constant NAN for continuous features or use a special category (such as ? ) to denote missing categorical values.","title":"Representing Your Data"},{"location":"representing-your-data.html#representing-your-data","text":"The library makes it easy to work with your data via the Dataset object, which is a specialized data container that every learner can recognize. A dataset is made up of a matrix of samples comprised of features which are usually scalar values. Each sample is a sequentially-ordered array with exactly the same number of elements as every other sample. The columns of the matrix contain the values for a particular feature represented by that column. The dimensionality of a sample is equal to the number of features it has. For example, the samples below are said to be 3-dimensional because they contain 3 feature columns. You'll notice that samples can be made up of a heterogeneous mix of data types. $samples = [ [ 0.1 , 21.5 , 'furry' ], [ 2.0 , - 5 , 'rough' ], [ 0.001 , - 10 , 'rough' ], ];","title":"Representing Your Data"},{"location":"representing-your-data.html#high-level-data-types","text":"The library comes with a higher-order type system that distinguishes types that are continuous, categorical (discrete), or some other data type. The distinction between types is important for determining the operations that can be performed on a particular feature. Library Type PHP Type Continuous Integer or floating point number Categorical String Image GD Image object or resource","title":"High-level Data Types"},{"location":"representing-your-data.html#continuous-features","text":"Continuous features represent some quantitative property of the sample and are represented as natural, integer, or real (floating point) numbers. They can be broken down into intervals, ratios, and counts each with their own properties and constraints. One property they all share, however, is that the distances between adjacent values are equal and consistent.","title":"Continuous Features"},{"location":"representing-your-data.html#intervals","text":"Intervals are the most general form of continuous measurement and can take on any value within the set of real numbers. Some examples of interval data include temperature in Celsius or Fahrenheit, income, and scores on a personality test.","title":"Intervals"},{"location":"representing-your-data.html#ratios","text":"Ratios are lower bounded at a fixed zero point. Due to this extra constraint, ratio variables are able to say something about the relative differences between samples by comparing numbers on the scale to absolute zero. Examples of ratio data include height, distance, and temperature in Kelvin.","title":"Ratios"},{"location":"representing-your-data.html#counts","text":"Count variables are limited to the set of natural (or counting ) numbers and therefore are always non-negative.","title":"Counts"},{"location":"representing-your-data.html#categorical-features","text":"Categories are discrete values that describe a qualitative property of a sample such as texture, genre, or political party. They are represented as strings and, unlike continuous features, have no numerical relationship between the values.","title":"Categorical Features"},{"location":"representing-your-data.html#categories","text":"Categorical or nominal variables specify which category a sample belongs to among a finite set of choices. For example, a texture feature might include categories such as rough , furry , or smooth .","title":"Categories"},{"location":"representing-your-data.html#ordinals","text":"Numeric strings such as '1' and '2' are considered categorical variables in our high-level type system. This conveniently allows you to represent ordinals as ordered categories in which the distances between the levels could be arbitrary.","title":"Ordinals"},{"location":"representing-your-data.html#booleans","text":"A boolean (or binary ) variable is a special case of a categorical variable in which the number of possible categories is strictly two. For example, to denote if a subject is tall or not you can use the tall and not tall categories respectively.","title":"Booleans"},{"location":"representing-your-data.html#text","text":"Text data are a product of language communication and can be viewed as a dense encoding of many sparse features. Initially, text blobs are imported as categorical features, however, they have little meaning as a category because the features are still encoded. Thus, import text blobs and use a preprocessing step to extract features such as word counts, weighted term frequencies, or word embeddings.","title":"Text"},{"location":"representing-your-data.html#images","text":"Images are represented as either the GD resource type or a GdImage object. An image type is a special type that holds a reference to the data stored within the image file. For this reason, images must eventually be converted to a scalar type, such as the RGB color intensity values of each pixel, before they can be the input to a learning algorithm.","title":"Images"},{"location":"representing-your-data.html#datetime","text":"There are a number of ways that date/time features can be represented in a dataset. One way is to discretize the value into days, months, and years using categories like june , july , august , and 2020 , 2021 . Date/times can also be represented as continuous features by converting them to a numerical timestamp.","title":"Date/Time"},{"location":"representing-your-data.html#what-about-null","text":"Null values are often used to indicate the absence of a value, however since they do not give any information as to the type of variable that is missing, they cannot be used in a dataset. Instead, represent missing values as either the standard PHP math constant NAN for continuous features or use a special category (such as ? ) to denote missing categorical values.","title":"What about NULL?"},{"location":"scoring.html","text":"Scoring # A Scoring anomaly detector is one that assigns anomaly scores to unknown samples in a dataset. The interface provides the score() method which returns a set of scores from the model. Higher scores indicate a greater degree of anomalousness. In addition, samples can be sorted by their anomaly score to identify the top outliers. Score a Dataset # Return the anomaly scores assigned to the samples in a dataset: public score ( Dataset $dataset ) : array $scores = $estimator -> score ( $dataset ); print_r ( $scores ); Array ( [ 0 ] => 0.35033 [ 1 ] => 0.40992 [ 2 ] => 1.68153 )","title":"Scoring"},{"location":"scoring.html#scoring","text":"A Scoring anomaly detector is one that assigns anomaly scores to unknown samples in a dataset. The interface provides the score() method which returns a set of scores from the model. Higher scores indicate a greater degree of anomalousness. In addition, samples can be sorted by their anomaly score to identify the top outliers.","title":"Scoring"},{"location":"scoring.html#score-a-dataset","text":"Return the anomaly scores assigned to the samples in a dataset: public score ( Dataset $dataset ) : array $scores = $estimator -> score ( $dataset ); print_r ( $scores ); Array ( [ 0 ] => 0.35033 [ 1 ] => 0.40992 [ 2 ] => 1.68153 )","title":"Score a Dataset"},{"location":"training.html","text":"Training # Most estimators have the ability to be trained with data. Estimators that require training are called Learners and implement the train() method among others. Training is the process of feeding data to the learner so that it can build an internal representation (or model ). Supervised learners require a Labeled training set. Unsupervised learners can be trained with either a Labeled or Unlabeled dataset but only the information contained within the features are used to build the model. Depending on the size of your dataset and choice of learning algorithm, training can a long time or just a few seconds. We recommend assessing your time (compute) and memory requirements before training large models. To begin training a learner, pass a training Dataset object to the train() method on the learner instance like in the example below. $estimator -> train ( $dataset ); We can verify that a learner has been trained by calling the trained() method which returns true if the estimator is ready to make predictions. var_dump ( $estimator -> trained ()); bool ( true ) Batch vs Online Learning # Batch learning is when a learner is trained in full using only one dataset in a single session. Calling the train() method on a learner instance is an example of batch learning. In contrast, online learning occurs when a learner is trained over multiple sessions with multiple datasets as small as a single sample each. Learners that are capable of being partially trained like this implement the Online interface which includes the partial() method for training with a dataset in an online scheme. Subsequent calls to the partial() method will continue training where the learner left off. Online learning is especially useful for when you have a dataset that is too large to fit into memory all at once or when your dataset is in the form of a stream. $estimator -> train ( $dataset1 ); $estimator -> partial ( $dataset2 ); $estimator -> partial ( $dataset3 ); Note After the initial training, the learner will expect subsequent training sets to contain the same number and order of features. Monitoring Progress # Since training is often an iterative process, it is useful to obtain feedback as to how the learner is progressing in real-time. For example, you may want to monitor the training loss to make sure that it isn't increasing instead of decreasing with training. Such early feedback saves you time by allowing you to abort training early if things aren't going well. Learners that implement the Verbose interface accept a PSR-3 logger instance that can be used to output training information at each time step (or epoch ). The library comes built-in with the Screen logger that does the job for most cases. use Rubix\\ML\\Classifiers\\LogisticRegression ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\Loggers\\Screen ; $estimator = new LogisticRegression ( 128 , new Adam ( 0.01 )); $estimator -> setLogger ( new Screen ()); $estimator -> train ( $dataset ); [2020-09-04 08:39:04] INFO: Logistic Regression (batch_size: 128, optimizer: Adam (rate: 0.01, momentum_decay: 0.1, norm_decay: 0.001), alpha: 0.0001, epochs: 1000, min_change: 0.0001, window: 5, cost_fn: Cross Entropy) initialized [2020-09-04 08:39:04] INFO: Epoch 1 - Cross Entropy: 0.16895133388673 [2020-09-04 08:39:04] INFO: Epoch 2 - Cross Entropy: 0.16559247705179 [2020-09-04 08:39:04] INFO: Epoch 3 - Cross Entropy: 0.16294448401323 [2020-09-04 08:39:04] INFO: Epoch 4 - Cross Entropy: 0.16040135038265 [2020-09-04 08:39:04] INFO: Epoch 5 - Cross Entropy: 0.15786801071483 [2020-09-04 08:39:04] INFO: Epoch 6 - Cross Entropy: 0.1553151426337 [2020-09-04 08:39:04] INFO: Epoch 7 - Cross Entropy: 0.15273253982757 [2020-09-04 08:39:04] INFO: Epoch 8 - Cross Entropy: 0.15011771931339 [2020-09-04 08:39:04] INFO: Epoch 9 - Cross Entropy: 0.14747194148672 [2020-09-04 08:39:04] INFO: Epoch 10 - Cross Entropy: 0.14479847759871 ... [2020-09-04 08:39:04] INFO: Epoch 77 - Cross Entropy: 0.0082096137827592 [2020-09-04 08:39:04] INFO: Epoch 78 - Cross Entropy: 0.0081004235278088 [2020-09-04 08:39:04] INFO: Epoch 79 - Cross Entropy: 0.0079956096838174 [2020-09-04 08:39:04] INFO: Epoch 80 - Cross Entropy: 0.0078948616067878 [2020-09-04 08:39:04] INFO: Epoch 81 - Cross Entropy: 0.0077978960869396 [2020-09-04 08:39:04] INFO: Training complete Parallel Training # Learners that implement the Parallel interface can utilize a parallel processing (multiprocessing) backend for training. Parallel computing can greatly reduce training time on multicore systems at the cost of some overhead to synchronize the data. For small datasets, the overhead may actually cause the runtime to increase. Most parallel learners do not use parallel processing by default, so to enable it you must set a parallel backend using the setBackend() method. In the example below, we'll train a Random Forest classifier with 500 trees in parallel using the Amp backend under the hood. By settings the $workers argument to 4 we tell the backend to use up to 4 cores at a time to execute the computation. use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ClassificationTree ; use Rubix\\ML\\Backends\\Amp ; $estimator = new RandomForest ( new ClassificationTree ( 20 ), 500 ); $estimator -> setBackend ( new Amp ( 4 )); $estimator -> train ( $dataset ); Feature Importances # Learners that implement the Ranks Features interface can evaluate the importance of each feature in the training set. Feature importances are defined as the degree to which an individual feature influences the model. Feature importances are useful for feature selection and for helping to explain predictions derived from a model. To output the importance scores, call the featureImportances() method of a trained learner that implements the Ranks Features interface. use Rubix\\ML\\Regressors\\Ridge ; $estimator = new Ridge (); $estimator -> train ( $dataset ); $importances = $estimator -> featureImportances (); print_r ( $importances ); Array ( [ 0 ] = > 0 .04757 [ 1 ] = > 0 .37948 [ 2 ] = > 0 .53170 [ 3 ] = > 0 .04123 )","title":"Training"},{"location":"training.html#training","text":"Most estimators have the ability to be trained with data. Estimators that require training are called Learners and implement the train() method among others. Training is the process of feeding data to the learner so that it can build an internal representation (or model ). Supervised learners require a Labeled training set. Unsupervised learners can be trained with either a Labeled or Unlabeled dataset but only the information contained within the features are used to build the model. Depending on the size of your dataset and choice of learning algorithm, training can a long time or just a few seconds. We recommend assessing your time (compute) and memory requirements before training large models. To begin training a learner, pass a training Dataset object to the train() method on the learner instance like in the example below. $estimator -> train ( $dataset ); We can verify that a learner has been trained by calling the trained() method which returns true if the estimator is ready to make predictions. var_dump ( $estimator -> trained ()); bool ( true )","title":"Training"},{"location":"training.html#batch-vs-online-learning","text":"Batch learning is when a learner is trained in full using only one dataset in a single session. Calling the train() method on a learner instance is an example of batch learning. In contrast, online learning occurs when a learner is trained over multiple sessions with multiple datasets as small as a single sample each. Learners that are capable of being partially trained like this implement the Online interface which includes the partial() method for training with a dataset in an online scheme. Subsequent calls to the partial() method will continue training where the learner left off. Online learning is especially useful for when you have a dataset that is too large to fit into memory all at once or when your dataset is in the form of a stream. $estimator -> train ( $dataset1 ); $estimator -> partial ( $dataset2 ); $estimator -> partial ( $dataset3 ); Note After the initial training, the learner will expect subsequent training sets to contain the same number and order of features.","title":"Batch vs Online Learning"},{"location":"training.html#monitoring-progress","text":"Since training is often an iterative process, it is useful to obtain feedback as to how the learner is progressing in real-time. For example, you may want to monitor the training loss to make sure that it isn't increasing instead of decreasing with training. Such early feedback saves you time by allowing you to abort training early if things aren't going well. Learners that implement the Verbose interface accept a PSR-3 logger instance that can be used to output training information at each time step (or epoch ). The library comes built-in with the Screen logger that does the job for most cases. use Rubix\\ML\\Classifiers\\LogisticRegression ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\Loggers\\Screen ; $estimator = new LogisticRegression ( 128 , new Adam ( 0.01 )); $estimator -> setLogger ( new Screen ()); $estimator -> train ( $dataset ); [2020-09-04 08:39:04] INFO: Logistic Regression (batch_size: 128, optimizer: Adam (rate: 0.01, momentum_decay: 0.1, norm_decay: 0.001), alpha: 0.0001, epochs: 1000, min_change: 0.0001, window: 5, cost_fn: Cross Entropy) initialized [2020-09-04 08:39:04] INFO: Epoch 1 - Cross Entropy: 0.16895133388673 [2020-09-04 08:39:04] INFO: Epoch 2 - Cross Entropy: 0.16559247705179 [2020-09-04 08:39:04] INFO: Epoch 3 - Cross Entropy: 0.16294448401323 [2020-09-04 08:39:04] INFO: Epoch 4 - Cross Entropy: 0.16040135038265 [2020-09-04 08:39:04] INFO: Epoch 5 - Cross Entropy: 0.15786801071483 [2020-09-04 08:39:04] INFO: Epoch 6 - Cross Entropy: 0.1553151426337 [2020-09-04 08:39:04] INFO: Epoch 7 - Cross Entropy: 0.15273253982757 [2020-09-04 08:39:04] INFO: Epoch 8 - Cross Entropy: 0.15011771931339 [2020-09-04 08:39:04] INFO: Epoch 9 - Cross Entropy: 0.14747194148672 [2020-09-04 08:39:04] INFO: Epoch 10 - Cross Entropy: 0.14479847759871 ... [2020-09-04 08:39:04] INFO: Epoch 77 - Cross Entropy: 0.0082096137827592 [2020-09-04 08:39:04] INFO: Epoch 78 - Cross Entropy: 0.0081004235278088 [2020-09-04 08:39:04] INFO: Epoch 79 - Cross Entropy: 0.0079956096838174 [2020-09-04 08:39:04] INFO: Epoch 80 - Cross Entropy: 0.0078948616067878 [2020-09-04 08:39:04] INFO: Epoch 81 - Cross Entropy: 0.0077978960869396 [2020-09-04 08:39:04] INFO: Training complete","title":"Monitoring Progress"},{"location":"training.html#parallel-training","text":"Learners that implement the Parallel interface can utilize a parallel processing (multiprocessing) backend for training. Parallel computing can greatly reduce training time on multicore systems at the cost of some overhead to synchronize the data. For small datasets, the overhead may actually cause the runtime to increase. Most parallel learners do not use parallel processing by default, so to enable it you must set a parallel backend using the setBackend() method. In the example below, we'll train a Random Forest classifier with 500 trees in parallel using the Amp backend under the hood. By settings the $workers argument to 4 we tell the backend to use up to 4 cores at a time to execute the computation. use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ClassificationTree ; use Rubix\\ML\\Backends\\Amp ; $estimator = new RandomForest ( new ClassificationTree ( 20 ), 500 ); $estimator -> setBackend ( new Amp ( 4 )); $estimator -> train ( $dataset );","title":"Parallel Training"},{"location":"training.html#feature-importances","text":"Learners that implement the Ranks Features interface can evaluate the importance of each feature in the training set. Feature importances are defined as the degree to which an individual feature influences the model. Feature importances are useful for feature selection and for helping to explain predictions derived from a model. To output the importance scores, call the featureImportances() method of a trained learner that implements the Ranks Features interface. use Rubix\\ML\\Regressors\\Ridge ; $estimator = new Ridge (); $estimator -> train ( $dataset ); $importances = $estimator -> featureImportances (); print_r ( $importances ); Array ( [ 0 ] = > 0 .04757 [ 1 ] = > 0 .37948 [ 2 ] = > 0 .53170 [ 3 ] = > 0 .04123 )","title":"Feature Importances"},{"location":"verbose.html","text":"Verbose # Verbose objects are capable of logging important events to any PSR-3 compatible logger such as Monolog , Analog , or the included Screen Logger . Logging is especially useful for monitoring the progress of the underlying learning algorithm in real-time. Set the Logger # To set the logger pass in any PSR-3 compatible logger instance: public setLogger ( LoggerInterface $logger ) : void Return the Logger # Return the logger or null if not set: public logger () : ? LoggerInterface use Rubix\\ML\\Regressors\\Adaline ; use Rubix\\ML\\Loggers\\Screen ; $estimator = new Adaline (); $estimator -> setLogger ( new Screen ( 'example' )); $estimator -> train ( $dataset ); [2020-08-05 04:26:11] INFO: Learner init Adaline {batch_size: 128, optimizer: Adam {rate: 0.01, momentum_decay: 0.1, norm_decay: 0.001}, alpha: 0.0001, epochs: 100, min_change: 0.001, window: 5, cost_fn: Huber Loss {alpha: 1}} [2020-08-05 04:26:11] INFO: Training started [2020-08-05 04:26:11] example.INFO: Epoch 1 - Huber Loss {alpha: 1}: 0.36839299586132 [2020-08-05 04:26:11] example.INFO: Epoch 2 - Huber Loss {alpha: 1}: 0.0018235958273629 [2020-08-05 04:26:11] example.INFO: Epoch 3 - Huber Loss {alpha: 1}: 0.0017358090553563 [2020-08-05 04:26:11] example.INFO: Training complete","title":"Verbose"},{"location":"verbose.html#verbose","text":"Verbose objects are capable of logging important events to any PSR-3 compatible logger such as Monolog , Analog , or the included Screen Logger . Logging is especially useful for monitoring the progress of the underlying learning algorithm in real-time.","title":"Verbose"},{"location":"verbose.html#set-the-logger","text":"To set the logger pass in any PSR-3 compatible logger instance: public setLogger ( LoggerInterface $logger ) : void","title":"Set the Logger"},{"location":"verbose.html#return-the-logger","text":"Return the logger or null if not set: public logger () : ? LoggerInterface use Rubix\\ML\\Regressors\\Adaline ; use Rubix\\ML\\Loggers\\Screen ; $estimator = new Adaline (); $estimator -> setLogger ( new Screen ( 'example' )); $estimator -> train ( $dataset ); [2020-08-05 04:26:11] INFO: Learner init Adaline {batch_size: 128, optimizer: Adam {rate: 0.01, momentum_decay: 0.1, norm_decay: 0.001}, alpha: 0.0001, epochs: 100, min_change: 0.001, window: 5, cost_fn: Huber Loss {alpha: 1}} [2020-08-05 04:26:11] INFO: Training started [2020-08-05 04:26:11] example.INFO: Epoch 1 - Huber Loss {alpha: 1}: 0.36839299586132 [2020-08-05 04:26:11] example.INFO: Epoch 2 - Huber Loss {alpha: 1}: 0.0018235958273629 [2020-08-05 04:26:11] example.INFO: Epoch 3 - Huber Loss {alpha: 1}: 0.0017358090553563 [2020-08-05 04:26:11] example.INFO: Training complete","title":"Return the Logger"},{"location":"what-is-machine-learning.html","text":"What is Machine Learning? # Machine Learning (ML) is a form of Artificial Intelligence (AI) that uses data to train a computer to perform tasks. Unlike traditional programming, in which rules are programmed explicitly, machine learning uses algorithms to build rulesets automatically. At a high level, machine learning is a collection of techniques borrowed from many disciplines including statistics, probability theory, and neuroscience combined with novel ideas for the purpose of gaining insight through data and computation. Machine Learning is further broken down into subcategories based on how the learners are trained and the tasks they handle. Supervised Learning # Supervised learning is a type of machine learning that incorporates a training signal in the form of labels which are often determined by a human expert. Labels are the desired output of a learner given the sample we are showing it. For this reason, you can think of supervised leaning as learning by example . There are two types of supervised learning to consider in Rubix ML. Classification # For classification problems, a learner is trained to differentiate samples among a set of k possible discrete classes. In this type of problem, the training labels are the classes that each sample belongs to. Examples of class labels include cat , dog , human , or any other categorical label. Classification problems include image recognition , text sentiment analysis , and Iris flower classification . Regression # Regression is a learning problem that aims to predict a continuous-valued outcome. In this case, the training labels are continuous data types such as integers and floating point numbers. Regression problems include estimating house prices , credit scoring, and the steering angle of an autonomous vehicle. Unsupervised Learning # A form of learning that does not require labeled data is called Unsupervised learning. Unsupervised learners focus on digesting patterns within just the samples. There are three types of unsupervised learning offered in Rubix ML. Clustering # Clustering takes a dataset and assigns each of the samples a discrete cluster number based on its similarity to other samples from the training set. It can be viewed as a weaker form of classification where the class names are unknown. Clustering is used to group colors , segment customer databases, and to discover communities within social networks for example. Anomaly Detection # Anomalies are defined as samples that have been generated by a different process than normal. Samples can either be flagged or ranked based on their anomaly score. Anomaly detection is used in information security for intrusion and denial of service detection, and in the financial industry to detect fraud. Manifold Learning # Manifold learning is a type of unsupervised non-linear dimensionality reduction used for embedding datasets into dense feature representations. Embedders can be used for visualizing high dimensional (4 or more) datasets in low (1 to 3) dimensions, or for compressing the information within the samples before input to a learning algorithm. Deep Learning # Deep Learning is a subset of machine learning that incorporates layers of computation that form feature representations of greater and greater complexity. It is a paradigm shift from feature engineering to letting the learner construct its own features from the raw data. Deep Learning is used in image recognition, natural language processing (NLP), and for other tasks involving very high-dimensional raw inputs. AutoML # Automated Machine Learning (AutoML) is the application of automated tools when designing machine learning models. The goal of AutoML is to simplify the machine learning lifecycle for non-experts and to facilitate rapid prototyping. In addition, AutoML can aid in the discovery of simpler and more accurate solutions than could otherwise be discovered by human intuition alone. Rubix provides a number of tools to help automate the machine learning process including hyper-parameter optimizers and feature selectors . Other Forms of ML # Although the supervised and unsupervised learning framework covers a substantial number of problems, there are other types of machine learning that the library does not support out of the box. Reinforcement Learning # Reinforcement Learning (RL) is a type of machine learning that aims to learn the optimal control of an agent within an environment through cumulative reward. The data used to train an RL learner are the states obtained by performing some action and then observing the response. If supervised learning is learning by example then reinforcement learning is learning from mistakes . Reinforcement learning is used to train AIs to play games such as Go, Chess, and Starcraft 2, and in robotics for movement planning . Sequence Learning # Sequence Learning is a type of ML that aims to predict the next value in a sequence such as the next word in a sentence or a future stock price. It differs from learning from sets of data in that the order of the samples matter. Time-series analysis is a special case of sequence learning where the sequences are ordered by time. Sequence-to-sequence Learning is used to denote when the output is not just the next value but the next sequence of values. Self-supervised Learning # A hybrid approach to learning is Self-supervised learning in which a learner is trained to predict the parts of a sample that were partially omitted during training. As such, supervised methods can be employed on unlabeled data to learn representations. Self-supervised learning is used in language models such as GPT-3 to generate sequences of text or in autonomous robots to learn from ancillary sensor feedback.","title":"What is Machine Learning?"},{"location":"what-is-machine-learning.html#what-is-machine-learning","text":"Machine Learning (ML) is a form of Artificial Intelligence (AI) that uses data to train a computer to perform tasks. Unlike traditional programming, in which rules are programmed explicitly, machine learning uses algorithms to build rulesets automatically. At a high level, machine learning is a collection of techniques borrowed from many disciplines including statistics, probability theory, and neuroscience combined with novel ideas for the purpose of gaining insight through data and computation. Machine Learning is further broken down into subcategories based on how the learners are trained and the tasks they handle.","title":"What is Machine Learning?"},{"location":"what-is-machine-learning.html#supervised-learning","text":"Supervised learning is a type of machine learning that incorporates a training signal in the form of labels which are often determined by a human expert. Labels are the desired output of a learner given the sample we are showing it. For this reason, you can think of supervised leaning as learning by example . There are two types of supervised learning to consider in Rubix ML.","title":"Supervised Learning"},{"location":"what-is-machine-learning.html#classification","text":"For classification problems, a learner is trained to differentiate samples among a set of k possible discrete classes. In this type of problem, the training labels are the classes that each sample belongs to. Examples of class labels include cat , dog , human , or any other categorical label. Classification problems include image recognition , text sentiment analysis , and Iris flower classification .","title":"Classification"},{"location":"what-is-machine-learning.html#regression","text":"Regression is a learning problem that aims to predict a continuous-valued outcome. In this case, the training labels are continuous data types such as integers and floating point numbers. Regression problems include estimating house prices , credit scoring, and the steering angle of an autonomous vehicle.","title":"Regression"},{"location":"what-is-machine-learning.html#unsupervised-learning","text":"A form of learning that does not require labeled data is called Unsupervised learning. Unsupervised learners focus on digesting patterns within just the samples. There are three types of unsupervised learning offered in Rubix ML.","title":"Unsupervised Learning"},{"location":"what-is-machine-learning.html#clustering","text":"Clustering takes a dataset and assigns each of the samples a discrete cluster number based on its similarity to other samples from the training set. It can be viewed as a weaker form of classification where the class names are unknown. Clustering is used to group colors , segment customer databases, and to discover communities within social networks for example.","title":"Clustering"},{"location":"what-is-machine-learning.html#anomaly-detection","text":"Anomalies are defined as samples that have been generated by a different process than normal. Samples can either be flagged or ranked based on their anomaly score. Anomaly detection is used in information security for intrusion and denial of service detection, and in the financial industry to detect fraud.","title":"Anomaly Detection"},{"location":"what-is-machine-learning.html#manifold-learning","text":"Manifold learning is a type of unsupervised non-linear dimensionality reduction used for embedding datasets into dense feature representations. Embedders can be used for visualizing high dimensional (4 or more) datasets in low (1 to 3) dimensions, or for compressing the information within the samples before input to a learning algorithm.","title":"Manifold Learning"},{"location":"what-is-machine-learning.html#deep-learning","text":"Deep Learning is a subset of machine learning that incorporates layers of computation that form feature representations of greater and greater complexity. It is a paradigm shift from feature engineering to letting the learner construct its own features from the raw data. Deep Learning is used in image recognition, natural language processing (NLP), and for other tasks involving very high-dimensional raw inputs.","title":"Deep Learning"},{"location":"what-is-machine-learning.html#automl","text":"Automated Machine Learning (AutoML) is the application of automated tools when designing machine learning models. The goal of AutoML is to simplify the machine learning lifecycle for non-experts and to facilitate rapid prototyping. In addition, AutoML can aid in the discovery of simpler and more accurate solutions than could otherwise be discovered by human intuition alone. Rubix provides a number of tools to help automate the machine learning process including hyper-parameter optimizers and feature selectors .","title":"AutoML"},{"location":"what-is-machine-learning.html#other-forms-of-ml","text":"Although the supervised and unsupervised learning framework covers a substantial number of problems, there are other types of machine learning that the library does not support out of the box.","title":"Other Forms of ML"},{"location":"what-is-machine-learning.html#reinforcement-learning","text":"Reinforcement Learning (RL) is a type of machine learning that aims to learn the optimal control of an agent within an environment through cumulative reward. The data used to train an RL learner are the states obtained by performing some action and then observing the response. If supervised learning is learning by example then reinforcement learning is learning from mistakes . Reinforcement learning is used to train AIs to play games such as Go, Chess, and Starcraft 2, and in robotics for movement planning .","title":"Reinforcement Learning"},{"location":"what-is-machine-learning.html#sequence-learning","text":"Sequence Learning is a type of ML that aims to predict the next value in a sequence such as the next word in a sentence or a future stock price. It differs from learning from sets of data in that the order of the samples matter. Time-series analysis is a special case of sequence learning where the sequences are ordered by time. Sequence-to-sequence Learning is used to denote when the output is not just the next value but the next sequence of values.","title":"Sequence Learning"},{"location":"what-is-machine-learning.html#self-supervised-learning","text":"A hybrid approach to learning is Self-supervised learning in which a learner is trained to predict the parts of a sample that were partially omitted during training. As such, supervised methods can be employed on unlabeled data to learn representations. Self-supervised learning is used in language models such as GPT-3 to generate sequences of text or in autonomous robots to learn from ancillary sensor feedback.","title":"Self-supervised Learning"},{"location":"anomaly-detectors/gaussian-mle.html","text":"[source] Gaussian MLE # The Gaussian Maximum Likelihood Estimator (MLE) is able to spot outliers by computing a probability density function (PDF) over the features assuming they are independently and normally (Gaussian) distributed. Samples that are assigned low probability density are more likely to be outliers. Interfaces: Estimator , Learner , Online , Scoring , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 contamination 0.1 float The proportion of outliers that are assumed to be present in the training set. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature. Example # use Rubix\\ML\\AnomalyDetectors\\GaussianMLE ; $estimator = new GaussianMLE ( 0.03 , 1e-8 ); Additional Methods # Return the column means computed from the training set: public means () : float [] Return the column variances computed from the training set: public variances () : float [] References # T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances. \u21a9","title":"Gaussian MLE"},{"location":"anomaly-detectors/gaussian-mle.html#gaussian-mle","text":"The Gaussian Maximum Likelihood Estimator (MLE) is able to spot outliers by computing a probability density function (PDF) over the features assuming they are independently and normally (Gaussian) distributed. Samples that are assigned low probability density are more likely to be outliers. Interfaces: Estimator , Learner , Online , Scoring , Persistable Data Type Compatibility: Continuous","title":"Gaussian MLE"},{"location":"anomaly-detectors/gaussian-mle.html#parameters","text":"# Name Default Type Description 1 contamination 0.1 float The proportion of outliers that are assumed to be present in the training set. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature.","title":"Parameters"},{"location":"anomaly-detectors/gaussian-mle.html#example","text":"use Rubix\\ML\\AnomalyDetectors\\GaussianMLE ; $estimator = new GaussianMLE ( 0.03 , 1e-8 );","title":"Example"},{"location":"anomaly-detectors/gaussian-mle.html#additional-methods","text":"Return the column means computed from the training set: public means () : float [] Return the column variances computed from the training set: public variances () : float []","title":"Additional Methods"},{"location":"anomaly-detectors/gaussian-mle.html#references","text":"T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances. \u21a9","title":"References"},{"location":"anomaly-detectors/isolation-forest.html","text":"[source] Isolation Forest # An ensemble of Isolation Trees that each specialize on a unique subset of the training set. Isolation Trees are a type of randomized decision tree that assign anomaly scores based on the depth a sample reaches when traversing the tree. Based on the premise that anomalies are isolated into their own nodes sooner, samples that receive high anomaly scores achieve the shallowest depth during traversal. Interfaces: Estimator , Learner , Scoring , Persistable Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 estimators 100 int The number of isolation trees to train in the ensemble. 2 ratio null float The ratio of samples to train each estimator with. If null, the subsample size will be set to 256. 3 contamination null float The proportion of outliers that are assumed to be present in the training set. If null, the threshold anomaly score will be set to 0.5. Example # use Rubix\\ML\\AnomalyDetectors\\IsolationForest ; $estimator = new IsolationForest ( 100 , 0.2 , 0.05 ); Additional Methods # This estimator does not have any additional methods. References # F. T. Liu et al. (2008). Isolation Forest. \u21a9 F. T. Liu et al. (2011). Isolation-based Anomaly Detection. \u21a9 M. Garchery et al. (2018). On the influence of categorical features in ranking anomalies using mixed data. \u21a9","title":"Isolation Forest"},{"location":"anomaly-detectors/isolation-forest.html#isolation-forest","text":"An ensemble of Isolation Trees that each specialize on a unique subset of the training set. Isolation Trees are a type of randomized decision tree that assign anomaly scores based on the depth a sample reaches when traversing the tree. Based on the premise that anomalies are isolated into their own nodes sooner, samples that receive high anomaly scores achieve the shallowest depth during traversal. Interfaces: Estimator , Learner , Scoring , Persistable Data Type Compatibility: Categorical, Continuous","title":"Isolation Forest"},{"location":"anomaly-detectors/isolation-forest.html#parameters","text":"# Name Default Type Description 1 estimators 100 int The number of isolation trees to train in the ensemble. 2 ratio null float The ratio of samples to train each estimator with. If null, the subsample size will be set to 256. 3 contamination null float The proportion of outliers that are assumed to be present in the training set. If null, the threshold anomaly score will be set to 0.5.","title":"Parameters"},{"location":"anomaly-detectors/isolation-forest.html#example","text":"use Rubix\\ML\\AnomalyDetectors\\IsolationForest ; $estimator = new IsolationForest ( 100 , 0.2 , 0.05 );","title":"Example"},{"location":"anomaly-detectors/isolation-forest.html#additional-methods","text":"This estimator does not have any additional methods.","title":"Additional Methods"},{"location":"anomaly-detectors/isolation-forest.html#references","text":"F. T. Liu et al. (2008). Isolation Forest. \u21a9 F. T. Liu et al. (2011). Isolation-based Anomaly Detection. \u21a9 M. Garchery et al. (2018). On the influence of categorical features in ranking anomalies using mixed data. \u21a9","title":"References"},{"location":"anomaly-detectors/local-outlier-factor.html","text":"[source] Local Outlier Factor # Local Outlier Factor (LOF) measures the local deviation of density of an unknown sample with respect to its k nearest neighbors from the training set. As such, LOF only considers the neighborhood of an unknown sample which enables it to detect anomalies within individual clusters of data. Interfaces: Estimator , Learner , Scoring , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 20 int The k nearest neighbors that form a local region. 2 contamination null float The proportion of outliers that are assumed to be present in the training set. 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches. Example # use Rubix\\ML\\AnomalyDetectors\\LocalOutlierFactor ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $estimator = new LocalOutlierFactor ( 20 , 0.1 , new BallTree ( 30 , new Euclidean )); Additional Methods # Return the base spatial tree instance: public tree () : Spatial References # M. M. Breunig et al. (2000). LOF: Identifying Density-Based Local Outliers. \u21a9","title":"Local Outlier Factor"},{"location":"anomaly-detectors/local-outlier-factor.html#local-outlier-factor","text":"Local Outlier Factor (LOF) measures the local deviation of density of an unknown sample with respect to its k nearest neighbors from the training set. As such, LOF only considers the neighborhood of an unknown sample which enables it to detect anomalies within individual clusters of data. Interfaces: Estimator , Learner , Scoring , Persistable Data Type Compatibility: Depends on distance kernel","title":"Local Outlier Factor"},{"location":"anomaly-detectors/local-outlier-factor.html#parameters","text":"# Name Default Type Description 1 k 20 int The k nearest neighbors that form a local region. 2 contamination null float The proportion of outliers that are assumed to be present in the training set. 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches.","title":"Parameters"},{"location":"anomaly-detectors/local-outlier-factor.html#example","text":"use Rubix\\ML\\AnomalyDetectors\\LocalOutlierFactor ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $estimator = new LocalOutlierFactor ( 20 , 0.1 , new BallTree ( 30 , new Euclidean ));","title":"Example"},{"location":"anomaly-detectors/local-outlier-factor.html#additional-methods","text":"Return the base spatial tree instance: public tree () : Spatial","title":"Additional Methods"},{"location":"anomaly-detectors/local-outlier-factor.html#references","text":"M. M. Breunig et al. (2000). LOF: Identifying Density-Based Local Outliers. \u21a9","title":"References"},{"location":"anomaly-detectors/loda.html","text":"[source] Loda # Lightweight Online Detector of Anomalies uses a collection of sparse random projection vectors to provide scalar inputs to an ensemble of unique one-dimensional equi-width histograms. Each histogram then estimates the probability density of the unknown sample using a limited feature set. The final predictions are derived from the averaged densities over the entire ensemble. Interfaces: Estimator , Learner , Online , Scoring , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 estimators 100 int The number of projection/histogram pairs in the ensemble. 2 bins null int The number of equi-width bins for each histogram. If null then will estimate bin count. 3 contamination 0.1 float The proportion of outliers that are assumed to be present in the training set. Example # use Rubix\\ML\\AnomalyDetectors\\Loda ; $estimator = new Loda ( 250 , 8 , 0.01 ); Additional Methods # This estimator does not have any additional methods. References # T. Pevn\u00fd. (2015). Loda: Lightweight on-line detector of anomalies. \u21a9 L. Birg\u00b4e et al. (2005). How Many Bins Should Be Put In A Regular Histogram. \u21a9","title":"Loda"},{"location":"anomaly-detectors/loda.html#loda","text":"Lightweight Online Detector of Anomalies uses a collection of sparse random projection vectors to provide scalar inputs to an ensemble of unique one-dimensional equi-width histograms. Each histogram then estimates the probability density of the unknown sample using a limited feature set. The final predictions are derived from the averaged densities over the entire ensemble. Interfaces: Estimator , Learner , Online , Scoring , Persistable Data Type Compatibility: Continuous","title":"Loda"},{"location":"anomaly-detectors/loda.html#parameters","text":"# Name Default Type Description 1 estimators 100 int The number of projection/histogram pairs in the ensemble. 2 bins null int The number of equi-width bins for each histogram. If null then will estimate bin count. 3 contamination 0.1 float The proportion of outliers that are assumed to be present in the training set.","title":"Parameters"},{"location":"anomaly-detectors/loda.html#example","text":"use Rubix\\ML\\AnomalyDetectors\\Loda ; $estimator = new Loda ( 250 , 8 , 0.01 );","title":"Example"},{"location":"anomaly-detectors/loda.html#additional-methods","text":"This estimator does not have any additional methods.","title":"Additional Methods"},{"location":"anomaly-detectors/loda.html#references","text":"T. Pevn\u00fd. (2015). Loda: Lightweight on-line detector of anomalies. \u21a9 L. Birg\u00b4e et al. (2005). How Many Bins Should Be Put In A Regular Histogram. \u21a9","title":"References"},{"location":"anomaly-detectors/one-class-svm.html","text":"[source] One Class SVM # An unsupervised Support Vector Machine (SVM) used for anomaly detection. The One Class SVM aims to find a maximum margin between a set of data points and the origin , rather than between classes such as with SVC . Note This estimator requires the SVM extension which uses the libsvm engine under the hood. Interfaces: Estimator , Learner Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 nu 0.1 float An upper bound on the percentage of margin errors and a lower bound on the percentage of support vectors. 2 kernel RBF Kernel The kernel function used to express non-linear data in higher dimensions. 3 shrinking true bool Should we use the shrinking heuristic? 4 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 5 cacheSize 100.0 float The size of the kernel cache in MB. Example # use Rubix\\ML\\AnomalyDetectors\\OneClassSVM ; use Rubix\\ML\\Kernels\\SVM\\Polynomial ; $estimator = new OneClassSVM ( 0.1 , new Polynomial ( 4 ), true , 1e-3 , 100.0 ); Additional Methods # Save the model data to the filesystem: public save ( string $path ) : void Load the model data from the filesystem: public load ( string $path ) : void References # C. Chang et al. (2011). LIBSVM: A library for support vector machines. \u21a9","title":"One Class SVM"},{"location":"anomaly-detectors/one-class-svm.html#one-class-svm","text":"An unsupervised Support Vector Machine (SVM) used for anomaly detection. The One Class SVM aims to find a maximum margin between a set of data points and the origin , rather than between classes such as with SVC . Note This estimator requires the SVM extension which uses the libsvm engine under the hood. Interfaces: Estimator , Learner Data Type Compatibility: Continuous","title":"One Class SVM"},{"location":"anomaly-detectors/one-class-svm.html#parameters","text":"# Name Default Type Description 1 nu 0.1 float An upper bound on the percentage of margin errors and a lower bound on the percentage of support vectors. 2 kernel RBF Kernel The kernel function used to express non-linear data in higher dimensions. 3 shrinking true bool Should we use the shrinking heuristic? 4 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 5 cacheSize 100.0 float The size of the kernel cache in MB.","title":"Parameters"},{"location":"anomaly-detectors/one-class-svm.html#example","text":"use Rubix\\ML\\AnomalyDetectors\\OneClassSVM ; use Rubix\\ML\\Kernels\\SVM\\Polynomial ; $estimator = new OneClassSVM ( 0.1 , new Polynomial ( 4 ), true , 1e-3 , 100.0 );","title":"Example"},{"location":"anomaly-detectors/one-class-svm.html#additional-methods","text":"Save the model data to the filesystem: public save ( string $path ) : void Load the model data from the filesystem: public load ( string $path ) : void","title":"Additional Methods"},{"location":"anomaly-detectors/one-class-svm.html#references","text":"C. Chang et al. (2011). LIBSVM: A library for support vector machines. \u21a9","title":"References"},{"location":"anomaly-detectors/robust-z-score.html","text":"[source] Robust Z-Score # A statistical anomaly detector that uses modified Z-Scores that are robust to preexisting outliers in the training set. The modified Z-Score is defined as the feature value minus the median over the median absolute deviation (MAD). Anomalies are flagged if their final weighted Z-Score exceeds a user-defined threshold. Note A beta value of 1 means the estimator only considers the maximum absolute Z-Score, whereas a setting of 0 indicates that only the average Z-Score factors into the final score. Interfaces: Estimator , Learner , Scoring , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 threshold 3.5 float The minimum Z-Score to be flagged as an anomaly. 2 beta 0.5 float The weight of the maximum Z-Score in the overall anomaly score. 3 smoothing 1e-9 float The amount of epsilon smoothing added to the MAD of each feature. Example # use Rubix\\ML\\AnomalyDetectors\\RobustZScore ; $estimator = new RobustZScore ( 3.5 , 0.25 , 1e-6 ); Additional Methods # Return the median of each feature column in the training set: public medians () : float [] | null Return the median absolute deviation (MAD) of each feature column in the training set: public mads () : float [] | null References # B. Iglewicz et al. (1993). How to Detect and Handle Outliers. \u21a9","title":"Robust Z-Score"},{"location":"anomaly-detectors/robust-z-score.html#robust-z-score","text":"A statistical anomaly detector that uses modified Z-Scores that are robust to preexisting outliers in the training set. The modified Z-Score is defined as the feature value minus the median over the median absolute deviation (MAD). Anomalies are flagged if their final weighted Z-Score exceeds a user-defined threshold. Note A beta value of 1 means the estimator only considers the maximum absolute Z-Score, whereas a setting of 0 indicates that only the average Z-Score factors into the final score. Interfaces: Estimator , Learner , Scoring , Persistable Data Type Compatibility: Continuous","title":"Robust Z-Score"},{"location":"anomaly-detectors/robust-z-score.html#parameters","text":"# Name Default Type Description 1 threshold 3.5 float The minimum Z-Score to be flagged as an anomaly. 2 beta 0.5 float The weight of the maximum Z-Score in the overall anomaly score. 3 smoothing 1e-9 float The amount of epsilon smoothing added to the MAD of each feature.","title":"Parameters"},{"location":"anomaly-detectors/robust-z-score.html#example","text":"use Rubix\\ML\\AnomalyDetectors\\RobustZScore ; $estimator = new RobustZScore ( 3.5 , 0.25 , 1e-6 );","title":"Example"},{"location":"anomaly-detectors/robust-z-score.html#additional-methods","text":"Return the median of each feature column in the training set: public medians () : float [] | null Return the median absolute deviation (MAD) of each feature column in the training set: public mads () : float [] | null","title":"Additional Methods"},{"location":"anomaly-detectors/robust-z-score.html#references","text":"B. Iglewicz et al. (1993). How to Detect and Handle Outliers. \u21a9","title":"References"},{"location":"backends/amp.html","text":"[source] Amp # Amp Parallel is a multiprocessing subsystem that requires no extensions. It uses a non-blocking concurrency framework that implements coroutines using PHP generator functions under the hood. Note The optimal number of workers will depend on the system specifications of the computer. Fewer workers than CPU cores may not achieve full processing potential but more workers than cores can cause excess overhead. Parameters # # Name Default Type Description 1 workers Auto int The maximum number of workers in the worker pool. If null then tries to autodetect CPU core count. Example # use Rubix\\ML\\Backends\\Amp ; $backend = new Amp ( 16 ); Additional Methods # Return the maximum number of workers in the worker pool: public workers () : int","title":"Amp"},{"location":"backends/amp.html#amp","text":"Amp Parallel is a multiprocessing subsystem that requires no extensions. It uses a non-blocking concurrency framework that implements coroutines using PHP generator functions under the hood. Note The optimal number of workers will depend on the system specifications of the computer. Fewer workers than CPU cores may not achieve full processing potential but more workers than cores can cause excess overhead.","title":"Amp"},{"location":"backends/amp.html#parameters","text":"# Name Default Type Description 1 workers Auto int The maximum number of workers in the worker pool. If null then tries to autodetect CPU core count.","title":"Parameters"},{"location":"backends/amp.html#example","text":"use Rubix\\ML\\Backends\\Amp ; $backend = new Amp ( 16 );","title":"Example"},{"location":"backends/amp.html#additional-methods","text":"Return the maximum number of workers in the worker pool: public workers () : int","title":"Additional Methods"},{"location":"backends/serial.html","text":"[source] Serial # The Serial backend executes tasks sequentially inside of a single process. The advantage of the Serial backend is that it has zero overhead, thus it may be faster than a parallel backend for small datasets. Note The Serial backend is the default for most objects that are capable of parallel processing. Parameters # This backend does not have any additional parameters. Example # use Rubix\\ML\\Backends\\Serial ; $backend = new Serial (); Additional Methods # This backend does not have any additional methods.","title":"Serial"},{"location":"backends/serial.html#serial","text":"The Serial backend executes tasks sequentially inside of a single process. The advantage of the Serial backend is that it has zero overhead, thus it may be faster than a parallel backend for small datasets. Note The Serial backend is the default for most objects that are capable of parallel processing.","title":"Serial"},{"location":"backends/serial.html#parameters","text":"This backend does not have any additional parameters.","title":"Parameters"},{"location":"backends/serial.html#example","text":"use Rubix\\ML\\Backends\\Serial ; $backend = new Serial ();","title":"Example"},{"location":"backends/serial.html#additional-methods","text":"This backend does not have any additional methods.","title":"Additional Methods"},{"location":"classifiers/adaboost.html","text":"[source] AdaBoost # Short for Adaptive Boosting , this ensemble classifier can improve the performance of an otherwise weak classifier by focusing more attention on samples that are harder to classify. It builds an additive model where, at each stage, a new learner is trained and given an influence score inversely proportional to the loss it incurs at that epoch. Note The default base learner is a Classification Tree with a max height of 1 i.e a Decision Stump . Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Depends on base learner Parameters # # Name Default Type Description 1 base ClassificationTree Learner The base weak classifier to be boosted. 2 rate 1.0 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.8 float The ratio of samples to subsample from the training set to train each weak learner. 4 epochs 100 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. Example # use Rubix\\ML\\Classifiers\\AdaBoost ; use Rubix\\ML\\Classifiers\\ExtraTreeClassifier ; $estimator = new AdaBoost ( new ExtraTreeClassifier ( 3 ), 0.1 , 0.5 , 200 , 1e-3 , 10 ); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null References # Y. Freund et al. (1996). A Decision-theoretic Generalization of On-line Learning and an Application to Boosting. \u21a9 J. Zhu et al. (2006). Multi-class AdaBoost. \u21a9","title":"AdaBoost"},{"location":"classifiers/adaboost.html#adaboost","text":"Short for Adaptive Boosting , this ensemble classifier can improve the performance of an otherwise weak classifier by focusing more attention on samples that are harder to classify. It builds an additive model where, at each stage, a new learner is trained and given an influence score inversely proportional to the loss it incurs at that epoch. Note The default base learner is a Classification Tree with a max height of 1 i.e a Decision Stump . Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Depends on base learner","title":"AdaBoost"},{"location":"classifiers/adaboost.html#parameters","text":"# Name Default Type Description 1 base ClassificationTree Learner The base weak classifier to be boosted. 2 rate 1.0 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.8 float The ratio of samples to subsample from the training set to train each weak learner. 4 epochs 100 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop.","title":"Parameters"},{"location":"classifiers/adaboost.html#example","text":"use Rubix\\ML\\Classifiers\\AdaBoost ; use Rubix\\ML\\Classifiers\\ExtraTreeClassifier ; $estimator = new AdaBoost ( new ExtraTreeClassifier ( 3 ), 0.1 , 0.5 , 200 , 1e-3 , 10 );","title":"Example"},{"location":"classifiers/adaboost.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null","title":"Additional Methods"},{"location":"classifiers/adaboost.html#references","text":"Y. Freund et al. (1996). A Decision-theoretic Generalization of On-line Learning and an Application to Boosting. \u21a9 J. Zhu et al. (2006). Multi-class AdaBoost. \u21a9","title":"References"},{"location":"classifiers/classification-tree.html","text":"[source] Classification Tree # A binary tree-based learner that greedily constructs a decision map for classification that minimizes the Gini impurity among the training labels within the leaf nodes. The height and bushiness of the tree can be determined by the user-defined max height and max leaf size hyper-parameters. Classification Trees also serve as the base learner of ensemble methods such as Random Forest and AdaBoost . Interfaces: Estimator , Learner , Probabilistic , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. 5 maxBins Auto int The maximum number of bins to consider when determining a split with a continuous feature as the split point. Example # use Rubix\\ML\\Classifiers\\ClassificationTree ; $estimator = new ClassificationTree ( 10 , 5 , 0.001 , null , null ); Additional Methods # Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int References: # W. Y. Loh. (2011). Classification and Regression Trees. \u21a9 K. Alsabti. et al. (1998). CLOUDS: A Decision Tree Classifier for Large Datasets. \u21a9","title":"Classification Tree"},{"location":"classifiers/classification-tree.html#classification-tree","text":"A binary tree-based learner that greedily constructs a decision map for classification that minimizes the Gini impurity among the training labels within the leaf nodes. The height and bushiness of the tree can be determined by the user-defined max height and max leaf size hyper-parameters. Classification Trees also serve as the base learner of ensemble methods such as Random Forest and AdaBoost . Interfaces: Estimator , Learner , Probabilistic , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous","title":"Classification Tree"},{"location":"classifiers/classification-tree.html#parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. 5 maxBins Auto int The maximum number of bins to consider when determining a split with a continuous feature as the split point.","title":"Parameters"},{"location":"classifiers/classification-tree.html#example","text":"use Rubix\\ML\\Classifiers\\ClassificationTree ; $estimator = new ClassificationTree ( 10 , 5 , 0.001 , null , null );","title":"Example"},{"location":"classifiers/classification-tree.html#additional-methods","text":"Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int","title":"Additional Methods"},{"location":"classifiers/classification-tree.html#references","text":"W. Y. Loh. (2011). Classification and Regression Trees. \u21a9 K. Alsabti. et al. (1998). CLOUDS: A Decision Tree Classifier for Large Datasets. \u21a9","title":"References:"},{"location":"classifiers/extra-tree-classifier.html","text":"[source] Extra Tree Classifier # An Extremely Randomized Classification Tree that recursively chooses node splits with the least entropy among a set of k (given by max features) random split points. Extra Trees are useful in ensembles such as Random Forest or AdaBoost as the weak learner or they can be used on their own. The strength of Extra Trees as compared to standard decision trees are their computational efficiency and lower prediction variance. Interfaces: Estimator , Learner , Probabilistic , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. Example # use Rubix\\ML\\Classifiers\\ExtraTreeClassifier ; $estimator = new ExtraTreeClassifier ( 50 , 3 , 1e-7 , 10 ); Additional Methods # Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int References # P. Geurts et al. (2005). Extremely Randomized Trees. \u21a9","title":"Extra Tree Classifier"},{"location":"classifiers/extra-tree-classifier.html#extra-tree-classifier","text":"An Extremely Randomized Classification Tree that recursively chooses node splits with the least entropy among a set of k (given by max features) random split points. Extra Trees are useful in ensembles such as Random Forest or AdaBoost as the weak learner or they can be used on their own. The strength of Extra Trees as compared to standard decision trees are their computational efficiency and lower prediction variance. Interfaces: Estimator , Learner , Probabilistic , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous","title":"Extra Tree Classifier"},{"location":"classifiers/extra-tree-classifier.html#parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split.","title":"Parameters"},{"location":"classifiers/extra-tree-classifier.html#example","text":"use Rubix\\ML\\Classifiers\\ExtraTreeClassifier ; $estimator = new ExtraTreeClassifier ( 50 , 3 , 1e-7 , 10 );","title":"Example"},{"location":"classifiers/extra-tree-classifier.html#additional-methods","text":"Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int","title":"Additional Methods"},{"location":"classifiers/extra-tree-classifier.html#references","text":"P. Geurts et al. (2005). Extremely Randomized Trees. \u21a9","title":"References"},{"location":"classifiers/gaussian-naive-bayes.html","text":"[source] Gaussian Naive Bayes # Gaussian Naive Bayes is a version of the Naive Bayes classifier for continuous features. It places a probability density function (PDF) over the features conditioned on a class basis and uses Bayes' Theorem to derive the final probabilities. In addition to the naive feature independence assumption, Gaussian Naive Bayes also assumes that all features are normally (Gaussian) distributed. Interfaces: Estimator , Learner , Online , Probabilistic , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 priors null array The class prior probabilities as an associative array with class labels as keys and their prior probabilities as values totalling 1. If null, then priors will automatically be computed from the training data. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature. Example # use Rubix\\ML\\Classifiers\\GaussianNB ; $estimator = new GaussianNB ([ 'benign' => 0.9 , 'malignant' => 0.1 , ], 1e-9 ); Additional Methods # Return the class prior probabilities: public priors () : float [] | null Return the mean of each feature column for each class: public means () : array [] | null Return the variance of each feature column for each class: public variances () : array [] | null References # T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances. \u21a9","title":"Gaussian Naive Bayes"},{"location":"classifiers/gaussian-naive-bayes.html#gaussian-naive-bayes","text":"Gaussian Naive Bayes is a version of the Naive Bayes classifier for continuous features. It places a probability density function (PDF) over the features conditioned on a class basis and uses Bayes' Theorem to derive the final probabilities. In addition to the naive feature independence assumption, Gaussian Naive Bayes also assumes that all features are normally (Gaussian) distributed. Interfaces: Estimator , Learner , Online , Probabilistic , Persistable Data Type Compatibility: Continuous","title":"Gaussian Naive Bayes"},{"location":"classifiers/gaussian-naive-bayes.html#parameters","text":"# Name Default Type Description 1 priors null array The class prior probabilities as an associative array with class labels as keys and their prior probabilities as values totalling 1. If null, then priors will automatically be computed from the training data. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature.","title":"Parameters"},{"location":"classifiers/gaussian-naive-bayes.html#example","text":"use Rubix\\ML\\Classifiers\\GaussianNB ; $estimator = new GaussianNB ([ 'benign' => 0.9 , 'malignant' => 0.1 , ], 1e-9 );","title":"Example"},{"location":"classifiers/gaussian-naive-bayes.html#additional-methods","text":"Return the class prior probabilities: public priors () : float [] | null Return the mean of each feature column for each class: public means () : array [] | null Return the variance of each feature column for each class: public variances () : array [] | null","title":"Additional Methods"},{"location":"classifiers/gaussian-naive-bayes.html#references","text":"T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances. \u21a9","title":"References"},{"location":"classifiers/k-nearest-neighbors.html","text":"[source] K Nearest Neighbors # A brute-force distance-based learning algorithm that locates the k nearest samples from the training set and predicts the class label that is most common. K Nearest Neighbors (KNN) is considered a lazy learner because it performs most of its computation at inference time. Note For a faster spatial tree-accelerated version of KNN, see KD Neighbors . Interfaces: Estimator , Learner , Online , Probabilistic , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. Example # use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $estimator = new KNearestNeighbors ( 3 , false , new Manhattan ()); Additional Methods # This estimator does not have any additional methods.","title":"K Nearest Neighbors"},{"location":"classifiers/k-nearest-neighbors.html#k-nearest-neighbors","text":"A brute-force distance-based learning algorithm that locates the k nearest samples from the training set and predicts the class label that is most common. K Nearest Neighbors (KNN) is considered a lazy learner because it performs most of its computation at inference time. Note For a faster spatial tree-accelerated version of KNN, see KD Neighbors . Interfaces: Estimator , Learner , Online , Probabilistic , Persistable Data Type Compatibility: Depends on distance kernel","title":"K Nearest Neighbors"},{"location":"classifiers/k-nearest-neighbors.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 kernel Euclidean Distance The distance kernel used to compute the distance between sample points.","title":"Parameters"},{"location":"classifiers/k-nearest-neighbors.html#example","text":"use Rubix\\ML\\Classifiers\\KNearestNeighbors ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $estimator = new KNearestNeighbors ( 3 , false , new Manhattan ());","title":"Example"},{"location":"classifiers/k-nearest-neighbors.html#additional-methods","text":"This estimator does not have any additional methods.","title":"Additional Methods"},{"location":"classifiers/kd-neighbors.html","text":"[source] K-d Neighbors # A fast K Nearest Neighbors algorithm that uses a binary search tree (BST) to divide the training set into neighborhoods that contain samples that are close together spatially. K-d Neighbors then does a binary search to locate the nearest neighborhood of an unknown sample and prunes all neighborhoods whose bounding box is further than the k 'th nearest neighbor found so far. The main advantage of K-d Neighbors over brute force KNN is that it is much more efficient, however it cannot be partially trained. Interfaces: Estimator , Learner , Probabilistic , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches. Example # use Rubix\\ML\\Classifiers\\KDNeighbors ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $estimator = new KDNeighbors ( 10 , false , new BallTree ( 40 , new Minkowski ())); Additional Methods # Return the base spatial tree instance: public tree () : Spatial","title":"K-d Neighbors"},{"location":"classifiers/kd-neighbors.html#k-d-neighbors","text":"A fast K Nearest Neighbors algorithm that uses a binary search tree (BST) to divide the training set into neighborhoods that contain samples that are close together spatially. K-d Neighbors then does a binary search to locate the nearest neighborhood of an unknown sample and prunes all neighborhoods whose bounding box is further than the k 'th nearest neighbor found so far. The main advantage of K-d Neighbors over brute force KNN is that it is much more efficient, however it cannot be partially trained. Interfaces: Estimator , Learner , Probabilistic , Persistable Data Type Compatibility: Depends on distance kernel","title":"K-d Neighbors"},{"location":"classifiers/kd-neighbors.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches.","title":"Parameters"},{"location":"classifiers/kd-neighbors.html#example","text":"use Rubix\\ML\\Classifiers\\KDNeighbors ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $estimator = new KDNeighbors ( 10 , false , new BallTree ( 40 , new Minkowski ()));","title":"Example"},{"location":"classifiers/kd-neighbors.html#additional-methods","text":"Return the base spatial tree instance: public tree () : Spatial","title":"Additional Methods"},{"location":"classifiers/logistic-regression.html","text":"[source] Logistic Regression # A linear classifier that uses the logistic ( sigmoid ) function to estimate the probabilities of exactly two class outcomes. The model parameters (weights and bias) are solved using Mini Batch Gradient Descent with pluggable optimizers and cost functions that run on the neural network subsystem. Interfaces: Estimator , Learner , Online , Probabilistic , Ranks Features , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 batchSize 128 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training. Example # use Rubix\\ML\\Classifiers\\LogisticRegression ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; $estimator = new LogisticRegression ( 64 , new Adam ( 0.001 ), 1e-4 , 100 , 1e-4 , 5 , new CrossEntropy ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the underlying neural network instance or null if untrained: public network () : Network | null","title":"Logistic Regression"},{"location":"classifiers/logistic-regression.html#logistic-regression","text":"A linear classifier that uses the logistic ( sigmoid ) function to estimate the probabilities of exactly two class outcomes. The model parameters (weights and bias) are solved using Mini Batch Gradient Descent with pluggable optimizers and cost functions that run on the neural network subsystem. Interfaces: Estimator , Learner , Online , Probabilistic , Ranks Features , Verbose , Persistable Data Type Compatibility: Continuous","title":"Logistic Regression"},{"location":"classifiers/logistic-regression.html#parameters","text":"# Name Default Type Description 1 batchSize 128 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training.","title":"Parameters"},{"location":"classifiers/logistic-regression.html#example","text":"use Rubix\\ML\\Classifiers\\LogisticRegression ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; $estimator = new LogisticRegression ( 64 , new Adam ( 0.001 ), 1e-4 , 100 , 1e-4 , 5 , new CrossEntropy ());","title":"Example"},{"location":"classifiers/logistic-regression.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the underlying neural network instance or null if untrained: public network () : Network | null","title":"Additional Methods"},{"location":"classifiers/logit-boost.html","text":"[source] Logit Boost # A stage-wise additive ensemble that uses regression trees to iteratively learn a Logistic Regression model for binary classification problems. Unlike standard Logistic Regression , Logit Boost has the ability to learn a smooth non-linear decision surface by training decision trees to follow the gradient of the cross entropy loss function. In addition, Logit Boost concentrates more effort on classifying samples that it is less certain about. Note Logit Boost utilizes progress monitoring via an internal validation set for snapshotting and early stopping. If there are not enough training samples to build an internal validation set given the user-specified holdout ratio then training will proceed with progress monitoring disabled. Interfaces: Estimator , Learner , Probabilistic , Verbose , Ranks Features , Persistable Data Type Compatibility: Depends on base learners Parameters # # Name Default Type Description 1 booster RegressionTree Learner The regressor used to fix up the error residuals of the base learner. 2 rate 0.1 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.5 float The ratio of samples to subsample from the training set to train each booster. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 7 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 8 metric F Beta Metric The metric used to score the generalization performance of the model during training. Example # use Rubix\\ML\\Classifiers\\LogitBoost ; use Rubix\\ML\\Regressors\\RegressionTree ; use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $estimator = new LogitBoost ( new RegressionTree ( 4 ), 0.1 , 0.5 , 1000 , 1e-4 , 5 , 0.1 , new FBeta ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the validation score for each epoch from the last training session: public scores () : float [] | null Return the loss for each epoch from the last training session: public losses () : float [] | null References # J. H. Friedman et al. (2000). Additive Logistic Regression: A Statistical View of Boosting. \u21a9 J. H. Friedman. (2001). Greedy Function Approximation: A Gradient Boosting Machine. \u21a9 J. H. Friedman. (1999). Stochastic Gradient Boosting. \u21a9 Y. Wei. et al. (2017). Early stopping for kernel boosting algorithms: A general analysis with localized complexities. \u21a9 G. Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. \u21a9","title":"Logit Boost"},{"location":"classifiers/logit-boost.html#logit-boost","text":"A stage-wise additive ensemble that uses regression trees to iteratively learn a Logistic Regression model for binary classification problems. Unlike standard Logistic Regression , Logit Boost has the ability to learn a smooth non-linear decision surface by training decision trees to follow the gradient of the cross entropy loss function. In addition, Logit Boost concentrates more effort on classifying samples that it is less certain about. Note Logit Boost utilizes progress monitoring via an internal validation set for snapshotting and early stopping. If there are not enough training samples to build an internal validation set given the user-specified holdout ratio then training will proceed with progress monitoring disabled. Interfaces: Estimator , Learner , Probabilistic , Verbose , Ranks Features , Persistable Data Type Compatibility: Depends on base learners","title":"Logit Boost"},{"location":"classifiers/logit-boost.html#parameters","text":"# Name Default Type Description 1 booster RegressionTree Learner The regressor used to fix up the error residuals of the base learner. 2 rate 0.1 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.5 float The ratio of samples to subsample from the training set to train each booster. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 7 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 8 metric F Beta Metric The metric used to score the generalization performance of the model during training.","title":"Parameters"},{"location":"classifiers/logit-boost.html#example","text":"use Rubix\\ML\\Classifiers\\LogitBoost ; use Rubix\\ML\\Regressors\\RegressionTree ; use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $estimator = new LogitBoost ( new RegressionTree ( 4 ), 0.1 , 0.5 , 1000 , 1e-4 , 5 , 0.1 , new FBeta ());","title":"Example"},{"location":"classifiers/logit-boost.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the validation score for each epoch from the last training session: public scores () : float [] | null Return the loss for each epoch from the last training session: public losses () : float [] | null","title":"Additional Methods"},{"location":"classifiers/logit-boost.html#references","text":"J. H. Friedman et al. (2000). Additive Logistic Regression: A Statistical View of Boosting. \u21a9 J. H. Friedman. (2001). Greedy Function Approximation: A Gradient Boosting Machine. \u21a9 J. H. Friedman. (1999). Stochastic Gradient Boosting. \u21a9 Y. Wei. et al. (2017). Early stopping for kernel boosting algorithms: A general analysis with localized complexities. \u21a9 G. Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. \u21a9","title":"References"},{"location":"classifiers/multilayer-perceptron.html","text":"[source] Multilayer Perceptron # A multiclass feed-forward neural network classifier with user-defined hidden layers. The Multilayer Perceptron is a deep learning model capable of forming higher-order feature representations through layers of computation. In addition, the MLP features progress monitoring which stops training when it can no longer improve the validation score. It also utilizes network snapshotting to make sure that it always has the best model parameters even if progress began to decline during training. Note If there are not enough training samples to build an internal validation set with the user-specified holdout ratio then progress monitoring will be disabled. Interfaces: Estimator , Learner , Online , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 hidden array An array composing the user-specified hidden layers of the network in order. 2 batchSize 128 int The number of training samples to process at a time. 3 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 4 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 5 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 6 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 7 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 8 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 9 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training. 10 metric FBeta Metric The validation metric used to score the generalization performance of the model during training. Example # use Rubix\\ML\\Classifiers\\MultilayerPerceptron ; use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Layers\\Dropout ; use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\Layers\\PReLU ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\LeakyReLU ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; use Rubix\\ML\\CrossValidation\\Metrics\\MCC ; $estimator = new MultilayerPerceptron ([ new Dense ( 200 ), new Activation ( new LeakyReLU ()), new Dropout ( 0.3 ), new Dense ( 100 ), new Activation ( new LeakyReLU ()), new Dropout ( 0.3 ), new Dense ( 50 ), new PReLU (), ], 128 , new Adam ( 0.001 ), 1e-4 , 1000 , 1e-3 , 3 , 0.1 , new CrossEntropy (), new MCC ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the validation score for each epoch from the last training session: public scores () : float [] | null Returns the underlying neural network instance or null if untrained: public network () : Network | null References # G. E. Hinton. (1989). Connectionist learning procedures. \u21a9 L. Prechelt. (1997). Early Stopping - but when? \u21a9","title":"Multilayer Perceptron"},{"location":"classifiers/multilayer-perceptron.html#multilayer-perceptron","text":"A multiclass feed-forward neural network classifier with user-defined hidden layers. The Multilayer Perceptron is a deep learning model capable of forming higher-order feature representations through layers of computation. In addition, the MLP features progress monitoring which stops training when it can no longer improve the validation score. It also utilizes network snapshotting to make sure that it always has the best model parameters even if progress began to decline during training. Note If there are not enough training samples to build an internal validation set with the user-specified holdout ratio then progress monitoring will be disabled. Interfaces: Estimator , Learner , Online , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous","title":"Multilayer Perceptron"},{"location":"classifiers/multilayer-perceptron.html#parameters","text":"# Name Default Type Description 1 hidden array An array composing the user-specified hidden layers of the network in order. 2 batchSize 128 int The number of training samples to process at a time. 3 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 4 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 5 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 6 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 7 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 8 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 9 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training. 10 metric FBeta Metric The validation metric used to score the generalization performance of the model during training.","title":"Parameters"},{"location":"classifiers/multilayer-perceptron.html#example","text":"use Rubix\\ML\\Classifiers\\MultilayerPerceptron ; use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Layers\\Dropout ; use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\Layers\\PReLU ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\LeakyReLU ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; use Rubix\\ML\\CrossValidation\\Metrics\\MCC ; $estimator = new MultilayerPerceptron ([ new Dense ( 200 ), new Activation ( new LeakyReLU ()), new Dropout ( 0.3 ), new Dense ( 100 ), new Activation ( new LeakyReLU ()), new Dropout ( 0.3 ), new Dense ( 50 ), new PReLU (), ], 128 , new Adam ( 0.001 ), 1e-4 , 1000 , 1e-3 , 3 , 0.1 , new CrossEntropy (), new MCC ());","title":"Example"},{"location":"classifiers/multilayer-perceptron.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the validation score for each epoch from the last training session: public scores () : float [] | null Returns the underlying neural network instance or null if untrained: public network () : Network | null","title":"Additional Methods"},{"location":"classifiers/multilayer-perceptron.html#references","text":"G. E. Hinton. (1989). Connectionist learning procedures. \u21a9 L. Prechelt. (1997). Early Stopping - but when? \u21a9","title":"References"},{"location":"classifiers/naive-bayes.html","text":"[source] Naive Bayes # Categorical Naive Bayes is a probability-based classifier that uses counting and Bayes' Theorem to derive the probabilities of a class given a sample of categorical features. The term naive refers to the fact that Naive Bayes treats each feature as if it was independent of the others even though this is usually not the case in real life. Note Each partial train has the overhead of recomputing the probability mass function for each feature per class. As such, it is better to train with fewer but larger training sets. Interfaces: Estimator , Learner , Online , Probabilistic , Persistable Data Type Compatibility: Categorical Parameters # # Name Default Type Description 1 priors null array The class prior probabilities as an associative array with class labels as keys and their prior probabilities as values totalling 1. If null, then priors will automatically be computed from the training data. 2 smoothing 1.0 float The amount of Laplace smoothing added to the probabilities. Example # use Rubix\\ML\\Classifiers\\NaiveBayes ; $estimator = new NaiveBayes ( 2.5 , [ 'spam' => 0.3 , 'not spam' => 0.7 , ]); Additional Methods # Return the class prior probabilities: public priors () : float [] | null Return the counts for each category per class: public counts () : array [] | null","title":"Naive Bayes"},{"location":"classifiers/naive-bayes.html#naive-bayes","text":"Categorical Naive Bayes is a probability-based classifier that uses counting and Bayes' Theorem to derive the probabilities of a class given a sample of categorical features. The term naive refers to the fact that Naive Bayes treats each feature as if it was independent of the others even though this is usually not the case in real life. Note Each partial train has the overhead of recomputing the probability mass function for each feature per class. As such, it is better to train with fewer but larger training sets. Interfaces: Estimator , Learner , Online , Probabilistic , Persistable Data Type Compatibility: Categorical","title":"Naive Bayes"},{"location":"classifiers/naive-bayes.html#parameters","text":"# Name Default Type Description 1 priors null array The class prior probabilities as an associative array with class labels as keys and their prior probabilities as values totalling 1. If null, then priors will automatically be computed from the training data. 2 smoothing 1.0 float The amount of Laplace smoothing added to the probabilities.","title":"Parameters"},{"location":"classifiers/naive-bayes.html#example","text":"use Rubix\\ML\\Classifiers\\NaiveBayes ; $estimator = new NaiveBayes ( 2.5 , [ 'spam' => 0.3 , 'not spam' => 0.7 , ]);","title":"Example"},{"location":"classifiers/naive-bayes.html#additional-methods","text":"Return the class prior probabilities: public priors () : float [] | null Return the counts for each category per class: public counts () : array [] | null","title":"Additional Methods"},{"location":"classifiers/radius-neighbors.html","text":"[source] Radius Neighbors # Radius Neighbors is a classifier that takes the distance-weighted vote of each neighbor within a cluster of a fixed user-defined radius to make a prediction. Since the radius of the search can be constrained, Radius Neighbors is more robust to outliers than K Nearest Neighbors . In addition, Radius Neighbors acts as a quasi-anomaly detector by flagging samples that have 0 neighbors within the search radius. Interfaces: Estimator , Learner , Probabilistic , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 radius 1.0 float The radius within which points are considered neighbors. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 outlierClass '?' string The class label for any samples that have 0 neighbors within the specified radius. 4 tree BallTree Spatial The spatial tree used to run range searches. Example # use Rubix\\ML\\Classifiers\\RadiusNeighbors ; use Rubix\\ML\\Graph\\Trees\\KDTree ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $estimator = new RadiusNeighbors ( 50.0 , true , '?' , new KDTree ( 100 , new Manhattan ())); Additional Methods # Return the base spatial tree instance: public tree () : Spatial","title":"Radius Neighbors"},{"location":"classifiers/radius-neighbors.html#radius-neighbors","text":"Radius Neighbors is a classifier that takes the distance-weighted vote of each neighbor within a cluster of a fixed user-defined radius to make a prediction. Since the radius of the search can be constrained, Radius Neighbors is more robust to outliers than K Nearest Neighbors . In addition, Radius Neighbors acts as a quasi-anomaly detector by flagging samples that have 0 neighbors within the search radius. Interfaces: Estimator , Learner , Probabilistic , Persistable Data Type Compatibility: Depends on distance kernel","title":"Radius Neighbors"},{"location":"classifiers/radius-neighbors.html#parameters","text":"# Name Default Type Description 1 radius 1.0 float The radius within which points are considered neighbors. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 outlierClass '?' string The class label for any samples that have 0 neighbors within the specified radius. 4 tree BallTree Spatial The spatial tree used to run range searches.","title":"Parameters"},{"location":"classifiers/radius-neighbors.html#example","text":"use Rubix\\ML\\Classifiers\\RadiusNeighbors ; use Rubix\\ML\\Graph\\Trees\\KDTree ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $estimator = new RadiusNeighbors ( 50.0 , true , '?' , new KDTree ( 100 , new Manhattan ()));","title":"Example"},{"location":"classifiers/radius-neighbors.html#additional-methods","text":"Return the base spatial tree instance: public tree () : Spatial","title":"Additional Methods"},{"location":"classifiers/random-forest.html","text":"[source] Random Forest # Random Forest (RF) is a classifier that trains an ensemble of Decision Trees ( Classification Trees or Extra Trees ) on random subsets ( bootstrap set) of the training data. Predictions are based on the probability scores returned from each tree in the ensemble, averaged and weighted equally. In addition to reliable predictions, Random Forest also returns reliable feature importance scores making it suitable for feature selection. Note The default base tree learner is a fully grown Classification Tree . Interfaces: Estimator , Learner , Probabilistic , Parallel , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 base ClassificationTree Learner The base learner. 2 estimators 100 int The number of learners to train in the ensemble. 3 ratio 0.2 float The ratio of samples from the training set to randomly subsample to train each base learner. 4 balanced false bool Should we sample the bootstrap set to compensate for imbalanced class labels? Example # use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ClassificationTree ; $estimator = new RandomForest ( new ClassificationTree ( 10 ), 300 , 0.1 , true ); Additional Methods # This estimator does not have any additional methods. References # L. Breiman. (2001). Random Forests. \u21a9 L. Breiman et al. (2005). Extremely Randomized Trees. \u21a9","title":"Random Forest"},{"location":"classifiers/random-forest.html#random-forest","text":"Random Forest (RF) is a classifier that trains an ensemble of Decision Trees ( Classification Trees or Extra Trees ) on random subsets ( bootstrap set) of the training data. Predictions are based on the probability scores returned from each tree in the ensemble, averaged and weighted equally. In addition to reliable predictions, Random Forest also returns reliable feature importance scores making it suitable for feature selection. Note The default base tree learner is a fully grown Classification Tree . Interfaces: Estimator , Learner , Probabilistic , Parallel , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous","title":"Random Forest"},{"location":"classifiers/random-forest.html#parameters","text":"# Name Default Type Description 1 base ClassificationTree Learner The base learner. 2 estimators 100 int The number of learners to train in the ensemble. 3 ratio 0.2 float The ratio of samples from the training set to randomly subsample to train each base learner. 4 balanced false bool Should we sample the bootstrap set to compensate for imbalanced class labels?","title":"Parameters"},{"location":"classifiers/random-forest.html#example","text":"use Rubix\\ML\\Classifiers\\RandomForest ; use Rubix\\ML\\Classifiers\\ClassificationTree ; $estimator = new RandomForest ( new ClassificationTree ( 10 ), 300 , 0.1 , true );","title":"Example"},{"location":"classifiers/random-forest.html#additional-methods","text":"This estimator does not have any additional methods.","title":"Additional Methods"},{"location":"classifiers/random-forest.html#references","text":"L. Breiman. (2001). Random Forests. \u21a9 L. Breiman et al. (2005). Extremely Randomized Trees. \u21a9","title":"References"},{"location":"classifiers/softmax-classifier.html","text":"[source] Softmax Classifier # A multiclass generalization of Logistic Regression using a single layer neural network with a Softmax output layer. Interfaces: Estimator , Learner , Online , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 batchSize 256 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 alpha 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training. Example # use Rubix\\ML\\Classifiers\\SoftmaxClassifier ; use Rubix\\ML\\NeuralNet\\Optimizers\\Momentum ; use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; $estimator = new SoftmaxClassifier ( 256 , new Momentum ( 0.001 ), 1e-4 , 300 , 1e-4 , 10 , new CrossEntropy ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the underlying neural network instance or null if untrained: public network () : Network | null","title":"Softmax Classifier"},{"location":"classifiers/softmax-classifier.html#softmax-classifier","text":"A multiclass generalization of Logistic Regression using a single layer neural network with a Softmax output layer. Interfaces: Estimator , Learner , Online , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous","title":"Softmax Classifier"},{"location":"classifiers/softmax-classifier.html#parameters","text":"# Name Default Type Description 1 batchSize 256 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 alpha 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn CrossEntropy ClassificationLoss The function that computes the loss associated with an erroneous activation during training.","title":"Parameters"},{"location":"classifiers/softmax-classifier.html#example","text":"use Rubix\\ML\\Classifiers\\SoftmaxClassifier ; use Rubix\\ML\\NeuralNet\\Optimizers\\Momentum ; use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; $estimator = new SoftmaxClassifier ( 256 , new Momentum ( 0.001 ), 1e-4 , 300 , 1e-4 , 10 , new CrossEntropy ());","title":"Example"},{"location":"classifiers/softmax-classifier.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the underlying neural network instance or null if untrained: public network () : Network | null","title":"Additional Methods"},{"location":"classifiers/svc.html","text":"[source] SVC # The multiclass Support Vector Machine (SVM) Classifier is a maximum margin classifier that can efficiently perform non-linear classification by implicitly mapping feature vectors into high-dimensional feature space using the kernel trick . Note This learner requires the SVM extension which uses the libsvm engine under the hood. Interfaces: Estimator , Learner Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 c 1.0 float The parameter that defines the width of the margin used to separate the classes. 2 kernel RBF Kernel The kernel function used to operate in higher dimensions. 3 shrinking true bool Should we use the shrinking heuristic? 4 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 5 cache size 100.0 float The size of the kernel cache in MB. Example # use Rubix\\ML\\Classifiers\\SVC ; use Rubix\\ML\\Kernels\\SVM\\Linear ; $estimator = new SVC ( 1.0 , new Linear (), true , 1e-3 , 100.0 ); Additional Methods # Save the model data to the filesystem: public save ( string $path ) : void Load the model data from the filesystem: public load ( string $path ) : void References # C. Chang et al. (2011). LIBSVM: A library for support vector machines. \u21a9","title":"SVC"},{"location":"classifiers/svc.html#svc","text":"The multiclass Support Vector Machine (SVM) Classifier is a maximum margin classifier that can efficiently perform non-linear classification by implicitly mapping feature vectors into high-dimensional feature space using the kernel trick . Note This learner requires the SVM extension which uses the libsvm engine under the hood. Interfaces: Estimator , Learner Data Type Compatibility: Continuous","title":"SVC"},{"location":"classifiers/svc.html#parameters","text":"# Name Default Type Description 1 c 1.0 float The parameter that defines the width of the margin used to separate the classes. 2 kernel RBF Kernel The kernel function used to operate in higher dimensions. 3 shrinking true bool Should we use the shrinking heuristic? 4 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 5 cache size 100.0 float The size of the kernel cache in MB.","title":"Parameters"},{"location":"classifiers/svc.html#example","text":"use Rubix\\ML\\Classifiers\\SVC ; use Rubix\\ML\\Kernels\\SVM\\Linear ; $estimator = new SVC ( 1.0 , new Linear (), true , 1e-3 , 100.0 );","title":"Example"},{"location":"classifiers/svc.html#additional-methods","text":"Save the model data to the filesystem: public save ( string $path ) : void Load the model data from the filesystem: public load ( string $path ) : void","title":"Additional Methods"},{"location":"classifiers/svc.html#references","text":"C. Chang et al. (2011). LIBSVM: A library for support vector machines. \u21a9","title":"References"},{"location":"clusterers/dbscan.html","text":"[source] DBSCAN # Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering algorithm able to find non-linearly separable and arbitrarily-shaped clusters given a radius and density constraint. In addition, DBSCAN can flag outliers (noise samples) and thus be used as a quasi-anomaly detector. Note Noise samples are assigned the cluster number -1. Interfaces: Estimator Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 radius 0.5 float The maximum distance between two points to be considered neighbors. 2 minDensity 5 int The minimum number of points within radius of each other to form a cluster. 3 tree BallTree Spatial The spatial tree used to run range searches. Example # use Rubix\\ML\\Clusterers\\DBSCAN ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Diagonal ; $estimator = new DBSCAN ( 4.0 , 5 , new BallTree ( 20 , new Diagonal ())); Additional Methods # This estimator does not have any additional methods. References # M. Ester et al. (1996). A Density-Based Algorithm for Discovering Clusters. \u21a9","title":"DBSCAN"},{"location":"clusterers/dbscan.html#dbscan","text":"Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering algorithm able to find non-linearly separable and arbitrarily-shaped clusters given a radius and density constraint. In addition, DBSCAN can flag outliers (noise samples) and thus be used as a quasi-anomaly detector. Note Noise samples are assigned the cluster number -1. Interfaces: Estimator Data Type Compatibility: Depends on distance kernel","title":"DBSCAN"},{"location":"clusterers/dbscan.html#parameters","text":"# Name Default Type Description 1 radius 0.5 float The maximum distance between two points to be considered neighbors. 2 minDensity 5 int The minimum number of points within radius of each other to form a cluster. 3 tree BallTree Spatial The spatial tree used to run range searches.","title":"Parameters"},{"location":"clusterers/dbscan.html#example","text":"use Rubix\\ML\\Clusterers\\DBSCAN ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Diagonal ; $estimator = new DBSCAN ( 4.0 , 5 , new BallTree ( 20 , new Diagonal ()));","title":"Example"},{"location":"clusterers/dbscan.html#additional-methods","text":"This estimator does not have any additional methods.","title":"Additional Methods"},{"location":"clusterers/dbscan.html#references","text":"M. Ester et al. (1996). A Density-Based Algorithm for Discovering Clusters. \u21a9","title":"References"},{"location":"clusterers/fuzzy-c-means.html","text":"[source] Fuzzy C Means # A distance-based soft-clustering algorithm that allows samples to belong to multiple clusters if they fall within a fuzzy region controlled by the fuzz hyper-parameter. Like K Means , Fuzzy C Means minimizes the inertia cost function, however, unlike K Means, FCM uses a batch solver that requires the entire training set to compute the update to the cluster centroids at each step. Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 c int The number of target clusters. 2 fuzz 2.0 float Determines the bandwidth of the fuzzy area. 3 epochs 300 int The maximum number of training rounds to execute. 4 minChange 1e-4 float The minimum change in the inertia for the algorithm to continue training. 5 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. 6 seeder PlusPlus Seeder The seeder used to initialize the cluster centroids. Example # use Rubix\\ML\\Clusterers\\FuzzyCMeans ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Clusterers\\Seeders\\Random ; $estimator = new FuzzyCMeans ( 5 , 1.2 , 400 , 1. , new Euclidean (), new Random ()); Additional Methods # Return the c computed centroids of the training set: public centroids () : array [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Returns the inertia at each epoch from the last round of training: public losses () : float [] | null References # J. C. Bezdek et al. (1984). FCM: The Fuzzy C-Means Clustering Algorithm. \u21a9","title":"Fuzzy C Means"},{"location":"clusterers/fuzzy-c-means.html#fuzzy-c-means","text":"A distance-based soft-clustering algorithm that allows samples to belong to multiple clusters if they fall within a fuzzy region controlled by the fuzz hyper-parameter. Like K Means , Fuzzy C Means minimizes the inertia cost function, however, unlike K Means, FCM uses a batch solver that requires the entire training set to compute the update to the cluster centroids at each step. Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous","title":"Fuzzy C Means"},{"location":"clusterers/fuzzy-c-means.html#parameters","text":"# Name Default Type Description 1 c int The number of target clusters. 2 fuzz 2.0 float Determines the bandwidth of the fuzzy area. 3 epochs 300 int The maximum number of training rounds to execute. 4 minChange 1e-4 float The minimum change in the inertia for the algorithm to continue training. 5 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. 6 seeder PlusPlus Seeder The seeder used to initialize the cluster centroids.","title":"Parameters"},{"location":"clusterers/fuzzy-c-means.html#example","text":"use Rubix\\ML\\Clusterers\\FuzzyCMeans ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Clusterers\\Seeders\\Random ; $estimator = new FuzzyCMeans ( 5 , 1.2 , 400 , 1. , new Euclidean (), new Random ());","title":"Example"},{"location":"clusterers/fuzzy-c-means.html#additional-methods","text":"Return the c computed centroids of the training set: public centroids () : array [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Returns the inertia at each epoch from the last round of training: public losses () : float [] | null","title":"Additional Methods"},{"location":"clusterers/fuzzy-c-means.html#references","text":"J. C. Bezdek et al. (1984). FCM: The Fuzzy C-Means Clustering Algorithm. \u21a9","title":"References"},{"location":"clusterers/gaussian-mixture.html","text":"[source] Gaussian Mixture # A Gaussian Mixture model (GMM) is a probabilistic model for representing the presence of clusters within an overall population without requiring a sample to know which sub-population it belongs to beforehand. GMMs are similar to centroid-based clusterers like K Means but allow both the cluster centers ( means ) as well as the radii ( variances ) to be learned as well. For this reason, GMMs are especially useful for clusterings that are of different radii. Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 k int The number of target clusters. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature. 3 epochs 100 int The maximum number of training rounds to execute. 4 minChange 1e-3 float The minimum change in the components necessary for the algorithm to continue training. 5 seeder PlusPlus Seeder The seeder used to initialize the Gaussian components. Example # use Rubix\\ML\\Clusterers\\GaussianMixture ; use Rubix\\ML\\Clusterers\\Seeders\\KMC2 ; $estimator = new GaussianMixture ( 5 , 1e-6 , 100 , 1e-4 , new KMC2 ( 50 )); Additional Methods # Return the cluster prior probabilities based on their representation over all training samples: public priors () : float [] Return the running means of each feature column for each cluster: public means () : array [] Return the variance of each feature column for each cluster: public variances () : array [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null References # A. P. Dempster et al. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. \u21a9 J. Blomer et al. (2016). Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models. \u21a9","title":"Gaussian Mixture"},{"location":"clusterers/gaussian-mixture.html#gaussian-mixture","text":"A Gaussian Mixture model (GMM) is a probabilistic model for representing the presence of clusters within an overall population without requiring a sample to know which sub-population it belongs to beforehand. GMMs are similar to centroid-based clusterers like K Means but allow both the cluster centers ( means ) as well as the radii ( variances ) to be learned as well. For this reason, GMMs are especially useful for clusterings that are of different radii. Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous","title":"Gaussian Mixture"},{"location":"clusterers/gaussian-mixture.html#parameters","text":"# Name Default Type Description 1 k int The number of target clusters. 2 smoothing 1e-9 float The amount of epsilon smoothing added to the variance of each feature. 3 epochs 100 int The maximum number of training rounds to execute. 4 minChange 1e-3 float The minimum change in the components necessary for the algorithm to continue training. 5 seeder PlusPlus Seeder The seeder used to initialize the Gaussian components.","title":"Parameters"},{"location":"clusterers/gaussian-mixture.html#example","text":"use Rubix\\ML\\Clusterers\\GaussianMixture ; use Rubix\\ML\\Clusterers\\Seeders\\KMC2 ; $estimator = new GaussianMixture ( 5 , 1e-6 , 100 , 1e-4 , new KMC2 ( 50 ));","title":"Example"},{"location":"clusterers/gaussian-mixture.html#additional-methods","text":"Return the cluster prior probabilities based on their representation over all training samples: public priors () : float [] Return the running means of each feature column for each cluster: public means () : array [] Return the variance of each feature column for each cluster: public variances () : array [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null","title":"Additional Methods"},{"location":"clusterers/gaussian-mixture.html#references","text":"A. P. Dempster et al. (1977). Maximum Likelihood from Incomplete Data via the EM Algorithm. \u21a9 J. Blomer et al. (2016). Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models. \u21a9","title":"References"},{"location":"clusterers/k-means.html","text":"[source] K Means # A fast online centroid-based hard clustering algorithm capable of grouping linearly separable data points given some prior knowledge of the target number of clusters (defined by k ). K Means is trained using adaptive Mini Batch Gradient Descent and minimizes the inertia cost function at each epoch. Inertia is defined as the average sum of distances between each sample and its nearest cluster centroid. Interfaces: Estimator , Learner , Online , Probabilistic , Persistable , Verbose Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 k int The number of target clusters. 2 batch size 128 int The size of each mini batch in samples. 3 epochs 1000 int The maximum number of training rounds to execute. 4 min change 1e-4 float The minimum change in the inertia for training to continue. 5 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 6 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. 7 seeder PlusPlus Seeder The seeder used to initialize the cluster centroids. Example # use Rubix\\ML\\Clusterers\\KMeans ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Clusterers\\Seeders\\PlusPlus ; $estimator = new KMeans ( 3 , 128 , 300 , 10.0 , 10 , new Euclidean (), new PlusPlus ()); Additional Methods # Return the k computed centroids of the training set: public centroids () : array [] Return the number of training samples that each centroid is responsible for: public sizes () : int [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null References # D. Sculley. (2010). Web-Scale K-Means Clustering. \u21a9","title":"K Means"},{"location":"clusterers/k-means.html#k-means","text":"A fast online centroid-based hard clustering algorithm capable of grouping linearly separable data points given some prior knowledge of the target number of clusters (defined by k ). K Means is trained using adaptive Mini Batch Gradient Descent and minimizes the inertia cost function at each epoch. Inertia is defined as the average sum of distances between each sample and its nearest cluster centroid. Interfaces: Estimator , Learner , Online , Probabilistic , Persistable , Verbose Data Type Compatibility: Continuous","title":"K Means"},{"location":"clusterers/k-means.html#parameters","text":"# Name Default Type Description 1 k int The number of target clusters. 2 batch size 128 int The size of each mini batch in samples. 3 epochs 1000 int The maximum number of training rounds to execute. 4 min change 1e-4 float The minimum change in the inertia for training to continue. 5 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 6 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. 7 seeder PlusPlus Seeder The seeder used to initialize the cluster centroids.","title":"Parameters"},{"location":"clusterers/k-means.html#example","text":"use Rubix\\ML\\Clusterers\\KMeans ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; use Rubix\\ML\\Clusterers\\Seeders\\PlusPlus ; $estimator = new KMeans ( 3 , 128 , 300 , 10.0 , 10 , new Euclidean (), new PlusPlus ());","title":"Example"},{"location":"clusterers/k-means.html#additional-methods","text":"Return the k computed centroids of the training set: public centroids () : array [] Return the number of training samples that each centroid is responsible for: public sizes () : int [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null","title":"Additional Methods"},{"location":"clusterers/k-means.html#references","text":"D. Sculley. (2010). Web-Scale K-Means Clustering. \u21a9","title":"References"},{"location":"clusterers/mean-shift.html","text":"[source] Mean Shift # A hierarchical clustering algorithm that uses peak (maxima) finding to locate the candidate centroids of a training set given a radius constraint. Near-duplicate centroids are merged together and the algorithm iterates on the remaining candidates in subsequent steps until the centroids stabilize. Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 radius float The bandwidth of the radial basis function. 2 ratio 0.1 float The ratio of samples from the training set to use as initial centroids. 3 epochs 100 int The maximum number of training rounds to execute. 4 minShift 1e-4 float The minimum shift in the position of the centroids necessary to continue training. 5 tree BallTree Spatial The spatial tree used to run range searches. 6 seeder Random Seeder The seeder used to initialize the cluster centroids. Example # use Rubix\\ML\\Clusterers\\MeanShift ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Clusterers\\Seeders\\KMC2 ; $estimator = new MeanShift ( 2.5 , 2000 , 1e-6 , 0.05 , new BallTree ( 100 ), new KMC2 ()); Additional Methods # Estimate the radius of a cluster that encompasses a certain percentage of the total training samples: public static estimateRadius ( Dataset $dataset , float $percentile = 30.0 , ? Distance $kernel = null ) : float Note Since radius estimation scales quadratically in the number of samples, for large datasets you can speed up the process by running it on a smaller subset of the training data. Return the centroids computed from the training set: public centroids () : array [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Returns the amount of centroid shift during each epoch of training: public losses () : float [] | null References # M. A. Carreira-Perpinan et al. (2015). A Review of Mean-shift Algorithms for Clustering. \u21a9 D. Comaniciu et al. (2012). Mean Shift: A Robust Approach Toward Feature Space Analysis. \u21a9","title":"Mean Shift"},{"location":"clusterers/mean-shift.html#mean-shift","text":"A hierarchical clustering algorithm that uses peak (maxima) finding to locate the candidate centroids of a training set given a radius constraint. Near-duplicate centroids are merged together and the algorithm iterates on the remaining candidates in subsequent steps until the centroids stabilize. Interfaces: Estimator , Learner , Probabilistic , Verbose , Persistable Data Type Compatibility: Continuous","title":"Mean Shift"},{"location":"clusterers/mean-shift.html#parameters","text":"# Name Default Type Description 1 radius float The bandwidth of the radial basis function. 2 ratio 0.1 float The ratio of samples from the training set to use as initial centroids. 3 epochs 100 int The maximum number of training rounds to execute. 4 minShift 1e-4 float The minimum shift in the position of the centroids necessary to continue training. 5 tree BallTree Spatial The spatial tree used to run range searches. 6 seeder Random Seeder The seeder used to initialize the cluster centroids.","title":"Parameters"},{"location":"clusterers/mean-shift.html#example","text":"use Rubix\\ML\\Clusterers\\MeanShift ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Clusterers\\Seeders\\KMC2 ; $estimator = new MeanShift ( 2.5 , 2000 , 1e-6 , 0.05 , new BallTree ( 100 ), new KMC2 ());","title":"Example"},{"location":"clusterers/mean-shift.html#additional-methods","text":"Estimate the radius of a cluster that encompasses a certain percentage of the total training samples: public static estimateRadius ( Dataset $dataset , float $percentile = 30.0 , ? Distance $kernel = null ) : float Note Since radius estimation scales quadratically in the number of samples, for large datasets you can speed up the process by running it on a smaller subset of the training data. Return the centroids computed from the training set: public centroids () : array [] Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Returns the amount of centroid shift during each epoch of training: public losses () : float [] | null","title":"Additional Methods"},{"location":"clusterers/mean-shift.html#references","text":"M. A. Carreira-Perpinan et al. (2015). A Review of Mean-shift Algorithms for Clustering. \u21a9 D. Comaniciu et al. (2012). Mean Shift: A Robust Approach Toward Feature Space Analysis. \u21a9","title":"References"},{"location":"clusterers/seeders/k-mc2.html","text":"[source] K-MC2 # A fast Plus Plus approximator that replaces the brute force method with a substantially faster Markov Chain Monte Carlo (MCMC) sampling procedure with comparable results. Parameters # # Name Default Type Description 1 m 50 int The number of candidate nodes in the Markov Chain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between samples. Example # use Rubix\\ML\\Clusterers\\Seeders\\KMC2 ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $seeder = new KMC2 ( 200 , new Euclidean ()); # O. Bachem et al. (2016). Approximate K-Means++ in Sublinear Time. \u21a9","title":"K-MC2"},{"location":"clusterers/seeders/k-mc2.html#k-mc2","text":"A fast Plus Plus approximator that replaces the brute force method with a substantially faster Markov Chain Monte Carlo (MCMC) sampling procedure with comparable results.","title":"K-MC2"},{"location":"clusterers/seeders/k-mc2.html#parameters","text":"# Name Default Type Description 1 m 50 int The number of candidate nodes in the Markov Chain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between samples.","title":"Parameters"},{"location":"clusterers/seeders/k-mc2.html#example","text":"use Rubix\\ML\\Clusterers\\Seeders\\KMC2 ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $seeder = new KMC2 ( 200 , new Euclidean ());","title":"Example"},{"location":"clusterers/seeders/k-mc2.html#_1","text":"O. Bachem et al. (2016). Approximate K-Means++ in Sublinear Time. \u21a9","title":""},{"location":"clusterers/seeders/plus-plus.html","text":"[source] Plus Plus # This seeder attempts to maximize the chances of seeding distant clusters while still remaining random. It does so by sequentially selecting random samples weighted by their distance from the previous seed. Parameters # # Name Default Type Description 1 kernel Euclidean Distance The distance kernel used to compute the distance between samples. Example # use Rubix\\ML\\Clusterers\\Seeders\\PlusPlus ; use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $seeder = new PlusPlus ( new Minkowski ( 5.0 )); References # D. Arthur et al. (2006). k-means++: The Advantages of Careful Seeding. \u21a9 A. Stetco et al. (2015). Fuzzy C-means++: Fuzzy C-means with effective seeding initialization. \u21a9","title":"Plus Plus"},{"location":"clusterers/seeders/plus-plus.html#plus-plus","text":"This seeder attempts to maximize the chances of seeding distant clusters while still remaining random. It does so by sequentially selecting random samples weighted by their distance from the previous seed.","title":"Plus Plus"},{"location":"clusterers/seeders/plus-plus.html#parameters","text":"# Name Default Type Description 1 kernel Euclidean Distance The distance kernel used to compute the distance between samples.","title":"Parameters"},{"location":"clusterers/seeders/plus-plus.html#example","text":"use Rubix\\ML\\Clusterers\\Seeders\\PlusPlus ; use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $seeder = new PlusPlus ( new Minkowski ( 5.0 ));","title":"Example"},{"location":"clusterers/seeders/plus-plus.html#references","text":"D. Arthur et al. (2006). k-means++: The Advantages of Careful Seeding. \u21a9 A. Stetco et al. (2015). Fuzzy C-means++: Fuzzy C-means with effective seeding initialization. \u21a9","title":"References"},{"location":"clusterers/seeders/preset.html","text":"[source] Preset # Generates centroids from a list of presets. Parameters # # Name Default Type Description 1 centroids array A list of predefined cluster centroids to sample from. Example # use Rubix\\ML\\Clusterers\\Seeders\\Preset ; $seeder = new Preset ([ [ 'foo' , 14 , 0.72 ], [ 'bar' , 16 , 0.92 ], ]);","title":"Preset"},{"location":"clusterers/seeders/preset.html#preset","text":"Generates centroids from a list of presets.","title":"Preset"},{"location":"clusterers/seeders/preset.html#parameters","text":"# Name Default Type Description 1 centroids array A list of predefined cluster centroids to sample from.","title":"Parameters"},{"location":"clusterers/seeders/preset.html#example","text":"use Rubix\\ML\\Clusterers\\Seeders\\Preset ; $seeder = new Preset ([ [ 'foo' , 14 , 0.72 ], [ 'bar' , 16 , 0.92 ], ]);","title":"Example"},{"location":"clusterers/seeders/random.html","text":"[source] Random # Completely random selection of seeds from a given dataset. Parameters # This seeder does not have any parameters. Example # use Rubix\\ML\\Clusterers\\Seeders\\Random ; $seeder = new Random ();","title":"Random"},{"location":"clusterers/seeders/random.html#random","text":"Completely random selection of seeds from a given dataset.","title":"Random"},{"location":"clusterers/seeders/random.html#parameters","text":"This seeder does not have any parameters.","title":"Parameters"},{"location":"clusterers/seeders/random.html#example","text":"use Rubix\\ML\\Clusterers\\Seeders\\Random ; $seeder = new Random ();","title":"Example"},{"location":"cross-validation/api.html","text":"Validator # Validators take an instance of a Learner , a Labeled dataset object, and a validation Metric and return a validation score that measures the generalization performance of the model using one of various cross validation techniques. Note There is no need to train the learner beforehand. The validator will automatically train the learner on subsets of the dataset created by the testing algorithm. Test a Learner # To train and test a Learner on a dataset and return the validation score: public test ( Learner $estimator , Labeled $dataset , Metric $metric ) : float use Rubix\\ML\\CrossValidation\\KFold ; use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $validator = new KFold ( 10 ); $score = $validator -> test ( $estimator , $dataset , new Accuracy ()); echo $score ; 0.75","title":"API Reference"},{"location":"cross-validation/api.html#validator","text":"Validators take an instance of a Learner , a Labeled dataset object, and a validation Metric and return a validation score that measures the generalization performance of the model using one of various cross validation techniques. Note There is no need to train the learner beforehand. The validator will automatically train the learner on subsets of the dataset created by the testing algorithm.","title":"Validator"},{"location":"cross-validation/api.html#test-a-learner","text":"To train and test a Learner on a dataset and return the validation score: public test ( Learner $estimator , Labeled $dataset , Metric $metric ) : float use Rubix\\ML\\CrossValidation\\KFold ; use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $validator = new KFold ( 10 ); $score = $validator -> test ( $estimator , $dataset , new Accuracy ()); echo $score ; 0.75","title":"Test a Learner"},{"location":"cross-validation/hold-out.html","text":"[source] Hold Out # Hold Out is a quick and simple cross validation technique that uses a validation set that is held out from the training data. The advantages of Hold Out is that the validation score is quick to compute, however it does not allow the learner to both train and test on all the data in the training set. Interfaces: Validator Parameters # # Name Default Type Description 1 ratio 0.2 float The ratio of samples to hold out for testing. Example # use Rubix\\ML\\CrossValidation\\HoldOut ; $validator = new HoldOut ( 0.3 );","title":"Hold Out"},{"location":"cross-validation/hold-out.html#hold-out","text":"Hold Out is a quick and simple cross validation technique that uses a validation set that is held out from the training data. The advantages of Hold Out is that the validation score is quick to compute, however it does not allow the learner to both train and test on all the data in the training set. Interfaces: Validator","title":"Hold Out"},{"location":"cross-validation/hold-out.html#parameters","text":"# Name Default Type Description 1 ratio 0.2 float The ratio of samples to hold out for testing.","title":"Parameters"},{"location":"cross-validation/hold-out.html#example","text":"use Rubix\\ML\\CrossValidation\\HoldOut ; $validator = new HoldOut ( 0.3 );","title":"Example"},{"location":"cross-validation/k-fold.html","text":"[source] K Fold # K Fold is a cross validation technique that splits the training set into k individual folds and for each training round uses 1 of the folds to test the model and the rest as training data. The final score is the average validation score over all of the k rounds. K Fold has the advantage of both training and testing on each sample in the dataset at least once. Interfaces: Validator , Parallel Parameters # # Name Default Type Description 1 k 5 int The number of folds to split the dataset into. Example # use Rubix\\ML\\CrossValidation\\KFold ; $validator = new KFold ( 5 , true );","title":"K Fold"},{"location":"cross-validation/k-fold.html#k-fold","text":"K Fold is a cross validation technique that splits the training set into k individual folds and for each training round uses 1 of the folds to test the model and the rest as training data. The final score is the average validation score over all of the k rounds. K Fold has the advantage of both training and testing on each sample in the dataset at least once. Interfaces: Validator , Parallel","title":"K Fold"},{"location":"cross-validation/k-fold.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of folds to split the dataset into.","title":"Parameters"},{"location":"cross-validation/k-fold.html#example","text":"use Rubix\\ML\\CrossValidation\\KFold ; $validator = new KFold ( 5 , true );","title":"Example"},{"location":"cross-validation/leave-p-out.html","text":"[source] Leave P Out # Leave P Out tests a learner with a unique holdout set of size p for each iteration until all samples have been tested. Although Leave P Out can take long with large datasets and small values of p, it is especially suited for small datasets. Interfaces: Validator , Parallel Parameters # # Name Default Type Description 1 p 10 int The number of samples to leave out each round for testing. Example # use Rubix\\ML\\CrossValidation\\LeavePOut ; $validator = new LeavePOut ( 50 );","title":"Leave P Out"},{"location":"cross-validation/leave-p-out.html#leave-p-out","text":"Leave P Out tests a learner with a unique holdout set of size p for each iteration until all samples have been tested. Although Leave P Out can take long with large datasets and small values of p, it is especially suited for small datasets. Interfaces: Validator , Parallel","title":"Leave P Out"},{"location":"cross-validation/leave-p-out.html#parameters","text":"# Name Default Type Description 1 p 10 int The number of samples to leave out each round for testing.","title":"Parameters"},{"location":"cross-validation/leave-p-out.html#example","text":"use Rubix\\ML\\CrossValidation\\LeavePOut ; $validator = new LeavePOut ( 50 );","title":"Example"},{"location":"cross-validation/monte-carlo.html","text":"[source] Monte Carlo # Monte Carlo cross validation (or repeated random subsampling ) is a technique that averages the validation score of a learner over a user-defined number of simulations where the learner is trained and tested on random splits of the dataset. The estimated validation score approaches the actual validation score as the number of simulations goes to infinity, however, only a tiny fraction of all possible simulations are needed to produce a pretty good approximation. Interfaces: Validator , Parallel Parameters # # Name Default Type Description 1 simulations 10 int The number of simulations i.e. random subsamplings of the dataset. 2 ratio 0.2 float The ratio of samples to hold out for testing. Example # use Rubix\\ML\\CrossValidation\\MonteCarlo ; $validator = new MonteCarlo ( 30 , 0.1 );","title":"Monte Carlo"},{"location":"cross-validation/monte-carlo.html#monte-carlo","text":"Monte Carlo cross validation (or repeated random subsampling ) is a technique that averages the validation score of a learner over a user-defined number of simulations where the learner is trained and tested on random splits of the dataset. The estimated validation score approaches the actual validation score as the number of simulations goes to infinity, however, only a tiny fraction of all possible simulations are needed to produce a pretty good approximation. Interfaces: Validator , Parallel","title":"Monte Carlo"},{"location":"cross-validation/monte-carlo.html#parameters","text":"# Name Default Type Description 1 simulations 10 int The number of simulations i.e. random subsamplings of the dataset. 2 ratio 0.2 float The ratio of samples to hold out for testing.","title":"Parameters"},{"location":"cross-validation/monte-carlo.html#example","text":"use Rubix\\ML\\CrossValidation\\MonteCarlo ; $validator = new MonteCarlo ( 30 , 0.1 );","title":"Example"},{"location":"cross-validation/metrics/accuracy.html","text":"[source] Accuracy # A quick and simple classification and anomaly detection metric defined as the number of true positives over the number of samples in the testing set. Since Accuracy gives equal weight to false positives and false negatives, it is not a good metric for datasets with a highly imbalanced distribution of labels. \\[ {\\displaystyle Accuracy = \\frac{TP}{TP + FP}} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: 0 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $metric = new Accuracy ();","title":"Accuracy"},{"location":"cross-validation/metrics/accuracy.html#accuracy","text":"A quick and simple classification and anomaly detection metric defined as the number of true positives over the number of samples in the testing set. Since Accuracy gives equal weight to false positives and false negatives, it is not a good metric for datasets with a highly imbalanced distribution of labels. \\[ {\\displaystyle Accuracy = \\frac{TP}{TP + FP}} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: 0 to 1","title":"Accuracy"},{"location":"cross-validation/metrics/accuracy.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/accuracy.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\Accuracy ; $metric = new Accuracy ();","title":"Example"},{"location":"cross-validation/metrics/api.html","text":"Metrics # Validation metrics are for used evaluating the generalization performance of an estimator. They output a score based on the predictions and known ground-truth labels. Note Some regression metrics output the negative of their value to maintain the convention that scores get better as they increase . Scoring Predictions # To compute a validation score, pass in the predictions from an estimator along with their expected labels. public score ( array $predictions , array $labels ) : float use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $predictions = $estimator -> predict ( $dataset ); $metric = new FBeta ( 1.0 ); $score = $metric -> score ( $predictions , $dataset -> labels ()); echo $score ; 0.88 Scoring Probabilities # Metrics that implement the ProbabilisticMetric interface calculate a validation score derived from the estimated probabilities of a Probabilistic estimator and their corresponding ground-truth labels. public score ( array $probabilities , array $labels ) : float use Rubix\\ML\\CrossValidation\\Metrics\\ProbabilisticAccuracy ; $probabilities = $estimator -> proba ( $dataset ); $metric = new ProbabilisticAccuracy ; $score = $metric -> score ( $probabilities , $dataset -> labels ()); Score Range # Output the minimum and maximum value the validation score can take in a 2-tuple . public range () : Rubix\\ML\\Tuple { float , float } [ $min , $max ] = $metric -> range () -> list (); echo \"min: $min , max: $max \" ; min: 0.0, max: 1.0","title":"API Reference"},{"location":"cross-validation/metrics/api.html#metrics","text":"Validation metrics are for used evaluating the generalization performance of an estimator. They output a score based on the predictions and known ground-truth labels. Note Some regression metrics output the negative of their value to maintain the convention that scores get better as they increase .","title":"Metrics"},{"location":"cross-validation/metrics/api.html#scoring-predictions","text":"To compute a validation score, pass in the predictions from an estimator along with their expected labels. public score ( array $predictions , array $labels ) : float use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $predictions = $estimator -> predict ( $dataset ); $metric = new FBeta ( 1.0 ); $score = $metric -> score ( $predictions , $dataset -> labels ()); echo $score ; 0.88","title":"Scoring Predictions"},{"location":"cross-validation/metrics/api.html#scoring-probabilities","text":"Metrics that implement the ProbabilisticMetric interface calculate a validation score derived from the estimated probabilities of a Probabilistic estimator and their corresponding ground-truth labels. public score ( array $probabilities , array $labels ) : float use Rubix\\ML\\CrossValidation\\Metrics\\ProbabilisticAccuracy ; $probabilities = $estimator -> proba ( $dataset ); $metric = new ProbabilisticAccuracy ; $score = $metric -> score ( $probabilities , $dataset -> labels ());","title":"Scoring Probabilities"},{"location":"cross-validation/metrics/api.html#score-range","text":"Output the minimum and maximum value the validation score can take in a 2-tuple . public range () : Rubix\\ML\\Tuple { float , float } [ $min , $max ] = $metric -> range () -> list (); echo \"min: $min , max: $max \" ; min: 0.0, max: 1.0","title":"Score Range"},{"location":"cross-validation/metrics/brier-score.html","text":"[source] Brier Score # Brier Score is a strictly proper scoring metric that is equivalent to applying mean squared error to the probabilities of a probabilistic estimator. Note Metric assumes probabilities are between 0 and 1 and their joint distribution sums to 1. Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Probabilistic Classifier Output Range: -2 to 0 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\BrierScore ; $metric = new BrierScore (); References # G. W. Brier. (1950). Verification of Forecasts Expresses in Terms of Probability. \u21a9","title":"Brier Score"},{"location":"cross-validation/metrics/brier-score.html#brier-score","text":"Brier Score is a strictly proper scoring metric that is equivalent to applying mean squared error to the probabilities of a probabilistic estimator. Note Metric assumes probabilities are between 0 and 1 and their joint distribution sums to 1. Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Probabilistic Classifier Output Range: -2 to 0","title":"Brier Score"},{"location":"cross-validation/metrics/brier-score.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/brier-score.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\BrierScore ; $metric = new BrierScore ();","title":"Example"},{"location":"cross-validation/metrics/brier-score.html#references","text":"G. W. Brier. (1950). Verification of Forecasts Expresses in Terms of Probability. \u21a9","title":"References"},{"location":"cross-validation/metrics/completeness.html","text":"[source] Completeness # A ground-truth clustering metric that measures the ratio of samples in a class that are also members of the same cluster. A cluster is said to be complete when all the samples in a class are contained in a cluster. \\[ {\\displaystyle Completeness = 1-\\frac{H(K, C)}{H(K)}} \\] Note Since this metric monotonically improves as the number of target clusters decreases, it should not be used as a metric to guide hyper-parameter tuning. Estimator Compatibility: Clusterer Output Range: 0 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\Completeness ; $metric = new Completeness ();","title":"Completeness"},{"location":"cross-validation/metrics/completeness.html#completeness","text":"A ground-truth clustering metric that measures the ratio of samples in a class that are also members of the same cluster. A cluster is said to be complete when all the samples in a class are contained in a cluster. \\[ {\\displaystyle Completeness = 1-\\frac{H(K, C)}{H(K)}} \\] Note Since this metric monotonically improves as the number of target clusters decreases, it should not be used as a metric to guide hyper-parameter tuning. Estimator Compatibility: Clusterer Output Range: 0 to 1","title":"Completeness"},{"location":"cross-validation/metrics/completeness.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/completeness.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\Completeness ; $metric = new Completeness ();","title":"Example"},{"location":"cross-validation/metrics/f-beta.html","text":"[source] F-Beta # A weighted harmonic mean of precision and recall, F-Beta is a both a versatile and balanced metric. The beta parameter controls the weight of precision in the combined score. As beta goes to infinity the score only considers recall, whereas when it goes to 0 it only considers precision. When beta is equal to 1, this metric is called an F1 score. \\[ {\\displaystyle F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: 0 to 1 Parameters # # Name Default Type Description 1 beta 1.0 float The ratio of weight given to precision over recall. Example # use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $metric = new FBeta ( 0.7 );","title":"F Beta"},{"location":"cross-validation/metrics/f-beta.html#f-beta","text":"A weighted harmonic mean of precision and recall, F-Beta is a both a versatile and balanced metric. The beta parameter controls the weight of precision in the combined score. As beta goes to infinity the score only considers recall, whereas when it goes to 0 it only considers precision. When beta is equal to 1, this metric is called an F1 score. \\[ {\\displaystyle F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: 0 to 1","title":"F-Beta"},{"location":"cross-validation/metrics/f-beta.html#parameters","text":"# Name Default Type Description 1 beta 1.0 float The ratio of weight given to precision over recall.","title":"Parameters"},{"location":"cross-validation/metrics/f-beta.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\FBeta ; $metric = new FBeta ( 0.7 );","title":"Example"},{"location":"cross-validation/metrics/homogeneity.html","text":"[source] Homogeneity # A ground-truth clustering metric that measures the ratio of samples in a cluster that are also members of the same class. A cluster is said to be homogeneous when the entire cluster is comprised of a single class of samples. \\[ {\\displaystyle Homogeneity = 1-\\frac{H(C, K)}{H(C)}} \\] Note Since this metric monotonically improves as the number of target clusters increases, it should not be used as a metric to guide hyper-parameter tuning. Estimator Compatibility: Clusterer Output Range: 0 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\Homogeneity ; $metric = new Homogeneity ();","title":"Homogeneity"},{"location":"cross-validation/metrics/homogeneity.html#homogeneity","text":"A ground-truth clustering metric that measures the ratio of samples in a cluster that are also members of the same class. A cluster is said to be homogeneous when the entire cluster is comprised of a single class of samples. \\[ {\\displaystyle Homogeneity = 1-\\frac{H(C, K)}{H(C)}} \\] Note Since this metric monotonically improves as the number of target clusters increases, it should not be used as a metric to guide hyper-parameter tuning. Estimator Compatibility: Clusterer Output Range: 0 to 1","title":"Homogeneity"},{"location":"cross-validation/metrics/homogeneity.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/homogeneity.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\Homogeneity ; $metric = new Homogeneity ();","title":"Example"},{"location":"cross-validation/metrics/informedness.html","text":"[source] Informedness # Informedness a multiclass generalization of Youden's J Statistic and can be interpreted as the probability that an estimator will make an informed prediction. Its value ranges from -1 through 1 and has a value of 0 when the test yields no useful information. \\[ {\\displaystyle Informedness = {\\frac {\\text{TP}}{{\\text{TP}}+{\\text{FN}}}}+{\\frac {\\text{TP}}{{\\text{TN}}+{\\text{FP}}}}-1} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: -1 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\Informedness ; $metric = new Informedness (); References # W. J. Youden. (1950). Index for Rating Diagnostic Tests. \u21a9","title":"Informedness"},{"location":"cross-validation/metrics/informedness.html#informedness","text":"Informedness a multiclass generalization of Youden's J Statistic and can be interpreted as the probability that an estimator will make an informed prediction. Its value ranges from -1 through 1 and has a value of 0 when the test yields no useful information. \\[ {\\displaystyle Informedness = {\\frac {\\text{TP}}{{\\text{TP}}+{\\text{FN}}}}+{\\frac {\\text{TP}}{{\\text{TN}}+{\\text{FP}}}}-1} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: -1 to 1","title":"Informedness"},{"location":"cross-validation/metrics/informedness.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/informedness.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\Informedness ; $metric = new Informedness ();","title":"Example"},{"location":"cross-validation/metrics/informedness.html#references","text":"W. J. Youden. (1950). Index for Rating Diagnostic Tests. \u21a9","title":"References"},{"location":"cross-validation/metrics/mcc.html","text":"[source] MCC # Matthews Correlation Coefficient (MCC) measures the quality of a classification by taking true and false positives and negatives into account. It is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. A coefficient of 1 represents a perfect prediction, 0 no better than random prediction, and \u22121 indicates total disagreement between prediction and observation. \\[ {\\displaystyle \\mathrm {MCC} = {\\frac {\\mathrm {TP} \\times \\mathrm {TN} -\\mathrm {FP} \\times \\mathrm {FN} }{\\sqrt {(\\mathrm {TP} +\\mathrm {FP} )(\\mathrm {TP} +\\mathrm {FN} )(\\mathrm {TN} +\\mathrm {FP} )(\\mathrm {TN} +\\mathrm {FN} )}}}} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: -1 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\MCC ; $metric = new MCC (); References # B. W. Matthews. (1975). Decision of the Predicted and Observed Secondary Structure of T4 Phage Lysozyme. \u21a9","title":"MCC"},{"location":"cross-validation/metrics/mcc.html#mcc","text":"Matthews Correlation Coefficient (MCC) measures the quality of a classification by taking true and false positives and negatives into account. It is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. A coefficient of 1 represents a perfect prediction, 0 no better than random prediction, and \u22121 indicates total disagreement between prediction and observation. \\[ {\\displaystyle \\mathrm {MCC} = {\\frac {\\mathrm {TP} \\times \\mathrm {TN} -\\mathrm {FP} \\times \\mathrm {FN} }{\\sqrt {(\\mathrm {TP} +\\mathrm {FP} )(\\mathrm {TP} +\\mathrm {FN} )(\\mathrm {TN} +\\mathrm {FP} )(\\mathrm {TN} +\\mathrm {FN} )}}}} \\] Estimator Compatibility: Classifier, Anomaly Detector Output Range: -1 to 1","title":"MCC"},{"location":"cross-validation/metrics/mcc.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/mcc.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\MCC ; $metric = new MCC ();","title":"Example"},{"location":"cross-validation/metrics/mcc.html#references","text":"B. W. Matthews. (1975). Decision of the Predicted and Observed Secondary Structure of T4 Phage Lysozyme. \u21a9","title":"References"},{"location":"cross-validation/metrics/mean-absolute-error.html","text":"[source] Mean Absolute Error # A scale-dependent metric that measures the average absolute error between a set of predictions and their ground-truth labels. One of the nice properties of MAE is that it has the same units of measurement as the labels being estimated. \\[ {\\displaystyle \\mathrm {MAE} = {\\frac {1}{n}}{\\sum _{i=1}^{n}\\left |Y_{i}-\\hat {Y_{i}}\\right|}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\MeanAbsoluteError ; $metric = new MeanAbsoluteError ();","title":"Mean Absolute Error"},{"location":"cross-validation/metrics/mean-absolute-error.html#mean-absolute-error","text":"A scale-dependent metric that measures the average absolute error between a set of predictions and their ground-truth labels. One of the nice properties of MAE is that it has the same units of measurement as the labels being estimated. \\[ {\\displaystyle \\mathrm {MAE} = {\\frac {1}{n}}{\\sum _{i=1}^{n}\\left |Y_{i}-\\hat {Y_{i}}\\right|}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0","title":"Mean Absolute Error"},{"location":"cross-validation/metrics/mean-absolute-error.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/mean-absolute-error.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\MeanAbsoluteError ; $metric = new MeanAbsoluteError ();","title":"Example"},{"location":"cross-validation/metrics/mean-squared-error.html","text":"[source] Mean Squared Error # A scale-dependent regression metric that gives greater weight to error scores the worse they are. Formally, Mean Squared Error (MSE) is the average of the squared differences between a set of predictions and their target labels. \\[ {\\displaystyle \\operatorname {MSE} = {\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\MeanSquaredError ; $metric = new MeanSquaredError ();","title":"Mean Squared Error"},{"location":"cross-validation/metrics/mean-squared-error.html#mean-squared-error","text":"A scale-dependent regression metric that gives greater weight to error scores the worse they are. Formally, Mean Squared Error (MSE) is the average of the squared differences between a set of predictions and their target labels. \\[ {\\displaystyle \\operatorname {MSE} = {\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0","title":"Mean Squared Error"},{"location":"cross-validation/metrics/mean-squared-error.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/mean-squared-error.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\MeanSquaredError ; $metric = new MeanSquaredError ();","title":"Example"},{"location":"cross-validation/metrics/median-absolute-error.html","text":"[source] Median Absolute Error # Median Absolute Error (MAD) is a robust measure of error, similar to MAE , that ignores highly erroneous predictions. Since MAD is a robust statistic, it works well even when used to measure non-normal distributions. \\[ {\\displaystyle \\operatorname {MAD} = \\operatorname {median} (|Y_{i}-{\\tilde {Y}}|)} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\MedianAbsoluteError ; $metric = new MedianAbsoluteError ();","title":"Median Absolute Error"},{"location":"cross-validation/metrics/median-absolute-error.html#median-absolute-error","text":"Median Absolute Error (MAD) is a robust measure of error, similar to MAE , that ignores highly erroneous predictions. Since MAD is a robust statistic, it works well even when used to measure non-normal distributions. \\[ {\\displaystyle \\operatorname {MAD} = \\operatorname {median} (|Y_{i}-{\\tilde {Y}}|)} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0","title":"Median Absolute Error"},{"location":"cross-validation/metrics/median-absolute-error.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/median-absolute-error.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\MedianAbsoluteError ; $metric = new MedianAbsoluteError ();","title":"Example"},{"location":"cross-validation/metrics/probabilistic-accuracy.html","text":"[source] Probabilistic Accuracy # This metric comes from the sports betting domain, where it's used to measure the accuracy of predictions by looking at the probabilities of class predictions. Accordingly, this metric places additional weight on the \"confidence\" of each prediction. Note Metric assumes probabilities are between 0 and 1 and their joint distribution sums to 1. Estimator Compatibility: Probabilistic Classifier Output Range: 0 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\ProbabilisticAccuracy ; $metric = new ProbabilisticAccuracy (); References # https://mercurius.io/en/learn/predicting-forecasting-football \u21a9","title":"Probabilistic Accuracy"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#probabilistic-accuracy","text":"This metric comes from the sports betting domain, where it's used to measure the accuracy of predictions by looking at the probabilities of class predictions. Accordingly, this metric places additional weight on the \"confidence\" of each prediction. Note Metric assumes probabilities are between 0 and 1 and their joint distribution sums to 1. Estimator Compatibility: Probabilistic Classifier Output Range: 0 to 1","title":"Probabilistic Accuracy"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\ProbabilisticAccuracy ; $metric = new ProbabilisticAccuracy ();","title":"Example"},{"location":"cross-validation/metrics/probabilistic-accuracy.html#references","text":"https://mercurius.io/en/learn/predicting-forecasting-football \u21a9","title":"References"},{"location":"cross-validation/metrics/r-squared.html","text":"[source] R Squared # The coefficient of determination or R Squared (R\u00b2) is the proportion of the variance in the target labels that is explainable from the predictions. It gives an indication as to how well the predictions approximate the labels. \\[ {\\displaystyle R^{2} = 1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}} \\] Estimator Compatibility: Regressor Output Range: -\u221e to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\RSquared ; $metric = new RSquared ();","title":"R Squared"},{"location":"cross-validation/metrics/r-squared.html#r-squared","text":"The coefficient of determination or R Squared (R\u00b2) is the proportion of the variance in the target labels that is explainable from the predictions. It gives an indication as to how well the predictions approximate the labels. \\[ {\\displaystyle R^{2} = 1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}} \\] Estimator Compatibility: Regressor Output Range: -\u221e to 1","title":"R Squared"},{"location":"cross-validation/metrics/r-squared.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/r-squared.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\RSquared ; $metric = new RSquared ();","title":"Example"},{"location":"cross-validation/metrics/rand-index.html","text":"[source] Rand Index # The Adjusted Rand Index is a measure of similarity between a clustering and some ground-truth that is adjusted for chance. It considers all pairs of samples that are assigned in the same or different clusters in the predicted and empirical clusterings. \\[ {\\displaystyle ARI = {\\frac {\\left.\\sum _{ij}{\\binom {n_{ij}}{2}}-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}{\\left.{\\frac {1}{2}}\\left[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}\\right]-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}}} \\] Estimator Compatibility: Regressor Output Range: -1 to 1 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\RandIndex ; $metric = new RandIndex (); References # W. M. Rand. (1971). Objective Criteria for the Evaluation of Clustering Methods. \u21a9","title":"Rand Index"},{"location":"cross-validation/metrics/rand-index.html#rand-index","text":"The Adjusted Rand Index is a measure of similarity between a clustering and some ground-truth that is adjusted for chance. It considers all pairs of samples that are assigned in the same or different clusters in the predicted and empirical clusterings. \\[ {\\displaystyle ARI = {\\frac {\\left.\\sum _{ij}{\\binom {n_{ij}}{2}}-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}{\\left.{\\frac {1}{2}}\\left[\\sum _{i}{\\binom {a_{i}}{2}}+\\sum _{j}{\\binom {b_{j}}{2}}\\right]-\\left[\\sum _{i}{\\binom {a_{i}}{2}}\\sum _{j}{\\binom {b_{j}}{2}}\\right]\\right/{\\binom {n}{2}}}}} \\] Estimator Compatibility: Regressor Output Range: -1 to 1","title":"Rand Index"},{"location":"cross-validation/metrics/rand-index.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/rand-index.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\RandIndex ; $metric = new RandIndex ();","title":"Example"},{"location":"cross-validation/metrics/rand-index.html#references","text":"W. M. Rand. (1971). Objective Criteria for the Evaluation of Clustering Methods. \u21a9","title":"References"},{"location":"cross-validation/metrics/rmse.html","text":"[source] RMSE # The Root Mean Squared Error (RMSE) is equivalent to the standard deviation of the error residuals in a regression problem. Since RMSE is just the square root of the MSE , RMSE is also sensitive to outliers because larger errors have a disproportionately large effect on the score. \\[ {\\displaystyle \\operatorname {RMSE} = {\\sqrt{ \\frac {1}{n} \\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}}}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\RMSE ; $metric = new RMSE ();","title":"RMSE"},{"location":"cross-validation/metrics/rmse.html#rmse","text":"The Root Mean Squared Error (RMSE) is equivalent to the standard deviation of the error residuals in a regression problem. Since RMSE is just the square root of the MSE , RMSE is also sensitive to outliers because larger errors have a disproportionately large effect on the score. \\[ {\\displaystyle \\operatorname {RMSE} = {\\sqrt{ \\frac {1}{n} \\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}}}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -\u221e to 0","title":"RMSE"},{"location":"cross-validation/metrics/rmse.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/rmse.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\RMSE ; $metric = new RMSE ();","title":"Example"},{"location":"cross-validation/metrics/smape.html","text":"[source] SMAPE # Symmetric Mean Absolute Percentage Error (SMAPE) is a scale-independent regression metric that expresses the relative error of a set of predictions and their labels as a percentage. It is an improvement over the non-symmetric MAPE in that it is both upper and lower bounded. \\[ {\\displaystyle {\\text{SMAPE}} = {\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)/2}}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -100 to 0 Parameters # This metric does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE ; $metric = new SMAPE (); References # V. Kreinovich. et al. (2014). How to Estimate Forecasting Quality: A System Motivated Derivation of Symmetric Mean Absolute Percentage Error (SMAPE) and Other Similar Characteristics. \u21a9","title":"SMAPE"},{"location":"cross-validation/metrics/smape.html#smape","text":"Symmetric Mean Absolute Percentage Error (SMAPE) is a scale-independent regression metric that expresses the relative error of a set of predictions and their labels as a percentage. It is an improvement over the non-symmetric MAPE in that it is both upper and lower bounded. \\[ {\\displaystyle {\\text{SMAPE}} = {\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {\\left|F_{t}-A_{t}\\right|}{(|A_{t}|+|F_{t}|)/2}}} \\] Note In order to maintain the convention of maximizing validation scores, this metric outputs the negative of the original score. Estimator Compatibility: Regressor Output Range: -100 to 0","title":"SMAPE"},{"location":"cross-validation/metrics/smape.html#parameters","text":"This metric does not have any parameters.","title":"Parameters"},{"location":"cross-validation/metrics/smape.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE ; $metric = new SMAPE ();","title":"Example"},{"location":"cross-validation/metrics/smape.html#references","text":"V. Kreinovich. et al. (2014). How to Estimate Forecasting Quality: A System Motivated Derivation of Symmetric Mean Absolute Percentage Error (SMAPE) and Other Similar Characteristics. \u21a9","title":"References"},{"location":"cross-validation/metrics/v-measure.html","text":"[source] V Measure # V Measure is an entropy-based clustering metric that balances Homogeneity and Completeness . It has the additional property of being symmetric in that the predictions and ground-truth can be swapped without changing the score. \\[ {\\displaystyle V_{\\beta} = \\frac{(1+\\beta)hc}{\\beta h + c}} \\] Estimator Compatibility: Clusterer Output Range: 0 to 1 Parameters # # Name Default Type Description 1 beta 1.0 float The ratio of weight given to homogeneity over completeness. Example # use Rubix\\ML\\CrossValidation\\Metrics\\VMeasure ; $metric = new VMeasure ( 1.0 ); References # A. Rosenberg et al. (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. \u21a9","title":"V Measure"},{"location":"cross-validation/metrics/v-measure.html#v-measure","text":"V Measure is an entropy-based clustering metric that balances Homogeneity and Completeness . It has the additional property of being symmetric in that the predictions and ground-truth can be swapped without changing the score. \\[ {\\displaystyle V_{\\beta} = \\frac{(1+\\beta)hc}{\\beta h + c}} \\] Estimator Compatibility: Clusterer Output Range: 0 to 1","title":"V Measure"},{"location":"cross-validation/metrics/v-measure.html#parameters","text":"# Name Default Type Description 1 beta 1.0 float The ratio of weight given to homogeneity over completeness.","title":"Parameters"},{"location":"cross-validation/metrics/v-measure.html#example","text":"use Rubix\\ML\\CrossValidation\\Metrics\\VMeasure ; $metric = new VMeasure ( 1.0 );","title":"Example"},{"location":"cross-validation/metrics/v-measure.html#references","text":"A. Rosenberg et al. (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. \u21a9","title":"References"},{"location":"cross-validation/reports/aggregate-report.html","text":"[source] Aggregate Report # A report generator that aggregates the output of multiple reports. Estimator Compatibility: Depends on base reports Parameters # # Name Default Type Description 1 reports array An array of report generators to aggregate keyed by a user-specified name. Example # use Rubix\\ML\\CrossValidation\\Reports\\AggregateReport ; use Rubix\\ML\\CrossValidation\\Reports\\ConfusionMatrix ; use Rubix\\ML\\CrossValidation\\Reports\\MulticlassBreakdown ; $report = new AggregateReport ([ 'breakdown' => new MulticlassBreakdown (), 'matrix' => new ConfusionMatrix (), ]);","title":"Aggregate Report"},{"location":"cross-validation/reports/aggregate-report.html#aggregate-report","text":"A report generator that aggregates the output of multiple reports. Estimator Compatibility: Depends on base reports","title":"Aggregate Report"},{"location":"cross-validation/reports/aggregate-report.html#parameters","text":"# Name Default Type Description 1 reports array An array of report generators to aggregate keyed by a user-specified name.","title":"Parameters"},{"location":"cross-validation/reports/aggregate-report.html#example","text":"use Rubix\\ML\\CrossValidation\\Reports\\AggregateReport ; use Rubix\\ML\\CrossValidation\\Reports\\ConfusionMatrix ; use Rubix\\ML\\CrossValidation\\Reports\\MulticlassBreakdown ; $report = new AggregateReport ([ 'breakdown' => new MulticlassBreakdown (), 'matrix' => new ConfusionMatrix (), ]);","title":"Example"},{"location":"cross-validation/reports/api.html","text":"Report Generators # Report generators output detailed reports from a validation set and a set of predictions. They are used in cross-validation to ascertain the generalization performance of an estimator. Generate a Report # To generate a report from the predictions of an estimator given the ground truth labels: public generate ( array $predictions , array $labels ) : Report use Rubix\\ML\\Reports\\ConfusionMatrix ; $predictions = $estimator -> predict ( $dataset ); $report = new ConfusionMatrix (); $results = $report -> generate ( $predictions , $dataset -> labels ()); Report Objects # The results of a report will be returned in a Report object whose attributes can be accessed like an associative array. In addition, report objects can be echoed to the terminal or even written to a file. Printing the Report # To display the human-readable form of the report, you can echo it out to the terminal. echo $results ; { \"dog\" : { \"dog\" : 12 , \"cat\" : 3 , \"turtle\" : 0 } , \"cat\" : { \"dog\" : 2 , \"cat\" : 9 , \"turtle\" : 1 } , \"turtle\" : { \"dog\" : 1 , \"cat\" : 0 , \"turtle\" : 11 } } Accessing Report Attributes # You can access individual report attributes by treating the report object as an associative array. $accuracy = $results [ 'accuracy' ]; Encoding the Report # To return a JSON encoding that can be written to a file, call the toJSON() method on the report object. public toJSON ( bool $pretty = true ) : Encoding $encoding = $report -> toJSON ();","title":"API Reference"},{"location":"cross-validation/reports/api.html#report-generators","text":"Report generators output detailed reports from a validation set and a set of predictions. They are used in cross-validation to ascertain the generalization performance of an estimator.","title":"Report Generators"},{"location":"cross-validation/reports/api.html#generate-a-report","text":"To generate a report from the predictions of an estimator given the ground truth labels: public generate ( array $predictions , array $labels ) : Report use Rubix\\ML\\Reports\\ConfusionMatrix ; $predictions = $estimator -> predict ( $dataset ); $report = new ConfusionMatrix (); $results = $report -> generate ( $predictions , $dataset -> labels ());","title":"Generate a Report"},{"location":"cross-validation/reports/api.html#report-objects","text":"The results of a report will be returned in a Report object whose attributes can be accessed like an associative array. In addition, report objects can be echoed to the terminal or even written to a file.","title":"Report Objects"},{"location":"cross-validation/reports/api.html#printing-the-report","text":"To display the human-readable form of the report, you can echo it out to the terminal. echo $results ; { \"dog\" : { \"dog\" : 12 , \"cat\" : 3 , \"turtle\" : 0 } , \"cat\" : { \"dog\" : 2 , \"cat\" : 9 , \"turtle\" : 1 } , \"turtle\" : { \"dog\" : 1 , \"cat\" : 0 , \"turtle\" : 11 } }","title":"Printing the Report"},{"location":"cross-validation/reports/api.html#accessing-report-attributes","text":"You can access individual report attributes by treating the report object as an associative array. $accuracy = $results [ 'accuracy' ];","title":"Accessing Report Attributes"},{"location":"cross-validation/reports/api.html#encoding-the-report","text":"To return a JSON encoding that can be written to a file, call the toJSON() method on the report object. public toJSON ( bool $pretty = true ) : Encoding $encoding = $report -> toJSON ();","title":"Encoding the Report"},{"location":"cross-validation/reports/confusion-matrix.html","text":"[source] Confusion Matrix # A Confusion Matrix is a square matrix (table) that visualizes the true positives, false positives, true negatives, and false negatives of a set of predictions and their corresponding labels. Estimator Compatibility: Classifier, Anomaly Detector Parameters # This report does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Reports\\ConfusionMatrix ; $report = new ConfusionMatrix (); $result = $report -> generate ( $predictions , $labels ); echo $result ; { \"dog\" : { \"dog\" : 12 , \"cat\" : 3 , \"turtle\" : 0 }, \"cat\" : { \"dog\" : 2 , \"cat\" : 9 , \"turtle\" : 1 }, \"turtle\" : { \"dog\" : 1 , \"cat\" : 0 , \"turtle\" : 11 } }","title":"Confusion Matrix"},{"location":"cross-validation/reports/confusion-matrix.html#confusion-matrix","text":"A Confusion Matrix is a square matrix (table) that visualizes the true positives, false positives, true negatives, and false negatives of a set of predictions and their corresponding labels. Estimator Compatibility: Classifier, Anomaly Detector","title":"Confusion Matrix"},{"location":"cross-validation/reports/confusion-matrix.html#parameters","text":"This report does not have any parameters.","title":"Parameters"},{"location":"cross-validation/reports/confusion-matrix.html#example","text":"use Rubix\\ML\\CrossValidation\\Reports\\ConfusionMatrix ; $report = new ConfusionMatrix (); $result = $report -> generate ( $predictions , $labels ); echo $result ; { \"dog\" : { \"dog\" : 12 , \"cat\" : 3 , \"turtle\" : 0 }, \"cat\" : { \"dog\" : 2 , \"cat\" : 9 , \"turtle\" : 1 }, \"turtle\" : { \"dog\" : 1 , \"cat\" : 0 , \"turtle\" : 11 } }","title":"Example"},{"location":"cross-validation/reports/contingency-table.html","text":"[source] Contingency Table # A Contingency Table is used to display the frequency distribution of class labels among a clustering. It is similar to a Confusion Matrix but uses the labels to establish ground-truth for a clustering problem instead. Estimator Compatibility: Clusterer Parameters # This report does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Reports\\ContingencyTable ; $report = new ContingencyTable (); $result = $report -> generate ( $predictions , $labels ); echo $result ; [ { \"lamb\" : 11 , \"wolf\" : 2 }, { \"lamb\" : 1 , \"wolf\" : 5 } ]","title":"Contingency Table"},{"location":"cross-validation/reports/contingency-table.html#contingency-table","text":"A Contingency Table is used to display the frequency distribution of class labels among a clustering. It is similar to a Confusion Matrix but uses the labels to establish ground-truth for a clustering problem instead. Estimator Compatibility: Clusterer","title":"Contingency Table"},{"location":"cross-validation/reports/contingency-table.html#parameters","text":"This report does not have any parameters.","title":"Parameters"},{"location":"cross-validation/reports/contingency-table.html#example","text":"use Rubix\\ML\\CrossValidation\\Reports\\ContingencyTable ; $report = new ContingencyTable (); $result = $report -> generate ( $predictions , $labels ); echo $result ; [ { \"lamb\" : 11 , \"wolf\" : 2 }, { \"lamb\" : 1 , \"wolf\" : 5 } ]","title":"Example"},{"location":"cross-validation/reports/error-analysis.html","text":"[source] Error Analysis # The Error Analysis report measures the differences between the predicted and target values of a regression problem using multiple error measurements (MAE, MSE, RMSE, MAPE, etc.) as well as statistics regarding the distribution of errors. Estimator Compatibility: Regressor Parameters # This report does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Reports\\ErrorAnalysis ; $report = new ErrorAnalysis (); $results = $report -> generate ( $predictions , $labels ); echo $results ; { \"mean absolute error\" : 0.8 , \"median absolute error\" : 1 , \"mean squared error\" : 1 , \"mean absolute percentage error\" : 14.02077497665733 , \"rms error\" : 1 , \"mean squared log error\" : 0.019107097505647368 , \"r squared\" : 0.9958930551562692 , \"error mean\" : -0.2 , \"error standard deviation\" : 0.9898464007663 , \"error skewness\" : -0.22963966338592326 , \"error kurtosis\" : -1.0520833333333324 , \"error min\" : -2 , \"error 25%\" : -1.0 , \"error median\" : 0.0 , \"error 75%\" : 0.75 , \"error max\" : 1 , \"cardinality\" : 10 }","title":"Error Analysis"},{"location":"cross-validation/reports/error-analysis.html#error-analysis","text":"The Error Analysis report measures the differences between the predicted and target values of a regression problem using multiple error measurements (MAE, MSE, RMSE, MAPE, etc.) as well as statistics regarding the distribution of errors. Estimator Compatibility: Regressor","title":"Error Analysis"},{"location":"cross-validation/reports/error-analysis.html#parameters","text":"This report does not have any parameters.","title":"Parameters"},{"location":"cross-validation/reports/error-analysis.html#example","text":"use Rubix\\ML\\CrossValidation\\Reports\\ErrorAnalysis ; $report = new ErrorAnalysis (); $results = $report -> generate ( $predictions , $labels ); echo $results ; { \"mean absolute error\" : 0.8 , \"median absolute error\" : 1 , \"mean squared error\" : 1 , \"mean absolute percentage error\" : 14.02077497665733 , \"rms error\" : 1 , \"mean squared log error\" : 0.019107097505647368 , \"r squared\" : 0.9958930551562692 , \"error mean\" : -0.2 , \"error standard deviation\" : 0.9898464007663 , \"error skewness\" : -0.22963966338592326 , \"error kurtosis\" : -1.0520833333333324 , \"error min\" : -2 , \"error 25%\" : -1.0 , \"error median\" : 0.0 , \"error 75%\" : 0.75 , \"error max\" : 1 , \"cardinality\" : 10 }","title":"Example"},{"location":"cross-validation/reports/multiclass-breakdown.html","text":"[source] Multi-class Breakdown # A multiclass classification report that computes a number of metrics (Accuracy, Precision, Recall, etc.) derived from their confusion matrix on an overall and individual class basis. Estimator Compatibility: Classifier, Anomaly Detector Parameters # This report does not have any parameters. Example # use Rubix\\ML\\CrossValidation\\Reports\\MulticlassBreakdown ; $report = new MulticlassBreakdown (); $results = $report -> generate ( $predictions , $labels ); echo $results ; { \"overall\" : { \"accuracy\" : 0.6 , \"accuracy balanced\" : 0.5833333333333333 , \"f1 score\" : 0.5833333333333333 , \"precision\" : 0.5833333333333333 , \"recall\" : 0.5833333333333333 , \"specificity\" : 0.5833333333333333 , \"negative predictive value\" : 0.5833333333333333 , \"false discovery rate\" : 0.4166666666666667 , \"miss rate\" : 0.4166666666666667 , \"fall out\" : 0.4166666666666667 , \"false omission rate\" : 0.4166666666666667 , \"mcc\" : 0.16666666666666666 , \"informedness\" : 0.16666666666666652 , \"markedness\" : 0.16666666666666652 , \"true positives\" : 3 , \"true negatives\" : 3 , \"false positives\" : 2 , \"false negatives\" : 2 , \"cardinality\" : 5 }, \"classes\" : { \"wolf\" : { \"accuracy\" : 0.6 , \"accuracy balanced\" : 0.5833333333333333 , \"f1 score\" : 0.6666666666666666 , \"precision\" : 0.6666666666666666 , \"recall\" : 0.6666666666666666 , \"specificity\" : 0.5 , \"negative predictive value\" : 0.5 , \"false discovery rate\" : 0.33333333333333337 , \"miss rate\" : 0.33333333333333337 , \"fall out\" : 0.5 , \"false omission rate\" : 0.5 , \"informedness\" : 0.16666666666666652 , \"markedness\" : 0.16666666666666652 , \"mcc\" : 0.16666666666666666 , \"true positives\" : 2 , \"true negatives\" : 1 , \"false positives\" : 1 , \"false negatives\" : 1 , \"cardinality\" : 3 , \"proportion\" : 0.6 }, \"lamb\" : { \"accuracy\" : 0.6 , \"accuracy balanced\" : 0.5833333333333333 , \"f1 score\" : 0.5 , \"precision\" : 0.5 , \"recall\" : 0.5 , \"specificity\" : 0.6666666666666666 , \"negative predictive value\" : 0.6666666666666666 , \"false discovery rate\" : 0.5 , \"miss rate\" : 0.5 , \"fall out\" : 0.33333333333333337 , \"false omission rate\" : 0.33333333333333337 , \"informedness\" : 0.16666666666666652 , \"markedness\" : 0.16666666666666652 , \"mcc\" : 0.16666666666666666 , \"true positives\" : 1 , \"true negatives\" : 2 , \"false positives\" : 1 , \"false negatives\" : 1 , \"cardinality\" : 2 , \"proportion\" : 0.4 } } }","title":"Multiclass Breakdown"},{"location":"cross-validation/reports/multiclass-breakdown.html#multi-class-breakdown","text":"A multiclass classification report that computes a number of metrics (Accuracy, Precision, Recall, etc.) derived from their confusion matrix on an overall and individual class basis. Estimator Compatibility: Classifier, Anomaly Detector","title":"Multi-class Breakdown"},{"location":"cross-validation/reports/multiclass-breakdown.html#parameters","text":"This report does not have any parameters.","title":"Parameters"},{"location":"cross-validation/reports/multiclass-breakdown.html#example","text":"use Rubix\\ML\\CrossValidation\\Reports\\MulticlassBreakdown ; $report = new MulticlassBreakdown (); $results = $report -> generate ( $predictions , $labels ); echo $results ; { \"overall\" : { \"accuracy\" : 0.6 , \"accuracy balanced\" : 0.5833333333333333 , \"f1 score\" : 0.5833333333333333 , \"precision\" : 0.5833333333333333 , \"recall\" : 0.5833333333333333 , \"specificity\" : 0.5833333333333333 , \"negative predictive value\" : 0.5833333333333333 , \"false discovery rate\" : 0.4166666666666667 , \"miss rate\" : 0.4166666666666667 , \"fall out\" : 0.4166666666666667 , \"false omission rate\" : 0.4166666666666667 , \"mcc\" : 0.16666666666666666 , \"informedness\" : 0.16666666666666652 , \"markedness\" : 0.16666666666666652 , \"true positives\" : 3 , \"true negatives\" : 3 , \"false positives\" : 2 , \"false negatives\" : 2 , \"cardinality\" : 5 }, \"classes\" : { \"wolf\" : { \"accuracy\" : 0.6 , \"accuracy balanced\" : 0.5833333333333333 , \"f1 score\" : 0.6666666666666666 , \"precision\" : 0.6666666666666666 , \"recall\" : 0.6666666666666666 , \"specificity\" : 0.5 , \"negative predictive value\" : 0.5 , \"false discovery rate\" : 0.33333333333333337 , \"miss rate\" : 0.33333333333333337 , \"fall out\" : 0.5 , \"false omission rate\" : 0.5 , \"informedness\" : 0.16666666666666652 , \"markedness\" : 0.16666666666666652 , \"mcc\" : 0.16666666666666666 , \"true positives\" : 2 , \"true negatives\" : 1 , \"false positives\" : 1 , \"false negatives\" : 1 , \"cardinality\" : 3 , \"proportion\" : 0.6 }, \"lamb\" : { \"accuracy\" : 0.6 , \"accuracy balanced\" : 0.5833333333333333 , \"f1 score\" : 0.5 , \"precision\" : 0.5 , \"recall\" : 0.5 , \"specificity\" : 0.6666666666666666 , \"negative predictive value\" : 0.6666666666666666 , \"false discovery rate\" : 0.5 , \"miss rate\" : 0.5 , \"fall out\" : 0.33333333333333337 , \"false omission rate\" : 0.33333333333333337 , \"informedness\" : 0.16666666666666652 , \"markedness\" : 0.16666666666666652 , \"mcc\" : 0.16666666666666666 , \"true positives\" : 1 , \"true negatives\" : 2 , \"false positives\" : 1 , \"false negatives\" : 1 , \"cardinality\" : 2 , \"proportion\" : 0.4 } } }","title":"Example"},{"location":"datasets/api.html","text":"Dataset Objects # Data are passed in specialized in-memory containers called Dataset objects. Dataset objects are table-like data structures that have operations for data manipulation. They can hold a heterogeneous mix of data types and they make it easy to transport data in a canonical way. Datasets consist of a matrix of samples in which each row constitutes a sample and each column represents the value of the feature represented by that column. They have the additional constraint that each feature column must contain values of the same high-level data type. Some datasets can contain labels for training or cross validation. In the example below, we instantiate a new Labeled dataset object by passing the samples and their labels as arguments to the constructor. use Rubix\\ML\\Datasets\\Labeled ; $samples = [ [ 0.1 , 20 , 'furry' ], [ 2.0 , - 5 , 'rough' ], ]; $labels = [ 'not monster' , 'monster' ]; $dataset = new Labeled ( $samples , $labels ); Factory Methods # Build a dataset with the records of a 2-dimensional iterable data table: public static fromIterator ( Traversable $iterator ) : self Note When building a Labeled dataset, the label values should be in the last column of the data table. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Datasets\\Extractors\\CSV ; $dataset = Labeled :: fromIterator ( new CSV ( 'example.csv' )); Properties # Return the number of rows in the dataset: public numSamples () : int Return the number of columns in the samples matrix: public numFeatures () : int Return a 2-tuple with the shape of the samples matrix: public shape () : array { int , int } [ $m , $n ] = $dataset -> shape (); echo \" $m x $n \" ; 1000 x 30 Data Types # Return the data types for each feature column: public featureTypes () : Rubix\\ML\\DataType [] Return the data type for a given column offset: public featureType ( int $offset ) : Rubix\\ML\\DataType echo $dataset -> featureType ( 15 ); categorical Selecting # Return all the samples in the dataset in a 2-dimensional array: public samples () : array [] Select a single row containing the sample at a given offset beginning at 0: public sample ( int $offset ) : mixed [] Return the columns of the sample matrix: public features () : array [] Select the values of a feature column at a given offset : public feature ( int $offset ) : mixed [] Head and Tail # Return the first n rows of data in a new dataset object: public head ( int $n = 10 ) : self $subset = $dataset -> head ( 10 ); Return the last n rows of data in a new dataset object: public tail ( int $n = 10 ) : self Taking and Leaving # Remove n rows from the dataset and return them in a new dataset: public take ( int $n = 1 ) : self Leave n samples on the dataset and return the rest in a new dataset: public leave ( int $n = 1 ) : self Splitting # Split the dataset into left and right subsets: public split ( float $ratio = 0.5 ) : array { self , self } [ $training , $testing ] = $dataset -> split ( 0.8 ); Folding # Fold the dataset to form k equal size datasets: public fold ( int $k = 10 ) : self [] Note If there are not enough samples to completely fill the last fold of the dataset then it will contain slightly fewer samples than the rest of the folds. $folds = $dataset -> fold ( 8 ); Slicing and Splicing # Return an n size portion of the dataset in a new dataset: public slice ( int $offset , int $n ) : self Remove a size n chunk of the dataset starting at offset and return it in a new dataset: public splice ( int $offset , int $n ) : self Batching # Batch the dataset into subsets containing a maximum of n rows per batch: public batch ( int $n = 50 ) : self [] $batches = $dataset -> batch ( 250 ); Randomization # Randomize the order of the dataset and return it for method chaining: public randomize () : self Generate a random subset of the dataset without replacement of size n : public randomSubset ( int $n ) : self $subset = $dataset -> randomSubset ( 50 ); Generate a random subset with replacement: public randomSubsetWithReplacement ( int $n ) : self $subset = $dataset -> randomSubsetWithReplacement ( 500 ); Generate a random weighted subset with replacement of size n : public randomWeightedSubsetWithReplacement ( int $n , array $weights ) : self $subset = $dataset -> randomWeightedSubsetWithReplacement ( 200 , $weights ); Applying Transformations # You can apply a Transformer to the samples in a Dataset object by passing it as an argument to the apply() method on the dataset object. If a Stateful transformer has not been fitted beforehand, it will automatically be fitted before being applied to the samples. public apply ( Transformer $transformer ) : self use Rubix\\ML\\Transformers\\RobustStandardizer ; $dataset -> apply ( new RobustStandardizer ); To reverse the transformation, pass a Reversible transformer to the dataset objects reverseApply() method. public apply ( Reversible $transformer ) : self use Rubix\\ML\\Transformers\\MaxAbsoluteScaler ; $transformer = new MaxAbsoluteScaler (); $dataset -> apply ( $transformer ); // Do something $dataset -> reverseApply ( $transformer ); Mapping and Filtering # Map a callback function over the records of the dataset and return the result in a new dataset object: public map ( callable $callback ) : self $addMeanColumn = function ( $record ) { $record [] = array_sum ( $record ) / count ( $record ); return $record ; }; $dataset = $dataset -> map ( $addMeanColumn ); Filter the records of the dataset using a callback function to determine if a row should be included in the return dataset: public filter ( callable $callback ) : self $tallPeople = function ( $record ) { return $record [ 3 ] > 178.5 ; }; $dataset = $dataset -> filter ( $tallPeople ); Stacking # Stack any number of dataset objects on top of each other to form a single dataset: public static stack ( array $datasets ) : self Note Datasets must have the same number of feature columns i.e. dimensionality. use Rubix\\ML\\Datasets\\Labeled ; $dataset = Labeled :: stack ([ $dataset1 , $dataset2 , $dataset3 , // ... ]); Merging and Joining # To merge the rows of this dataset with another dataset: public merge ( Dataset $dataset ) : self Note Datasets must have the same number of columns. $dataset = $dataset1 -> merge ( $dataset2 ); To join the columns of this dataset with another dataset: public join ( Dataset $dataset ) : self Note Datasets must have the same number of rows. $dataset = $dataset1 -> join ( $dataset2 ); Descriptive Statistics # Return an array of statistics such as the central tendency, dispersion and shape of each continuous feature column and the joint probabilities of each category for every categorical feature column: public describe () : Rubix\\ML\\Report echo $dataset -> describe (); [ { \"offset\" : 0 , \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"friendly\" : 0.6666666666666666 , \"loner\" : 0.3333333333333333 } }, { \"offset\" : 1 , \"type\" : \"continuous\" , \"mean\" : 0.3333333333333333 , \"standard deviation\" : 3.129252661934191 , \"skewness\" : -0.4481030843690633 , \"kurtosis\" : -1.1330702741786107 , \"min\" : -5 , \"25%\" : -1.375 , \"median\" : 0.8 , \"75%\" : 2.825 , \"max\" : 4 } ] Sorting # Sort the records in the dataset using a callback for comparisons between samples. The callback function accepts two records to be compared and should return true if the records should be swapped. public function sort ( callable $callback ) : self $sorted = $dataset -> sort ( function ( $recordA , $recordB ) { return $recordA [ 2 ] > $recordB [ 2 ]; }); De-duplication # Remove duplicate rows from the dataset: public deduplicate () : self Exporting # Export the dataset to the location and format given by a Writable extractor: public exportTo ( Writable $extractor ) : void use Rubix\\ML\\Extractors\\NDJSON ; $dataset -> exportTo ( new NDJSON ( 'example.ndjson' ));","title":"API Reference"},{"location":"datasets/api.html#dataset-objects","text":"Data are passed in specialized in-memory containers called Dataset objects. Dataset objects are table-like data structures that have operations for data manipulation. They can hold a heterogeneous mix of data types and they make it easy to transport data in a canonical way. Datasets consist of a matrix of samples in which each row constitutes a sample and each column represents the value of the feature represented by that column. They have the additional constraint that each feature column must contain values of the same high-level data type. Some datasets can contain labels for training or cross validation. In the example below, we instantiate a new Labeled dataset object by passing the samples and their labels as arguments to the constructor. use Rubix\\ML\\Datasets\\Labeled ; $samples = [ [ 0.1 , 20 , 'furry' ], [ 2.0 , - 5 , 'rough' ], ]; $labels = [ 'not monster' , 'monster' ]; $dataset = new Labeled ( $samples , $labels );","title":"Dataset Objects"},{"location":"datasets/api.html#factory-methods","text":"Build a dataset with the records of a 2-dimensional iterable data table: public static fromIterator ( Traversable $iterator ) : self Note When building a Labeled dataset, the label values should be in the last column of the data table. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Datasets\\Extractors\\CSV ; $dataset = Labeled :: fromIterator ( new CSV ( 'example.csv' ));","title":"Factory Methods"},{"location":"datasets/api.html#properties","text":"Return the number of rows in the dataset: public numSamples () : int Return the number of columns in the samples matrix: public numFeatures () : int Return a 2-tuple with the shape of the samples matrix: public shape () : array { int , int } [ $m , $n ] = $dataset -> shape (); echo \" $m x $n \" ; 1000 x 30","title":"Properties"},{"location":"datasets/api.html#data-types","text":"Return the data types for each feature column: public featureTypes () : Rubix\\ML\\DataType [] Return the data type for a given column offset: public featureType ( int $offset ) : Rubix\\ML\\DataType echo $dataset -> featureType ( 15 ); categorical","title":"Data Types"},{"location":"datasets/api.html#selecting","text":"Return all the samples in the dataset in a 2-dimensional array: public samples () : array [] Select a single row containing the sample at a given offset beginning at 0: public sample ( int $offset ) : mixed [] Return the columns of the sample matrix: public features () : array [] Select the values of a feature column at a given offset : public feature ( int $offset ) : mixed []","title":"Selecting"},{"location":"datasets/api.html#head-and-tail","text":"Return the first n rows of data in a new dataset object: public head ( int $n = 10 ) : self $subset = $dataset -> head ( 10 ); Return the last n rows of data in a new dataset object: public tail ( int $n = 10 ) : self","title":"Head and Tail"},{"location":"datasets/api.html#taking-and-leaving","text":"Remove n rows from the dataset and return them in a new dataset: public take ( int $n = 1 ) : self Leave n samples on the dataset and return the rest in a new dataset: public leave ( int $n = 1 ) : self","title":"Taking and Leaving"},{"location":"datasets/api.html#splitting","text":"Split the dataset into left and right subsets: public split ( float $ratio = 0.5 ) : array { self , self } [ $training , $testing ] = $dataset -> split ( 0.8 );","title":"Splitting"},{"location":"datasets/api.html#folding","text":"Fold the dataset to form k equal size datasets: public fold ( int $k = 10 ) : self [] Note If there are not enough samples to completely fill the last fold of the dataset then it will contain slightly fewer samples than the rest of the folds. $folds = $dataset -> fold ( 8 );","title":"Folding"},{"location":"datasets/api.html#slicing-and-splicing","text":"Return an n size portion of the dataset in a new dataset: public slice ( int $offset , int $n ) : self Remove a size n chunk of the dataset starting at offset and return it in a new dataset: public splice ( int $offset , int $n ) : self","title":"Slicing and Splicing"},{"location":"datasets/api.html#batching","text":"Batch the dataset into subsets containing a maximum of n rows per batch: public batch ( int $n = 50 ) : self [] $batches = $dataset -> batch ( 250 );","title":"Batching"},{"location":"datasets/api.html#randomization","text":"Randomize the order of the dataset and return it for method chaining: public randomize () : self Generate a random subset of the dataset without replacement of size n : public randomSubset ( int $n ) : self $subset = $dataset -> randomSubset ( 50 ); Generate a random subset with replacement: public randomSubsetWithReplacement ( int $n ) : self $subset = $dataset -> randomSubsetWithReplacement ( 500 ); Generate a random weighted subset with replacement of size n : public randomWeightedSubsetWithReplacement ( int $n , array $weights ) : self $subset = $dataset -> randomWeightedSubsetWithReplacement ( 200 , $weights );","title":"Randomization"},{"location":"datasets/api.html#applying-transformations","text":"You can apply a Transformer to the samples in a Dataset object by passing it as an argument to the apply() method on the dataset object. If a Stateful transformer has not been fitted beforehand, it will automatically be fitted before being applied to the samples. public apply ( Transformer $transformer ) : self use Rubix\\ML\\Transformers\\RobustStandardizer ; $dataset -> apply ( new RobustStandardizer ); To reverse the transformation, pass a Reversible transformer to the dataset objects reverseApply() method. public apply ( Reversible $transformer ) : self use Rubix\\ML\\Transformers\\MaxAbsoluteScaler ; $transformer = new MaxAbsoluteScaler (); $dataset -> apply ( $transformer ); // Do something $dataset -> reverseApply ( $transformer );","title":"Applying Transformations"},{"location":"datasets/api.html#mapping-and-filtering","text":"Map a callback function over the records of the dataset and return the result in a new dataset object: public map ( callable $callback ) : self $addMeanColumn = function ( $record ) { $record [] = array_sum ( $record ) / count ( $record ); return $record ; }; $dataset = $dataset -> map ( $addMeanColumn ); Filter the records of the dataset using a callback function to determine if a row should be included in the return dataset: public filter ( callable $callback ) : self $tallPeople = function ( $record ) { return $record [ 3 ] > 178.5 ; }; $dataset = $dataset -> filter ( $tallPeople );","title":"Mapping and Filtering"},{"location":"datasets/api.html#stacking","text":"Stack any number of dataset objects on top of each other to form a single dataset: public static stack ( array $datasets ) : self Note Datasets must have the same number of feature columns i.e. dimensionality. use Rubix\\ML\\Datasets\\Labeled ; $dataset = Labeled :: stack ([ $dataset1 , $dataset2 , $dataset3 , // ... ]);","title":"Stacking"},{"location":"datasets/api.html#merging-and-joining","text":"To merge the rows of this dataset with another dataset: public merge ( Dataset $dataset ) : self Note Datasets must have the same number of columns. $dataset = $dataset1 -> merge ( $dataset2 ); To join the columns of this dataset with another dataset: public join ( Dataset $dataset ) : self Note Datasets must have the same number of rows. $dataset = $dataset1 -> join ( $dataset2 );","title":"Merging and Joining"},{"location":"datasets/api.html#descriptive-statistics","text":"Return an array of statistics such as the central tendency, dispersion and shape of each continuous feature column and the joint probabilities of each category for every categorical feature column: public describe () : Rubix\\ML\\Report echo $dataset -> describe (); [ { \"offset\" : 0 , \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"friendly\" : 0.6666666666666666 , \"loner\" : 0.3333333333333333 } }, { \"offset\" : 1 , \"type\" : \"continuous\" , \"mean\" : 0.3333333333333333 , \"standard deviation\" : 3.129252661934191 , \"skewness\" : -0.4481030843690633 , \"kurtosis\" : -1.1330702741786107 , \"min\" : -5 , \"25%\" : -1.375 , \"median\" : 0.8 , \"75%\" : 2.825 , \"max\" : 4 } ]","title":"Descriptive Statistics"},{"location":"datasets/api.html#sorting","text":"Sort the records in the dataset using a callback for comparisons between samples. The callback function accepts two records to be compared and should return true if the records should be swapped. public function sort ( callable $callback ) : self $sorted = $dataset -> sort ( function ( $recordA , $recordB ) { return $recordA [ 2 ] > $recordB [ 2 ]; });","title":"Sorting"},{"location":"datasets/api.html#de-duplication","text":"Remove duplicate rows from the dataset: public deduplicate () : self","title":"De-duplication"},{"location":"datasets/api.html#exporting","text":"Export the dataset to the location and format given by a Writable extractor: public exportTo ( Writable $extractor ) : void use Rubix\\ML\\Extractors\\NDJSON ; $dataset -> exportTo ( new NDJSON ( 'example.ndjson' ));","title":"Exporting"},{"location":"datasets/labeled.html","text":"[source] Labeled # A Labeled dataset is used to train supervised learners and for testing a model by providing the ground-truth. In addition to the standard dataset API, a labeled dataset can perform operations such as stratification and sorting the dataset using the label column. Note Since PHP silently converts integer strings (ex. '1' ) to integers in some circumstances, you should not use integer strings as class labels. Instead, use an appropriate non-integer string class name such as 'class 1' , '#1' , or 'first' . Parameters # # Name Default Type Description 1 samples array A 2-dimensional array consisting of rows of samples and columns with feature values. 2 labels array A 1-dimensional array of labels that correspond to each sample in the dataset. 2 verify true bool Should we verify the data? Example # use Rubix\\ML\\Datasets\\Labeled ; $samples = [ [ 0.1 , 20 , 'furry' ], [ 2.0 , - 5 , 'rough' ], [ 0.01 , 5 , 'furry' ], ]; $labels = [ 'not monster' , 'monster' , 'not monster' ]; $dataset = new Labeled ( $samples , $labels ); Additional Methods # Selectors # Return the labels of the dataset in an array: public labels () : array Return a single label at the given row offset: public label ( int $offset ) : mixed Return all of the possible outcomes i.e. the unique labels in an array: public possibleOutcomes () : array print_r ( $dataset -> possibleOutcomes ()); Array ( [ 0 ] => female [ 1 ] => male ) Data Types # Return the data type of the label: public labelType () : Rubix\\ML\\DataType echo $dataset -> labelType (); continuous Stratification # Group samples by their class label and return them in their own dataset: public stratifyByLabel () : array $strata = $dataset -> stratifyByLabel (); Split the dataset into left and right subsets such that the proportions of class labels remain intact: public stratifiedSplit ( $ratio = 0.5 ) : array [ $training , $testing ] = $dataset -> stratifiedSplit ( 0.8 ); Return k equal size subsets of the dataset such that class proportions remain intact: public stratifiedFold ( $k = 10 ) : array $folds = $dataset -> stratifiedFold ( 3 ); Transform Labels # Transform the labels in the dataset using a callback function and return self for method chaining: public transformLabels ( callable $fn ) : self Note The callback function called for each individual label and should return the transformed label as a continuous or categorical value. $dataset -> transformLabels ( 'intval' ); // $dataset -> transformLabels ( function ( $label ) { return $label > 0.5 ? 'yes' : 'no' ; }); Describe by Label # Describe the features of the dataset broken down by categorical label: public describeByLabel () : Report echo $dataset -> describeByLabel (); { \"not monster\" : [ { \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"friendly\" : 0.75 , \"loner\" : 0.25 } }, { \"type\" : \"continuous\" , \"mean\" : 1.125 , \"variance\" : 12.776875 , \"standard deviation\" : 3.574475485997911 , \"skewness\" : -1.0795676577113944 , \"kurtosis\" : -0.7175867765792474 , \"min\" : -5 , \"25%\" : 0.6999999999999993 , \"median\" : 2.75 , \"75%\" : 3.175 , \"max\" : 4 } ], \"monster\" : [ { \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"loner\" : 0.5 , \"friendly\" : 0.5 } }, { \"type\" : \"continuous\" , \"mean\" : -1.25 , \"standard deviation\" : 0.25 , \"skewness\" : 0 , \"kurtosis\" : -2 , \"min\" : -1.5 , \"25%\" : -1.375 , \"median\" : -1.25 , \"75%\" : -1.125 , \"max\" : -1 } ] }","title":"Labeled"},{"location":"datasets/labeled.html#labeled","text":"A Labeled dataset is used to train supervised learners and for testing a model by providing the ground-truth. In addition to the standard dataset API, a labeled dataset can perform operations such as stratification and sorting the dataset using the label column. Note Since PHP silently converts integer strings (ex. '1' ) to integers in some circumstances, you should not use integer strings as class labels. Instead, use an appropriate non-integer string class name such as 'class 1' , '#1' , or 'first' .","title":"Labeled"},{"location":"datasets/labeled.html#parameters","text":"# Name Default Type Description 1 samples array A 2-dimensional array consisting of rows of samples and columns with feature values. 2 labels array A 1-dimensional array of labels that correspond to each sample in the dataset. 2 verify true bool Should we verify the data?","title":"Parameters"},{"location":"datasets/labeled.html#example","text":"use Rubix\\ML\\Datasets\\Labeled ; $samples = [ [ 0.1 , 20 , 'furry' ], [ 2.0 , - 5 , 'rough' ], [ 0.01 , 5 , 'furry' ], ]; $labels = [ 'not monster' , 'monster' , 'not monster' ]; $dataset = new Labeled ( $samples , $labels );","title":"Example"},{"location":"datasets/labeled.html#additional-methods","text":"","title":"Additional Methods"},{"location":"datasets/labeled.html#selectors","text":"Return the labels of the dataset in an array: public labels () : array Return a single label at the given row offset: public label ( int $offset ) : mixed Return all of the possible outcomes i.e. the unique labels in an array: public possibleOutcomes () : array print_r ( $dataset -> possibleOutcomes ()); Array ( [ 0 ] => female [ 1 ] => male )","title":"Selectors"},{"location":"datasets/labeled.html#data-types","text":"Return the data type of the label: public labelType () : Rubix\\ML\\DataType echo $dataset -> labelType (); continuous","title":"Data Types"},{"location":"datasets/labeled.html#stratification","text":"Group samples by their class label and return them in their own dataset: public stratifyByLabel () : array $strata = $dataset -> stratifyByLabel (); Split the dataset into left and right subsets such that the proportions of class labels remain intact: public stratifiedSplit ( $ratio = 0.5 ) : array [ $training , $testing ] = $dataset -> stratifiedSplit ( 0.8 ); Return k equal size subsets of the dataset such that class proportions remain intact: public stratifiedFold ( $k = 10 ) : array $folds = $dataset -> stratifiedFold ( 3 );","title":"Stratification"},{"location":"datasets/labeled.html#transform-labels","text":"Transform the labels in the dataset using a callback function and return self for method chaining: public transformLabels ( callable $fn ) : self Note The callback function called for each individual label and should return the transformed label as a continuous or categorical value. $dataset -> transformLabels ( 'intval' ); // $dataset -> transformLabels ( function ( $label ) { return $label > 0.5 ? 'yes' : 'no' ; });","title":"Transform Labels"},{"location":"datasets/labeled.html#describe-by-label","text":"Describe the features of the dataset broken down by categorical label: public describeByLabel () : Report echo $dataset -> describeByLabel (); { \"not monster\" : [ { \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"friendly\" : 0.75 , \"loner\" : 0.25 } }, { \"type\" : \"continuous\" , \"mean\" : 1.125 , \"variance\" : 12.776875 , \"standard deviation\" : 3.574475485997911 , \"skewness\" : -1.0795676577113944 , \"kurtosis\" : -0.7175867765792474 , \"min\" : -5 , \"25%\" : 0.6999999999999993 , \"median\" : 2.75 , \"75%\" : 3.175 , \"max\" : 4 } ], \"monster\" : [ { \"type\" : \"categorical\" , \"num categories\" : 2 , \"probabilities\" : { \"loner\" : 0.5 , \"friendly\" : 0.5 } }, { \"type\" : \"continuous\" , \"mean\" : -1.25 , \"standard deviation\" : 0.25 , \"skewness\" : 0 , \"kurtosis\" : -2 , \"min\" : -1.5 , \"25%\" : -1.375 , \"median\" : -1.25 , \"75%\" : -1.125 , \"max\" : -1 } ] }","title":"Describe by Label"},{"location":"datasets/unlabeled.html","text":"[source] Unlabeled # Unlabeled datasets are used to train unsupervised learners and for feeding unknown samples into an estimator to make predictions. As their name implies, they do not require a corresponding label for each sample. Parameters # # Name Default Type Description 1 samples array A 2-dimensional array consisting of rows of samples and columns with feature values. 2 verify true bool Should we verify the data? Example # use Rubix\\ML\\Datasets\\Unlabeled ; $samples = [ [ 0.1 , 20 , 'furry' ], [ 2.0 , - 5 , 'rough' ], [ 0.001 , - 10 , 'rough' ], ]; $dataset = new Unlabeled ( $samples ); Additional Methods # This dataset does not have any additional methods.","title":"Unlabeled"},{"location":"datasets/unlabeled.html#unlabeled","text":"Unlabeled datasets are used to train unsupervised learners and for feeding unknown samples into an estimator to make predictions. As their name implies, they do not require a corresponding label for each sample.","title":"Unlabeled"},{"location":"datasets/unlabeled.html#parameters","text":"# Name Default Type Description 1 samples array A 2-dimensional array consisting of rows of samples and columns with feature values. 2 verify true bool Should we verify the data?","title":"Parameters"},{"location":"datasets/unlabeled.html#example","text":"use Rubix\\ML\\Datasets\\Unlabeled ; $samples = [ [ 0.1 , 20 , 'furry' ], [ 2.0 , - 5 , 'rough' ], [ 0.001 , - 10 , 'rough' ], ]; $dataset = new Unlabeled ( $samples );","title":"Example"},{"location":"datasets/unlabeled.html#additional-methods","text":"This dataset does not have any additional methods.","title":"Additional Methods"},{"location":"datasets/generators/agglomerate.html","text":"[source] Agglomerate # An Agglomerate is a collection of generators with each of them given a user-defined label. Agglomerates are useful for classification, clustering, and anomaly detection problems where the target label is a discrete value. Data Types: Depends on base generators Label Type: Categorical Parameters # # Name Default Type Description 1 generators array A collection of generators indexed by their given label. 2 weights Auto array A set of arbitrary weight values corresponding to a generator's proportion of the overall agglomeration. If no weights are given, each generator is assigned equal weight. Example # use Rubix\\ML\\Datasets\\Generators\\Agglomerate ; use Rubix\\ML\\Datasets\\Generators\\Blob ; use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; use Rubix\\ML\\Datasets\\Generators\\Circle ; $generator = new Agglomerate ([ 'foo' => new Blob ([ 5 , 2 ], 1.0 ), 'bar' => new HalfMoon ( - 3 , 5 , 1.5 , 90.0 , 0.1 ), 'baz' => new Circle ( 2 , - 4 , 2.0 , 0.05 ), ], [ 3.5 , 4.0 , 5.0 , ]); Additional Methods # Return the normalized weight values of each generator in the agglomerate: public weights () : array","title":"Agglomerate"},{"location":"datasets/generators/agglomerate.html#agglomerate","text":"An Agglomerate is a collection of generators with each of them given a user-defined label. Agglomerates are useful for classification, clustering, and anomaly detection problems where the target label is a discrete value. Data Types: Depends on base generators Label Type: Categorical","title":"Agglomerate"},{"location":"datasets/generators/agglomerate.html#parameters","text":"# Name Default Type Description 1 generators array A collection of generators indexed by their given label. 2 weights Auto array A set of arbitrary weight values corresponding to a generator's proportion of the overall agglomeration. If no weights are given, each generator is assigned equal weight.","title":"Parameters"},{"location":"datasets/generators/agglomerate.html#example","text":"use Rubix\\ML\\Datasets\\Generators\\Agglomerate ; use Rubix\\ML\\Datasets\\Generators\\Blob ; use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; use Rubix\\ML\\Datasets\\Generators\\Circle ; $generator = new Agglomerate ([ 'foo' => new Blob ([ 5 , 2 ], 1.0 ), 'bar' => new HalfMoon ( - 3 , 5 , 1.5 , 90.0 , 0.1 ), 'baz' => new Circle ( 2 , - 4 , 2.0 , 0.05 ), ], [ 3.5 , 4.0 , 5.0 , ]);","title":"Example"},{"location":"datasets/generators/agglomerate.html#additional-methods","text":"Return the normalized weight values of each generator in the agglomerate: public weights () : array","title":"Additional Methods"},{"location":"datasets/generators/api.html","text":"Generators # Dataset generators produce synthetic datasets of a user-specified shape and dimensionality. Synthetic data is useful for a number of tasks including experimentation, testing, benchmarking, and demonstration purposes. Generate a Dataset # To generate a Dataset object with n records: public generate ( int $n ) : Dataset use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; $generator = new HalfMoon ( 0.0 , 0.0 ); $dataset = $generator -> generate ( 1000 );","title":"API Reference"},{"location":"datasets/generators/api.html#generators","text":"Dataset generators produce synthetic datasets of a user-specified shape and dimensionality. Synthetic data is useful for a number of tasks including experimentation, testing, benchmarking, and demonstration purposes.","title":"Generators"},{"location":"datasets/generators/api.html#generate-a-dataset","text":"To generate a Dataset object with n records: public generate ( int $n ) : Dataset use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; $generator = new HalfMoon ( 0.0 , 0.0 ); $dataset = $generator -> generate ( 1000 );","title":"Generate a Dataset"},{"location":"datasets/generators/blob.html","text":"[source] Blob # A normally distributed (Gaussian) n-dimensional blob of samples centered at a given vector. The standard deviation can be set for the whole blob or for each feature column independently. When a global standard deviation is used, the resulting blob will be isotropic and will converge asymptotically to a sphere. Data Types: Continuous Label Type: Unlabeled Parameters # # Name Default Type Description 1 center [0, 0] array An array containing the coordinates of the center of the blob. 2 stddev 1.0 float or array Either the global standard deviation or an array with the standard deviation on a per feature column basis. Example # use Rubix\\ML\\Datasets\\Generators\\Blob ; $generator = new Blob ([ - 1.2 , - 5. , 2.6 , 0.8 , 10. ], 0.25 ); Additional Methods # This generator does not have any additional methods.","title":"Blob"},{"location":"datasets/generators/blob.html#blob","text":"A normally distributed (Gaussian) n-dimensional blob of samples centered at a given vector. The standard deviation can be set for the whole blob or for each feature column independently. When a global standard deviation is used, the resulting blob will be isotropic and will converge asymptotically to a sphere. Data Types: Continuous Label Type: Unlabeled","title":"Blob"},{"location":"datasets/generators/blob.html#parameters","text":"# Name Default Type Description 1 center [0, 0] array An array containing the coordinates of the center of the blob. 2 stddev 1.0 float or array Either the global standard deviation or an array with the standard deviation on a per feature column basis.","title":"Parameters"},{"location":"datasets/generators/blob.html#example","text":"use Rubix\\ML\\Datasets\\Generators\\Blob ; $generator = new Blob ([ - 1.2 , - 5. , 2.6 , 0.8 , 10. ], 0.25 );","title":"Example"},{"location":"datasets/generators/blob.html#additional-methods","text":"This generator does not have any additional methods.","title":"Additional Methods"},{"location":"datasets/generators/circle.html","text":"[source] Circle # Creates a dataset of points forming a circle in 2 dimensions. The label of each sample is the random value used to generate the projection measured in degrees. Data Types: Continuous Label Type: Continuous Parameters # # Name Default Type Description 1 x 0.0 float The x coordinate of the center of the circle. 2 y 0.0 float The y coordinate of the center of the circle. 3 scale 1.0 float The scaling factor of the circle. 4 noise 0.1 float The amount of Gaussian noise to add to each data point as a ratio of the scaling factor. Example # use Rubix\\ML\\Datasets\\Generators\\Circle ; $generator = new Circle ( 0.0 , 0.0 , 100 , 0.1 ); Additional Methods # This generator does not have any additional methods.","title":"Circle"},{"location":"datasets/generators/circle.html#circle","text":"Creates a dataset of points forming a circle in 2 dimensions. The label of each sample is the random value used to generate the projection measured in degrees. Data Types: Continuous Label Type: Continuous","title":"Circle"},{"location":"datasets/generators/circle.html#parameters","text":"# Name Default Type Description 1 x 0.0 float The x coordinate of the center of the circle. 2 y 0.0 float The y coordinate of the center of the circle. 3 scale 1.0 float The scaling factor of the circle. 4 noise 0.1 float The amount of Gaussian noise to add to each data point as a ratio of the scaling factor.","title":"Parameters"},{"location":"datasets/generators/circle.html#example","text":"use Rubix\\ML\\Datasets\\Generators\\Circle ; $generator = new Circle ( 0.0 , 0.0 , 100 , 0.1 );","title":"Example"},{"location":"datasets/generators/circle.html#additional-methods","text":"This generator does not have any additional methods.","title":"Additional Methods"},{"location":"datasets/generators/half-moon.html","text":"[source] Half Moon # Generates a dataset consisting of 2-d samples that form the shape of a half moon when plotted on a scatter plot chart. Data Types: Continuous Label Type: Continuous Parameters # # Name Default Type Description 1 x 0.0 float The x coordinate of the center of the half moon. 2 y 0.0 float The y coordinate of the center of the half moon. 3 scale 1.0 float The scaling factor of the half moon. 4 rotate 90.0 float The amount in degrees to rotate the half moon counterclockwise. 5 noise 0.1 float The amount of Gaussian noise to add to each data point as a percentage of the scaling factor. Example # use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; $generator = new HalfMoon ( 4.0 , 0.0 , 6 , 180.0 , 0.2 ); Additional Methods # This generator does not have any additional methods.","title":"Half Moon"},{"location":"datasets/generators/half-moon.html#half-moon","text":"Generates a dataset consisting of 2-d samples that form the shape of a half moon when plotted on a scatter plot chart. Data Types: Continuous Label Type: Continuous","title":"Half Moon"},{"location":"datasets/generators/half-moon.html#parameters","text":"# Name Default Type Description 1 x 0.0 float The x coordinate of the center of the half moon. 2 y 0.0 float The y coordinate of the center of the half moon. 3 scale 1.0 float The scaling factor of the half moon. 4 rotate 90.0 float The amount in degrees to rotate the half moon counterclockwise. 5 noise 0.1 float The amount of Gaussian noise to add to each data point as a percentage of the scaling factor.","title":"Parameters"},{"location":"datasets/generators/half-moon.html#example","text":"use Rubix\\ML\\Datasets\\Generators\\HalfMoon ; $generator = new HalfMoon ( 4.0 , 0.0 , 6 , 180.0 , 0.2 );","title":"Example"},{"location":"datasets/generators/half-moon.html#additional-methods","text":"This generator does not have any additional methods.","title":"Additional Methods"},{"location":"datasets/generators/hyperplane.html","text":"[source] Hyperplane # Generates a labeled dataset whose samples form a hyperplane in n-dimensional vector space and whose labels are continuous values drawn from a uniform random distribution between -1 and 1. When the number of coefficients is either 1, 2 or 3, the samples form points, lines, and planes respectively. Due to its linearity, Hyperplane is especially useful for testing linear regression models. Data Types: Continuous Label Type: Continuous Parameters # # Name Default Type Description 1 coefficients [1, -1] array The n coefficients of the hyperplane where n is the dimensionality. 2 intercept 0.0 float The y intercept term. 3 noise 0.1 float The factor of gaussian noise to add to the data points. Example # use Rubix\\ML\\Datasets\\Generators\\Hyperplane ; $generator = new Hyperplane ([ 0.1 , 3 , - 5 , 0.01 ], 150.0 , 0.25 ); Additional Methods # This generator does not have any additional methods.","title":"Hyperplane"},{"location":"datasets/generators/hyperplane.html#hyperplane","text":"Generates a labeled dataset whose samples form a hyperplane in n-dimensional vector space and whose labels are continuous values drawn from a uniform random distribution between -1 and 1. When the number of coefficients is either 1, 2 or 3, the samples form points, lines, and planes respectively. Due to its linearity, Hyperplane is especially useful for testing linear regression models. Data Types: Continuous Label Type: Continuous","title":"Hyperplane"},{"location":"datasets/generators/hyperplane.html#parameters","text":"# Name Default Type Description 1 coefficients [1, -1] array The n coefficients of the hyperplane where n is the dimensionality. 2 intercept 0.0 float The y intercept term. 3 noise 0.1 float The factor of gaussian noise to add to the data points.","title":"Parameters"},{"location":"datasets/generators/hyperplane.html#example","text":"use Rubix\\ML\\Datasets\\Generators\\Hyperplane ; $generator = new Hyperplane ([ 0.1 , 3 , - 5 , 0.01 ], 150.0 , 0.25 );","title":"Example"},{"location":"datasets/generators/hyperplane.html#additional-methods","text":"This generator does not have any additional methods.","title":"Additional Methods"},{"location":"datasets/generators/swiss-roll.html","text":"[source] Swiss Roll # Generate a non-linear 3-dimensional dataset resembling a swiss roll or spiral. The labels are the seeds to the swiss roll transformation. Data Types: Continuous Label Type: Continuous Parameters # # Name Default Type Description 1 x 0.0 float The x coordinate of the center of the swiss roll. 2 y 0.0 float The y coordinate of the center of the swiss roll. 3 z 0.0 float The z coordinate of the center of the swiss roll. 4 scale 1.0 float The scaling factor of the swiss roll. 5 depth 21.0 float The depth of the swiss roll i.e the scale of the y axis. 6 noise 0.1 float The standard deviation of the gaussian noise. Example # use Rubix\\ML\\Datasets\\Generators\\SwissRoll ; $generator = new SwissRoll ( 5.5 , 1.5 , - 2.0 , 10 , 21.0 , 0.2 ); Additional Methods # This generator does not have any additional methods.","title":"Swiss Roll"},{"location":"datasets/generators/swiss-roll.html#swiss-roll","text":"Generate a non-linear 3-dimensional dataset resembling a swiss roll or spiral. The labels are the seeds to the swiss roll transformation. Data Types: Continuous Label Type: Continuous","title":"Swiss Roll"},{"location":"datasets/generators/swiss-roll.html#parameters","text":"# Name Default Type Description 1 x 0.0 float The x coordinate of the center of the swiss roll. 2 y 0.0 float The y coordinate of the center of the swiss roll. 3 z 0.0 float The z coordinate of the center of the swiss roll. 4 scale 1.0 float The scaling factor of the swiss roll. 5 depth 21.0 float The depth of the swiss roll i.e the scale of the y axis. 6 noise 0.1 float The standard deviation of the gaussian noise.","title":"Parameters"},{"location":"datasets/generators/swiss-roll.html#example","text":"use Rubix\\ML\\Datasets\\Generators\\SwissRoll ; $generator = new SwissRoll ( 5.5 , 1.5 , - 2.0 , 10 , 21.0 , 0.2 );","title":"Example"},{"location":"datasets/generators/swiss-roll.html#additional-methods","text":"This generator does not have any additional methods.","title":"Additional Methods"},{"location":"extractors/api.html","text":"Extractors # Extractors are data table iterators that help you import data from various source formats such as CSV, NDJSON, and SQL in an efficient way. They implement one of the standard PHP Traversable interfaces and are compatible anywhere the iterable pseudotype is accepted. Extractors that implement the Writable interface can be used to save other iterators such as dataset objects and other extractors. Iterate # Calling foreach on an extractor object iterates over the rows of the data table. In the example below, we'll use the CSV extractor to print out the rows of the dataset to the console. use Rubix\\ML\\Extractors\\CSV ; foreach ( new CSV ( 'example.csv' ) as $row ) { print_r ( $row ); } We can also instantiate a new Dataset object by passing an extractor to the fromIterator() method. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Extractors\\NDJSON ; $dataset = Labeled :: fromIterator ( new NDJSON ( 'example.ndjson' )); Write to Storage # Extractors that implement the Writable interface have an additional export() method that takes another iterable type and writes it to the storage location specified by the user in the format of the extractor. public export ( iterable $iterator , ? array $header = null ) : void $extractor -> export ( $dataset ); Note The extractor will overwrite any existing data if the file or database already exists. Return an Iterator # To return the underlying iterator wrapped by the extractor object: public getIterator () : Traversable The example below shows how you can instantiate a new dataset object using only a portion of the source dataset by wrapping an underlying iterator with the standard PHP library's Limit Iterator . use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Datasets\\Unlabeled ; use LimitIterator ; $extractor = new NDJSON ( 'example.ndjson' ); $iterator = new LimitIterator ( $extractor -> getIterator (), 500 , 1000 ); $dataset = Unlabeled :: fromIterator ( $iterator );","title":"API Reference"},{"location":"extractors/api.html#extractors","text":"Extractors are data table iterators that help you import data from various source formats such as CSV, NDJSON, and SQL in an efficient way. They implement one of the standard PHP Traversable interfaces and are compatible anywhere the iterable pseudotype is accepted. Extractors that implement the Writable interface can be used to save other iterators such as dataset objects and other extractors.","title":"Extractors"},{"location":"extractors/api.html#iterate","text":"Calling foreach on an extractor object iterates over the rows of the data table. In the example below, we'll use the CSV extractor to print out the rows of the dataset to the console. use Rubix\\ML\\Extractors\\CSV ; foreach ( new CSV ( 'example.csv' ) as $row ) { print_r ( $row ); } We can also instantiate a new Dataset object by passing an extractor to the fromIterator() method. use Rubix\\ML\\Datasets\\Labeled ; use Rubix\\ML\\Extractors\\NDJSON ; $dataset = Labeled :: fromIterator ( new NDJSON ( 'example.ndjson' ));","title":"Iterate"},{"location":"extractors/api.html#write-to-storage","text":"Extractors that implement the Writable interface have an additional export() method that takes another iterable type and writes it to the storage location specified by the user in the format of the extractor. public export ( iterable $iterator , ? array $header = null ) : void $extractor -> export ( $dataset ); Note The extractor will overwrite any existing data if the file or database already exists.","title":"Write to Storage"},{"location":"extractors/api.html#return-an-iterator","text":"To return the underlying iterator wrapped by the extractor object: public getIterator () : Traversable The example below shows how you can instantiate a new dataset object using only a portion of the source dataset by wrapping an underlying iterator with the standard PHP library's Limit Iterator . use Rubix\\ML\\Extractors\\NDJSON ; use Rubix\\ML\\Datasets\\Unlabeled ; use LimitIterator ; $extractor = new NDJSON ( 'example.ndjson' ); $iterator = new LimitIterator ( $extractor -> getIterator (), 500 , 1000 ); $dataset = Unlabeled :: fromIterator ( $iterator );","title":"Return an Iterator"},{"location":"extractors/column-picker.html","text":"[source] Column Picker # An extractor that wraps another iterator and selects and reorders the columns of the data table according to the keys specified by the user. The key of a column may either be a string or a column number (integer) depending on the way the columns are indexed in the base iterator. Interfaces: Extractor Parameters # # Name Default Type Description 1 iterator Traversable The base iterator. 2 keys array The string and/or integer keys of the columns to pick and reorder from the table Example # use Rubix\\ML\\Extractors\\ColumnPicker ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new ColumnPicker ( new CSV ( 'example.csv' , true ), [ 'attitude' , 'texture' , 'class' , 'rating' , ]); Additional Methods # This extractor does not have any additional methods.","title":"Column Picker"},{"location":"extractors/column-picker.html#column-picker","text":"An extractor that wraps another iterator and selects and reorders the columns of the data table according to the keys specified by the user. The key of a column may either be a string or a column number (integer) depending on the way the columns are indexed in the base iterator. Interfaces: Extractor","title":"Column Picker"},{"location":"extractors/column-picker.html#parameters","text":"# Name Default Type Description 1 iterator Traversable The base iterator. 2 keys array The string and/or integer keys of the columns to pick and reorder from the table","title":"Parameters"},{"location":"extractors/column-picker.html#example","text":"use Rubix\\ML\\Extractors\\ColumnPicker ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new ColumnPicker ( new CSV ( 'example.csv' , true ), [ 'attitude' , 'texture' , 'class' , 'rating' , ]);","title":"Example"},{"location":"extractors/column-picker.html#additional-methods","text":"This extractor does not have any additional methods.","title":"Additional Methods"},{"location":"extractors/concatenator.html","text":"[source] Concatenator # Concatenates multiple iterators by joining the tail of one with the head of another. Interfaces: Extractor Parameters # # Name Default Type Description 1 iterators array The iterators to concatenate together. Example # use Rubix\\ML\\Extractors\\Concatenator ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new Concatenator ([ new CSV ( 'dataset1.csv' ), new CSV ( 'dataset2.csv' ), new CSV ( 'dataset3.csv' ), ]); Additional Methods # This extractor does not have any additional methods.","title":"Concatenator"},{"location":"extractors/concatenator.html#concatenator","text":"Concatenates multiple iterators by joining the tail of one with the head of another. Interfaces: Extractor","title":"Concatenator"},{"location":"extractors/concatenator.html#parameters","text":"# Name Default Type Description 1 iterators array The iterators to concatenate together.","title":"Parameters"},{"location":"extractors/concatenator.html#example","text":"use Rubix\\ML\\Extractors\\Concatenator ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new Concatenator ([ new CSV ( 'dataset1.csv' ), new CSV ( 'dataset2.csv' ), new CSV ( 'dataset3.csv' ), ]);","title":"Example"},{"location":"extractors/concatenator.html#additional-methods","text":"This extractor does not have any additional methods.","title":"Additional Methods"},{"location":"extractors/csv.html","text":"[source] CSV # A plain-text format that use newlines to delineate rows and a user-specified delimiter (usually a comma) to separate the values of each column in a data table. Comma-Separated Values (CSV) format is a common format but suffers from not being able to retain type information - thus, all data is imported as categorical data (strings) by default. Note This implementation of CSV is based on the definition in RFC 4180 . Interfaces: Extractor , Writable Parameters # # Name Default Type Description 1 path string The path to the CSV file. 2 header false bool Does the CSV document have a header as the first row? 3 delimiter ',' string The character that delineates the values of the columns of the data table. 4 enclosure '\"' string The character used to enclose a cell that contains a delimiter in the body. Example # use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'example.csv' , true , ',' , '\"' ); Additional Methods # Return the column titles of the data table. public header () : array References # T. Shafranovich. (2005). Common Format and MIME Type for Comma-Separated Values (CSV) Files. \u21a9","title":"CSV"},{"location":"extractors/csv.html#csv","text":"A plain-text format that use newlines to delineate rows and a user-specified delimiter (usually a comma) to separate the values of each column in a data table. Comma-Separated Values (CSV) format is a common format but suffers from not being able to retain type information - thus, all data is imported as categorical data (strings) by default. Note This implementation of CSV is based on the definition in RFC 4180 . Interfaces: Extractor , Writable","title":"CSV"},{"location":"extractors/csv.html#parameters","text":"# Name Default Type Description 1 path string The path to the CSV file. 2 header false bool Does the CSV document have a header as the first row? 3 delimiter ',' string The character that delineates the values of the columns of the data table. 4 enclosure '\"' string The character used to enclose a cell that contains a delimiter in the body.","title":"Parameters"},{"location":"extractors/csv.html#example","text":"use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'example.csv' , true , ',' , '\"' );","title":"Example"},{"location":"extractors/csv.html#additional-methods","text":"Return the column titles of the data table. public header () : array","title":"Additional Methods"},{"location":"extractors/csv.html#references","text":"T. Shafranovich. (2005). Common Format and MIME Type for Comma-Separated Values (CSV) Files. \u21a9","title":"References"},{"location":"extractors/deduplicator.html","text":"[source] Deduplicator # Removes duplicate records from a dataset while the records are in flight. Deduplicator uses a Bloom filter under the hood to probabilistically identify records that have already been seen before. Note Due to its probabilistic nature, Deduplicator may mistakenly drop unique records at a bounded rate. Interfaces: Extractor Parameters # # Name Default Type Description 1 iterator Traversable The base iterator. 2 maxFalsePositiveRate 0.001 float The false positive rate to remain below. 3 numHashes 4 int The number of hash functions used, i.e. the number of slices per layer. Set to null for auto. 4 layerSize 32000000 int The size of each layer of the filter in bits. Example # use Rubix\\ML\\Extractors\\Deduplicator ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new Deduplicator ( new CSV ( 'example.csv' , true ), 0.01 , 3 , 32000000 ); Additional Methods # Return the number of records that have been dropped so far. public dropped () : int","title":"Deduplicator"},{"location":"extractors/deduplicator.html#deduplicator","text":"Removes duplicate records from a dataset while the records are in flight. Deduplicator uses a Bloom filter under the hood to probabilistically identify records that have already been seen before. Note Due to its probabilistic nature, Deduplicator may mistakenly drop unique records at a bounded rate. Interfaces: Extractor","title":"Deduplicator"},{"location":"extractors/deduplicator.html#parameters","text":"# Name Default Type Description 1 iterator Traversable The base iterator. 2 maxFalsePositiveRate 0.001 float The false positive rate to remain below. 3 numHashes 4 int The number of hash functions used, i.e. the number of slices per layer. Set to null for auto. 4 layerSize 32000000 int The size of each layer of the filter in bits.","title":"Parameters"},{"location":"extractors/deduplicator.html#example","text":"use Rubix\\ML\\Extractors\\Deduplicator ; use Rubix\\ML\\Extractors\\CSV ; $extractor = new Deduplicator ( new CSV ( 'example.csv' , true ), 0.01 , 3 , 32000000 );","title":"Example"},{"location":"extractors/deduplicator.html#additional-methods","text":"Return the number of records that have been dropped so far. public dropped () : int","title":"Additional Methods"},{"location":"extractors/ndjson.html","text":"[source] NDJSON # NDJSON or Newline Delimited JSON files contain rows of data encoded in Javascript Object Notation (JSON) arrays or objects. The format is like a mix of JSON and CSV and has the advantage of retaining data type information and being read into memory incrementally. Note Empty lines are ignored by the parser. Interfaces: Extractor , Writable Parameters # # Name Default Type Description 1 path string The path to the NDJSON file. Example # use Rubix\\ML\\Extractors\\NDJSON ; $extractor = new NDJSON ( 'example.ndjson' ); Additional Methods # This extractor does not have any additional methods.","title":"NDJSON"},{"location":"extractors/ndjson.html#ndjson","text":"NDJSON or Newline Delimited JSON files contain rows of data encoded in Javascript Object Notation (JSON) arrays or objects. The format is like a mix of JSON and CSV and has the advantage of retaining data type information and being read into memory incrementally. Note Empty lines are ignored by the parser. Interfaces: Extractor , Writable","title":"NDJSON"},{"location":"extractors/ndjson.html#parameters","text":"# Name Default Type Description 1 path string The path to the NDJSON file.","title":"Parameters"},{"location":"extractors/ndjson.html#example","text":"use Rubix\\ML\\Extractors\\NDJSON ; $extractor = new NDJSON ( 'example.ndjson' );","title":"Example"},{"location":"extractors/ndjson.html#additional-methods","text":"This extractor does not have any additional methods.","title":"Additional Methods"},{"location":"extractors/sql-table.html","text":"[source] SQL Table # The SQL table extractor iterates over the rows of a relational database table. It works with the PHP Data Objects (PDO) interface to connect to a broad selection of databases such MySQL, PostgreSQL, and Sqlite. Note This extractor requires the PDO extension . Note The order in which the rows are iterated over is not guaranteed. Use a custom query with ORDER BY statement if ordering matters. Interfaces: Extractor Parameters # # Name Default Type Description 1 connection PDO The PDO connection to the database. 2 table string The name of the table to select from. 3 batch size 256 int The number of rows of the table to load in a single query. Example # use Rubix\\ML\\Extractors\\SQLTable ; use PDO ; $connection = new PDO ( 'sqlite:/example.sqlite' ); $this -> extractor = new SQLTable ( $connection , 'users' , 256 ); Additional Methods # Return the column titles of the data table. public header () : array","title":"SQL Table"},{"location":"extractors/sql-table.html#sql-table","text":"The SQL table extractor iterates over the rows of a relational database table. It works with the PHP Data Objects (PDO) interface to connect to a broad selection of databases such MySQL, PostgreSQL, and Sqlite. Note This extractor requires the PDO extension . Note The order in which the rows are iterated over is not guaranteed. Use a custom query with ORDER BY statement if ordering matters. Interfaces: Extractor","title":"SQL Table"},{"location":"extractors/sql-table.html#parameters","text":"# Name Default Type Description 1 connection PDO The PDO connection to the database. 2 table string The name of the table to select from. 3 batch size 256 int The number of rows of the table to load in a single query.","title":"Parameters"},{"location":"extractors/sql-table.html#example","text":"use Rubix\\ML\\Extractors\\SQLTable ; use PDO ; $connection = new PDO ( 'sqlite:/example.sqlite' ); $this -> extractor = new SQLTable ( $connection , 'users' , 256 );","title":"Example"},{"location":"extractors/sql-table.html#additional-methods","text":"Return the column titles of the data table. public header () : array","title":"Additional Methods"},{"location":"graph/trees/ball-tree.html","text":"[source] Ball Tree # A binary spatial tree that partitions a dataset into successively smaller and tighter ball nodes whose boundaries are defined by a hypersphere. Ball Tree works well in higher dimensions since the partitioning schema does not rely on a finite number of 1-dimensional axis aligned splits such as with k-d tree . Interfaces: Binary Tree, Spatial Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 maxLeafSize 30 int The maximum number of samples that each leaf node can contain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. Example # use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $tree = new BallTree ( 40 , new Euclidean ()); Additional Methods # This tree does not have any additional methods. References # S. M. Omohundro. (1989). Five Balltree Construction Algorithms. \u21a9 M. Dolatshah et al. (2015). Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor search in metric spaces. \u21a9","title":"Ball Tree"},{"location":"graph/trees/ball-tree.html#ball-tree","text":"A binary spatial tree that partitions a dataset into successively smaller and tighter ball nodes whose boundaries are defined by a hypersphere. Ball Tree works well in higher dimensions since the partitioning schema does not rely on a finite number of 1-dimensional axis aligned splits such as with k-d tree . Interfaces: Binary Tree, Spatial Data Type Compatibility: Depends on distance kernel","title":"Ball Tree"},{"location":"graph/trees/ball-tree.html#parameters","text":"# Name Default Type Description 1 maxLeafSize 30 int The maximum number of samples that each leaf node can contain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between sample points.","title":"Parameters"},{"location":"graph/trees/ball-tree.html#example","text":"use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $tree = new BallTree ( 40 , new Euclidean ());","title":"Example"},{"location":"graph/trees/ball-tree.html#additional-methods","text":"This tree does not have any additional methods.","title":"Additional Methods"},{"location":"graph/trees/ball-tree.html#references","text":"S. M. Omohundro. (1989). Five Balltree Construction Algorithms. \u21a9 M. Dolatshah et al. (2015). Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor search in metric spaces. \u21a9","title":"References"},{"location":"graph/trees/k-d-tree.html","text":"[source] K-d Tree # A multi-dimensional binary spatial tree for fast nearest neighbor queries. The K-d tree construction algorithm separates data points into bounded hypercubes or boxes that are used to determine which branches to prune off during nearest neighbor and range searches enabling them to complete in sub-linear time. Interfaces: Binary Tree, Spatial Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 maxLeafSize 30 int The maximum number of samples that each leaf node can contain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. Example # use Rubix\\ML\\Graph\\Trees\\KDTree ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $tree = new KDTree ( 30 , new Euclidean ()); Additional Methods # This tree does not have any additional methods. References # J. L. Bentley. (1975). Multidimensional Binary Search Trees Used for Associative Searching. \u21a9","title":"K-d Tree"},{"location":"graph/trees/k-d-tree.html#k-d-tree","text":"A multi-dimensional binary spatial tree for fast nearest neighbor queries. The K-d tree construction algorithm separates data points into bounded hypercubes or boxes that are used to determine which branches to prune off during nearest neighbor and range searches enabling them to complete in sub-linear time. Interfaces: Binary Tree, Spatial Data Type Compatibility: Continuous","title":"K-d Tree"},{"location":"graph/trees/k-d-tree.html#parameters","text":"# Name Default Type Description 1 maxLeafSize 30 int The maximum number of samples that each leaf node can contain. 2 kernel Euclidean Distance The distance kernel used to compute the distance between sample points.","title":"Parameters"},{"location":"graph/trees/k-d-tree.html#example","text":"use Rubix\\ML\\Graph\\Trees\\KDTree ; use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $tree = new KDTree ( 30 , new Euclidean ());","title":"Example"},{"location":"graph/trees/k-d-tree.html#additional-methods","text":"This tree does not have any additional methods.","title":"Additional Methods"},{"location":"graph/trees/k-d-tree.html#references","text":"J. L. Bentley. (1975). Multidimensional Binary Search Trees Used for Associative Searching. \u21a9","title":"References"},{"location":"helpers/params.html","text":"Params # Generate distributions of values to use in conjunction with Grid Search or other forms of model selection and/or cross validation. Generate Params # To generate a unique distribution of integer parameters: public static ints ( int $min , int $max , int $n = 10 ) : array use Rubix\\ML\\Helpers\\Params ; $ints = Params :: ints ( 0 , 100 , 5 ); print_r ( $ints ); Array ( [ 0 ] => 88 [ 1 ] => 48 [ 2 ] => 64 [ 3 ] => 100 [ 4 ] => 42 ) To generate a random distribution of floating point parameters: public static floats ( float $min , float $max , int $n = 10 ) : array use Rubix\\ML\\Helpers\\Params ; $floats = Params :: floats ( 0 , 100 , 5 ); print_r ( $floats ); Array ( [ 0 ] => 42.65728 [ 1 ] => 66.74335 [ 2 ] => 15.17243 [ 3 ] => 71.92631 [ 4 ] => 4.638863 ) To generate a uniformly spaced grid of parameters: public static grid ( float $min , float $max , int $n = 10 ) : array use Rubix\\ML\\Helpers\\Params ; $grid = Params :: grid ( 0 , 100 , 5 ); print_r ( $grid ); Array ( [ 0 ] => 0 [ 1 ] => 25 [ 2 ] => 50 [ 3 ] => 75 [ 4 ] => 100 )","title":"Params"},{"location":"helpers/params.html#params","text":"Generate distributions of values to use in conjunction with Grid Search or other forms of model selection and/or cross validation.","title":"Params"},{"location":"helpers/params.html#generate-params","text":"To generate a unique distribution of integer parameters: public static ints ( int $min , int $max , int $n = 10 ) : array use Rubix\\ML\\Helpers\\Params ; $ints = Params :: ints ( 0 , 100 , 5 ); print_r ( $ints ); Array ( [ 0 ] => 88 [ 1 ] => 48 [ 2 ] => 64 [ 3 ] => 100 [ 4 ] => 42 ) To generate a random distribution of floating point parameters: public static floats ( float $min , float $max , int $n = 10 ) : array use Rubix\\ML\\Helpers\\Params ; $floats = Params :: floats ( 0 , 100 , 5 ); print_r ( $floats ); Array ( [ 0 ] => 42.65728 [ 1 ] => 66.74335 [ 2 ] => 15.17243 [ 3 ] => 71.92631 [ 4 ] => 4.638863 ) To generate a uniformly spaced grid of parameters: public static grid ( float $min , float $max , int $n = 10 ) : array use Rubix\\ML\\Helpers\\Params ; $grid = Params :: grid ( 0 , 100 , 5 ); print_r ( $grid ); Array ( [ 0 ] => 0 [ 1 ] => 25 [ 2 ] => 50 [ 3 ] => 75 [ 4 ] => 100 )","title":"Generate Params"},{"location":"kernels/distance/canberra.html","text":"[source] Canberra # A weighted version of the Manhattan distance, Canberra examines the sum of a series of fractional differences between two samples. Canberra can be very sensitive when both coordinates are near zero. \\[ Canberra(\\mathbf {a} ,\\mathbf {b} )=\\sum _{i=1}^{n}{\\frac {|a_{i}-b_{i}|}{|a_{i}|+|b_{i}|}} \\] Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Canberra ; $kernel = new Canberra (); References # G. N. Lance et al. (1967). Mixed-data classificatory programs I. Agglomerative Systems. \u21a9","title":"Canberra"},{"location":"kernels/distance/canberra.html#canberra","text":"A weighted version of the Manhattan distance, Canberra examines the sum of a series of fractional differences between two samples. Canberra can be very sensitive when both coordinates are near zero. \\[ Canberra(\\mathbf {a} ,\\mathbf {b} )=\\sum _{i=1}^{n}{\\frac {|a_{i}-b_{i}|}{|a_{i}|+|b_{i}|}} \\] Data Type Compatibility: Continuous","title":"Canberra"},{"location":"kernels/distance/canberra.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/canberra.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Canberra ; $kernel = new Canberra ();","title":"Example"},{"location":"kernels/distance/canberra.html#references","text":"G. N. Lance et al. (1967). Mixed-data classificatory programs I. Agglomerative Systems. \u21a9","title":"References"},{"location":"kernels/distance/cosine.html","text":"[source] Cosine # Cosine Similarity is a measure that ignores the magnitude of the distance between two non-zero vectors thus acting as strictly a judgement of orientation. Two vectors with the same orientation have a cosine similarity of 1, whereas two vectors oriented at 90\u00b0 relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1. To be used as a distance function, we subtract the Cosine Similarity from 1 in order to satisfy the positive semi-definite condition, therefore the Cosine distance is a number between 0 and 2. \\[ {\\displaystyle {\\text{Cosine}}=1 - {\\mathbf {A} \\cdot \\mathbf {B} \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}=1 - {\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}}} \\] Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Cosine ; $kernel = new Cosine ();","title":"Cosine"},{"location":"kernels/distance/cosine.html#cosine","text":"Cosine Similarity is a measure that ignores the magnitude of the distance between two non-zero vectors thus acting as strictly a judgement of orientation. Two vectors with the same orientation have a cosine similarity of 1, whereas two vectors oriented at 90\u00b0 relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1. To be used as a distance function, we subtract the Cosine Similarity from 1 in order to satisfy the positive semi-definite condition, therefore the Cosine distance is a number between 0 and 2. \\[ {\\displaystyle {\\text{Cosine}}=1 - {\\mathbf {A} \\cdot \\mathbf {B} \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}=1 - {\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}}} \\] Data Type Compatibility: Continuous","title":"Cosine"},{"location":"kernels/distance/cosine.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/cosine.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Cosine ; $kernel = new Cosine ();","title":"Example"},{"location":"kernels/distance/diagonal.html","text":"[source] Diagonal # The Diagonal (a.k.a. Chebyshev ) distance is a measure that constrains movement to horizontal, vertical, and diagonal. An example of a game that uses diagonal movement is chess. \\[ {\\displaystyle Diagonal(a,b)=\\max _{i}(|a_{i}-b_{i}|)} \\] Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Diagonal ; $kernel = new Diagonal ();","title":"Diagonal"},{"location":"kernels/distance/diagonal.html#diagonal","text":"The Diagonal (a.k.a. Chebyshev ) distance is a measure that constrains movement to horizontal, vertical, and diagonal. An example of a game that uses diagonal movement is chess. \\[ {\\displaystyle Diagonal(a,b)=\\max _{i}(|a_{i}-b_{i}|)} \\] Data Type Compatibility: Continuous","title":"Diagonal"},{"location":"kernels/distance/diagonal.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/diagonal.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Diagonal ; $kernel = new Diagonal ();","title":"Example"},{"location":"kernels/distance/euclidean.html","text":"[source] Euclidean # The straight line ( bee line) distance between two points. Euclidean distance has the nice property of being invariant under any rotation. \\[ Euclidean\\left(a,b\\right) = \\sqrt {\\sum _{i=1}^{n} \\left( a_{i}-b_{i}\\right)^2} \\] Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $kernel = new Euclidean (); References # J. K. Dixon. (1978). Pattern Recognition with Partly Missing Data. \u21a9","title":"Euclidean"},{"location":"kernels/distance/euclidean.html#euclidean","text":"The straight line ( bee line) distance between two points. Euclidean distance has the nice property of being invariant under any rotation. \\[ Euclidean\\left(a,b\\right) = \\sqrt {\\sum _{i=1}^{n} \\left( a_{i}-b_{i}\\right)^2} \\] Data Type Compatibility: Continuous","title":"Euclidean"},{"location":"kernels/distance/euclidean.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/euclidean.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Euclidean ; $kernel = new Euclidean ();","title":"Example"},{"location":"kernels/distance/euclidean.html#references","text":"J. K. Dixon. (1978). Pattern Recognition with Partly Missing Data. \u21a9","title":"References"},{"location":"kernels/distance/hamming.html","text":"[source] Hamming # A categorical distance function that measures distance as the number of substitutions necessary to convert one sample to the other. Data Type Compatibility: Categorical Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Hamming ; $kernel = new Hamming (); References # R. W. Hamming. (1950). Error detecting and error correcting codes. \u21a9","title":"Hamming"},{"location":"kernels/distance/hamming.html#hamming","text":"A categorical distance function that measures distance as the number of substitutions necessary to convert one sample to the other. Data Type Compatibility: Categorical","title":"Hamming"},{"location":"kernels/distance/hamming.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/hamming.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Hamming ; $kernel = new Hamming ();","title":"Example"},{"location":"kernels/distance/hamming.html#references","text":"R. W. Hamming. (1950). Error detecting and error correcting codes. \u21a9","title":"References"},{"location":"kernels/distance/jaccard.html","text":"[source] Jaccard # The generalized Jaccard distance is a measure of distance with a range from 0 to 1 and can be thought of as the size of the intersection divided by the size of the union of two points if they were consisted only of binary random variables. Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Jaccard ; $kernel = new Jaccard ();","title":"Jaccard"},{"location":"kernels/distance/jaccard.html#jaccard","text":"The generalized Jaccard distance is a measure of distance with a range from 0 to 1 and can be thought of as the size of the intersection divided by the size of the union of two points if they were consisted only of binary random variables. Data Type Compatibility: Continuous","title":"Jaccard"},{"location":"kernels/distance/jaccard.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/jaccard.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Jaccard ; $kernel = new Jaccard ();","title":"Example"},{"location":"kernels/distance/manhattan.html","text":"[source] Manhattan # A distance metric that constrains movement to horizontal and vertical, similar to navigating the city blocks of Manhattan. An example of a board game that uses this type of movement is Checkers. \\[ Manhattan(\\mathbf {a} ,\\mathbf {b})=\\|\\mathbf {a} -\\mathbf {b} \\|_{1}=\\sum _{i=1}^{n}|a_{i}-b_{i}| \\] Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $kernel = new Manhattan ();","title":"Manhattan"},{"location":"kernels/distance/manhattan.html#manhattan","text":"A distance metric that constrains movement to horizontal and vertical, similar to navigating the city blocks of Manhattan. An example of a board game that uses this type of movement is Checkers. \\[ Manhattan(\\mathbf {a} ,\\mathbf {b})=\\|\\mathbf {a} -\\mathbf {b} \\|_{1}=\\sum _{i=1}^{n}|a_{i}-b_{i}| \\] Data Type Compatibility: Continuous","title":"Manhattan"},{"location":"kernels/distance/manhattan.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/manhattan.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $kernel = new Manhattan ();","title":"Example"},{"location":"kernels/distance/minkowski.html","text":"[source] Minkowski # The Minkowski distance can be considered as a generalization of both the Euclidean and Manhattan distances. When the lambda parameter is set to 1 or 2, the distance is equivalent to Manhattan and Euclidean respectively. \\[ {\\displaystyle Minkowski\\left(a,b\\right)=\\left(\\sum _{i=1}^{n}|a_{i}-b_{i}|^{p}\\right)^{\\frac {1}{p}}} \\] Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 lambda 3.0 float Controls the curvature of the unit circle drawn from a point at a fixed distance. Example # use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $kernel = new Minkowski ( 4.0 );","title":"Minkowski"},{"location":"kernels/distance/minkowski.html#minkowski","text":"The Minkowski distance can be considered as a generalization of both the Euclidean and Manhattan distances. When the lambda parameter is set to 1 or 2, the distance is equivalent to Manhattan and Euclidean respectively. \\[ {\\displaystyle Minkowski\\left(a,b\\right)=\\left(\\sum _{i=1}^{n}|a_{i}-b_{i}|^{p}\\right)^{\\frac {1}{p}}} \\] Data Type Compatibility: Continuous","title":"Minkowski"},{"location":"kernels/distance/minkowski.html#parameters","text":"# Name Default Type Description 1 lambda 3.0 float Controls the curvature of the unit circle drawn from a point at a fixed distance.","title":"Parameters"},{"location":"kernels/distance/minkowski.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\Minkowski ; $kernel = new Minkowski ( 4.0 );","title":"Example"},{"location":"kernels/distance/safe-euclidean.html","text":"[source] Safe Euclidean # An Euclidean distance metric suitable for samples that may contain NaN (not a number) values i.e. missing data. The Safe Euclidean metric approximates the Euclidean distance function by dropping NaN values and scaling the distance according to the proportion of non-NaNs (in either a or b or both) to compensate. Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $kernel = new SafeEuclidean (); References # J. K. Dixon. (1978). Pattern Recognition with Partly Missing Data. \u21a9","title":"Safe Euclidean"},{"location":"kernels/distance/safe-euclidean.html#safe-euclidean","text":"An Euclidean distance metric suitable for samples that may contain NaN (not a number) values i.e. missing data. The Safe Euclidean metric approximates the Euclidean distance function by dropping NaN values and scaling the distance according to the proportion of non-NaNs (in either a or b or both) to compensate. Data Type Compatibility: Continuous","title":"Safe Euclidean"},{"location":"kernels/distance/safe-euclidean.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/safe-euclidean.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $kernel = new SafeEuclidean ();","title":"Example"},{"location":"kernels/distance/safe-euclidean.html#references","text":"J. K. Dixon. (1978). Pattern Recognition with Partly Missing Data. \u21a9","title":"References"},{"location":"kernels/distance/sparse-cosine.html","text":"[source] Sparse Cosine # A version of the Cosine distance kernel that is specifically optimized for computing sparse vectors. Data Type Compatibility: Continuous Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\Distance\\SparseCosine ; $kernel = new SparseCosine ();","title":"Sparse Cosine"},{"location":"kernels/distance/sparse-cosine.html#sparse-cosine","text":"A version of the Cosine distance kernel that is specifically optimized for computing sparse vectors. Data Type Compatibility: Continuous","title":"Sparse Cosine"},{"location":"kernels/distance/sparse-cosine.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/distance/sparse-cosine.html#example","text":"use Rubix\\ML\\Kernels\\Distance\\SparseCosine ; $kernel = new SparseCosine ();","title":"Example"},{"location":"kernels/svm/linear.html","text":"[source] Linear # A simple linear kernel computed by the dot product of two vectors. Parameters # This kernel does not have any parameters. Example # use Rubix\\ML\\Kernels\\SVM\\Linear ; $kernel = new Linear ();","title":"Linear"},{"location":"kernels/svm/linear.html#linear","text":"A simple linear kernel computed by the dot product of two vectors.","title":"Linear"},{"location":"kernels/svm/linear.html#parameters","text":"This kernel does not have any parameters.","title":"Parameters"},{"location":"kernels/svm/linear.html#example","text":"use Rubix\\ML\\Kernels\\SVM\\Linear ; $kernel = new Linear ();","title":"Example"},{"location":"kernels/svm/polynomial.html","text":"[source] Polynomial # This kernel projects a sample vector using polynomials of the p'th degree. Parameters # # Name Default Type Description 1 degree 3 int The degree of the polynomial. 2 gamma null float The kernel coefficient. 3 coef0 0. float The independent term. Example # use Rubix\\ML\\Kernels\\SVM\\Polynomial ; $kernel = new Polynomial ( 3 , null , 0. );","title":"Polynomial"},{"location":"kernels/svm/polynomial.html#polynomial","text":"This kernel projects a sample vector using polynomials of the p'th degree.","title":"Polynomial"},{"location":"kernels/svm/polynomial.html#parameters","text":"# Name Default Type Description 1 degree 3 int The degree of the polynomial. 2 gamma null float The kernel coefficient. 3 coef0 0. float The independent term.","title":"Parameters"},{"location":"kernels/svm/polynomial.html#example","text":"use Rubix\\ML\\Kernels\\SVM\\Polynomial ; $kernel = new Polynomial ( 3 , null , 0. );","title":"Example"},{"location":"kernels/svm/rbf.html","text":"[source] RBF # Non linear radial basis function (RBF) computes the distance from a centroid or origin. Parameters # # Name Default Type Description 1 gamma null float The kernel coefficient. Example # use Rubix\\ML\\Kernels\\SVM\\RBF ; $kernel = new RBF ( null );","title":"RBF"},{"location":"kernels/svm/rbf.html#rbf","text":"Non linear radial basis function (RBF) computes the distance from a centroid or origin.","title":"RBF"},{"location":"kernels/svm/rbf.html#parameters","text":"# Name Default Type Description 1 gamma null float The kernel coefficient.","title":"Parameters"},{"location":"kernels/svm/rbf.html#example","text":"use Rubix\\ML\\Kernels\\SVM\\RBF ; $kernel = new RBF ( null );","title":"Example"},{"location":"kernels/svm/sigmoidal.html","text":"[source] Sigmoidal # S shaped nonliearity kernel with output values ranging from -1 to 1. Parameters # # Name Default Type Description 1 gamma null float The kernel coefficient. 2 coef0 0. float The independent term. Example # use Rubix\\ML\\Kernels\\SVM\\Sigmoidal ; $kernel = new Sigmoidal ( null , 0. );","title":"Sigmoidal"},{"location":"kernels/svm/sigmoidal.html#sigmoidal","text":"S shaped nonliearity kernel with output values ranging from -1 to 1.","title":"Sigmoidal"},{"location":"kernels/svm/sigmoidal.html#parameters","text":"# Name Default Type Description 1 gamma null float The kernel coefficient. 2 coef0 0. float The independent term.","title":"Parameters"},{"location":"kernels/svm/sigmoidal.html#example","text":"use Rubix\\ML\\Kernels\\SVM\\Sigmoidal ; $kernel = new Sigmoidal ( null , 0. );","title":"Example"},{"location":"loggers/screen.html","text":"[source] Screen # A logger that displays log messages to the standard output. Parameters # # Name Default Type Description 1 channel '' string The channel name that appears on each line. 2 timestampFormat 'Y-m-d H:i:s' string The format of the timestamp. Example # use Rubix\\ML\\Loggers\\Screen ; $logger = new Screen ( 'mlp' , 'Y-m-d H:i:s' );","title":"Screen"},{"location":"loggers/screen.html#screen","text":"A logger that displays log messages to the standard output.","title":"Screen"},{"location":"loggers/screen.html#parameters","text":"# Name Default Type Description 1 channel '' string The channel name that appears on each line. 2 timestampFormat 'Y-m-d H:i:s' string The format of the timestamp.","title":"Parameters"},{"location":"loggers/screen.html#example","text":"use Rubix\\ML\\Loggers\\Screen ; $logger = new Screen ( 'mlp' , 'Y-m-d H:i:s' );","title":"Example"},{"location":"neural-network/activation-functions/elu.html","text":"[source] ELU # Exponential Linear Units are a type of rectifier that soften the transition from non-activated to activated using the exponential function. As such, ELU produces smoother gradients than the piecewise linear ReLU function. \\[ {\\displaystyle ELU = {\\begin{cases}\\alpha \\left(e^{x}-1\\right)&{\\text{if }}x\\leq 0\\\\x&{\\text{if }}x>0\\end{cases}}} \\] Parameters # # Name Default Type Description 1 alpha 1.0 float The value at which leakage will begin to saturate. Ex. alpha = 1.0 means that the output will never be less than -1.0 when inactivated. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ELU ; $activationFunction = new ELU ( 2.5 ); References # D. A. Clevert et al. (2016). Fast and Accurate Deep Network Learning by Exponential Linear Units. \u21a9","title":"ELU"},{"location":"neural-network/activation-functions/elu.html#elu","text":"Exponential Linear Units are a type of rectifier that soften the transition from non-activated to activated using the exponential function. As such, ELU produces smoother gradients than the piecewise linear ReLU function. \\[ {\\displaystyle ELU = {\\begin{cases}\\alpha \\left(e^{x}-1\\right)&{\\text{if }}x\\leq 0\\\\x&{\\text{if }}x>0\\end{cases}}} \\]","title":"ELU"},{"location":"neural-network/activation-functions/elu.html#parameters","text":"# Name Default Type Description 1 alpha 1.0 float The value at which leakage will begin to saturate. Ex. alpha = 1.0 means that the output will never be less than -1.0 when inactivated.","title":"Parameters"},{"location":"neural-network/activation-functions/elu.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ELU ; $activationFunction = new ELU ( 2.5 );","title":"Example"},{"location":"neural-network/activation-functions/elu.html#references","text":"D. A. Clevert et al. (2016). Fast and Accurate Deep Network Learning by Exponential Linear Units. \u21a9","title":"References"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html","text":"[source] Hyperbolic Tangent # An S-shaped function that squeezes the input value into an output space between -1 and 1. Hyperbolic Tangent (or tanh ) has the advantage of being zero centered, however is known to saturate with highly positive or negative input values which can slow down training if the activations become too intense. \\[ {\\displaystyle \\tanh(x)={\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}} \\] Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\HyperbolicTangent ; $activationFunction = new HyperbolicTangent ();","title":"Hyperbolic Tangent"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html#hyperbolic-tangent","text":"An S-shaped function that squeezes the input value into an output space between -1 and 1. Hyperbolic Tangent (or tanh ) has the advantage of being zero centered, however is known to saturate with highly positive or negative input values which can slow down training if the activations become too intense. \\[ {\\displaystyle \\tanh(x)={\\frac {e^{x}-e^{-x}}{e^{x}+e^{-x}}}} \\]","title":"Hyperbolic Tangent"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/hyperbolic-tangent.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\HyperbolicTangent ; $activationFunction = new HyperbolicTangent ();","title":"Example"},{"location":"neural-network/activation-functions/leaky-relu.html","text":"[source] Leaky ReLU # Leaky Rectified Linear Units are activation functions that output x when x is greater or equal to 0 or x scaled by a small leakage coefficient when the input is less than 0. Leaky rectifiers have the benefit of allowing a small gradient to flow through during backpropagation even though they might not have activated during the forward pass. \\[ {\\displaystyle LeakyReLU = {\\begin{cases}\\lambda x&{\\text{if }}x<0\\\\x&{\\text{if }}x\\geq 0\\end{cases}}} \\] Parameters # # Name Default Type Description 1 leakage 0.1 float The amount of leakage as a proportion of the input value to allow to pass through when not inactivated. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\LeakyReLU ; $activationFunction = new LeakyReLU ( 0.3 ); References # A. L. Maas et al. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models. \u21a9","title":"Leaky ReLU"},{"location":"neural-network/activation-functions/leaky-relu.html#leaky-relu","text":"Leaky Rectified Linear Units are activation functions that output x when x is greater or equal to 0 or x scaled by a small leakage coefficient when the input is less than 0. Leaky rectifiers have the benefit of allowing a small gradient to flow through during backpropagation even though they might not have activated during the forward pass. \\[ {\\displaystyle LeakyReLU = {\\begin{cases}\\lambda x&{\\text{if }}x<0\\\\x&{\\text{if }}x\\geq 0\\end{cases}}} \\]","title":"Leaky ReLU"},{"location":"neural-network/activation-functions/leaky-relu.html#parameters","text":"# Name Default Type Description 1 leakage 0.1 float The amount of leakage as a proportion of the input value to allow to pass through when not inactivated.","title":"Parameters"},{"location":"neural-network/activation-functions/leaky-relu.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\LeakyReLU ; $activationFunction = new LeakyReLU ( 0.3 );","title":"Example"},{"location":"neural-network/activation-functions/leaky-relu.html#references","text":"A. L. Maas et al. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models. \u21a9","title":"References"},{"location":"neural-network/activation-functions/relu.html","text":"[source] ReLU # Rectified Linear Units (ReLU) only output the positive signal of the input. They have the benefit of having a monotonic derivative and are cheap to compute. \\[ {\\displaystyle ReLU = {\\begin{aligned}&{\\begin{cases}0&{\\text{if }}x\\leq 0\\\\x&{\\text{if }}x>0\\end{cases}}=&\\max\\{0,x\\}\\end{aligned}}} \\] Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; $activationFunction = new ReLU ( 0.1 ); References # A. L. Maas et al. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models. \u21a9 K. Konda et al. (2015). Zero-bias Autoencoders and the Benefits of Co-adapting Features. \u21a9","title":"ReLU"},{"location":"neural-network/activation-functions/relu.html#relu","text":"Rectified Linear Units (ReLU) only output the positive signal of the input. They have the benefit of having a monotonic derivative and are cheap to compute. \\[ {\\displaystyle ReLU = {\\begin{aligned}&{\\begin{cases}0&{\\text{if }}x\\leq 0\\\\x&{\\text{if }}x>0\\end{cases}}=&\\max\\{0,x\\}\\end{aligned}}} \\]","title":"ReLU"},{"location":"neural-network/activation-functions/relu.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/relu.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; $activationFunction = new ReLU ( 0.1 );","title":"Example"},{"location":"neural-network/activation-functions/relu.html#references","text":"A. L. Maas et al. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models. \u21a9 K. Konda et al. (2015). Zero-bias Autoencoders and the Benefits of Co-adapting Features. \u21a9","title":"References"},{"location":"neural-network/activation-functions/selu.html","text":"[source] SELU # Scaled Exponential Linear Units (SELU) are a self-normalizing activation function based on the ELU activation function. Neuronal activations of SELU networks automatically converge toward zero mean and unit variance, unlike explicitly normalized networks such as those with Batch Norm hidden layers. \\[ {\\displaystyle SELU = 1.0507 {\\begin{cases}1.67326 (e^{x}-1)&{\\text{if }}x<0\\\\x&{\\text{if }}x\\geq 0\\end{cases}}} \\] Parameters # This actvation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SELU ; $activationFunction = new SELU (); References # G. Klambauer et al. (2017). Self-Normalizing Neural Networks. \u21a9","title":"SELU"},{"location":"neural-network/activation-functions/selu.html#selu","text":"Scaled Exponential Linear Units (SELU) are a self-normalizing activation function based on the ELU activation function. Neuronal activations of SELU networks automatically converge toward zero mean and unit variance, unlike explicitly normalized networks such as those with Batch Norm hidden layers. \\[ {\\displaystyle SELU = 1.0507 {\\begin{cases}1.67326 (e^{x}-1)&{\\text{if }}x<0\\\\x&{\\text{if }}x\\geq 0\\end{cases}}} \\]","title":"SELU"},{"location":"neural-network/activation-functions/selu.html#parameters","text":"This actvation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/selu.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SELU ; $activationFunction = new SELU ();","title":"Example"},{"location":"neural-network/activation-functions/selu.html#references","text":"G. Klambauer et al. (2017). Self-Normalizing Neural Networks. \u21a9","title":"References"},{"location":"neural-network/activation-functions/sigmoid.html","text":"[source] Sigmoid # A bounded S-shaped function (sometimes called the Logistic function) with an output value between 0 and 1. The output of the sigmoid function has the advantage of being interpretable as a probability, however it is not zero-centered and tends to saturate if inputs become large. \\[ {\\displaystyle Sigmoid = {\\frac {1}{1+e^{-x}}}} \\] Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Sigmoid ; $activationFunction = new Sigmoid ();","title":"Sigmoid"},{"location":"neural-network/activation-functions/sigmoid.html#sigmoid","text":"A bounded S-shaped function (sometimes called the Logistic function) with an output value between 0 and 1. The output of the sigmoid function has the advantage of being interpretable as a probability, however it is not zero-centered and tends to saturate if inputs become large. \\[ {\\displaystyle Sigmoid = {\\frac {1}{1+e^{-x}}}} \\]","title":"Sigmoid"},{"location":"neural-network/activation-functions/sigmoid.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/sigmoid.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Sigmoid ; $activationFunction = new Sigmoid ();","title":"Example"},{"location":"neural-network/activation-functions/silu.html","text":"[source] SiLU # Sigmoid Linear Units are smooth and non-monotonic rectified activation functions. Their inputs are weighted by the Sigmoid activation function acting as a self-gating mechanism. Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SiLU ; $activationFunction = new SiLU (); References # S. Elwing et al. (2017). Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. \u21a9","title":"SiLU"},{"location":"neural-network/activation-functions/silu.html#silu","text":"Sigmoid Linear Units are smooth and non-monotonic rectified activation functions. Their inputs are weighted by the Sigmoid activation function acting as a self-gating mechanism.","title":"SiLU"},{"location":"neural-network/activation-functions/silu.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/silu.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SiLU ; $activationFunction = new SiLU ();","title":"Example"},{"location":"neural-network/activation-functions/silu.html#references","text":"S. Elwing et al. (2017). Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. \u21a9","title":"References"},{"location":"neural-network/activation-functions/soft-plus.html","text":"[source] Soft Plus # A smooth approximation of the piecewise linear ReLU activation function. \\[ {\\displaystyle Soft-Plus = \\log \\left(1+e^{x}\\right)} \\] Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SoftPlus ; $activationFunction = new SoftPlus (); References # X. Glorot et al. (2011). Deep Sparse Rectifier Neural Networks. \u21a9","title":"Soft Plus"},{"location":"neural-network/activation-functions/soft-plus.html#soft-plus","text":"A smooth approximation of the piecewise linear ReLU activation function. \\[ {\\displaystyle Soft-Plus = \\log \\left(1+e^{x}\\right)} \\]","title":"Soft Plus"},{"location":"neural-network/activation-functions/soft-plus.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/soft-plus.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\SoftPlus ; $activationFunction = new SoftPlus ();","title":"Example"},{"location":"neural-network/activation-functions/soft-plus.html#references","text":"X. Glorot et al. (2011). Deep Sparse Rectifier Neural Networks. \u21a9","title":"References"},{"location":"neural-network/activation-functions/softmax.html","text":"[source] Softmax # The Softmax function is a generalization of the Sigmoid function that squashes each activation between 0 and 1 with the addition that all activations add up to 1. Together, these properties allow the output of the Softmax function to be interpretable as a joint probability distribution. \\[ {\\displaystyle Softmax = {\\frac {e^{x_{i}}}{\\sum _{j=1}^{J}e^{x_{j}}}}} \\] Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Softmax ; $activationFunction = new Softmax ();","title":"Softmax"},{"location":"neural-network/activation-functions/softmax.html#softmax","text":"The Softmax function is a generalization of the Sigmoid function that squashes each activation between 0 and 1 with the addition that all activations add up to 1. Together, these properties allow the output of the Softmax function to be interpretable as a joint probability distribution. \\[ {\\displaystyle Softmax = {\\frac {e^{x_{i}}}{\\sum _{j=1}^{J}e^{x_{j}}}}} \\]","title":"Softmax"},{"location":"neural-network/activation-functions/softmax.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/softmax.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Softmax ; $activationFunction = new Softmax ();","title":"Example"},{"location":"neural-network/activation-functions/softsign.html","text":"[source] Softsign # A smooth sigmoid-shaped function that squashes the input between -1 and 1. \\[ {\\displaystyle Softsign = {\\frac {x}{1+|x|}}} \\] Parameters # This activation function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Softsign ; $activationFunction = new Softsign (); References # X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks. \u21a9","title":"Soft Sign"},{"location":"neural-network/activation-functions/softsign.html#softsign","text":"A smooth sigmoid-shaped function that squashes the input between -1 and 1. \\[ {\\displaystyle Softsign = {\\frac {x}{1+|x|}}} \\]","title":"Softsign"},{"location":"neural-network/activation-functions/softsign.html#parameters","text":"This activation function does not have any parameters.","title":"Parameters"},{"location":"neural-network/activation-functions/softsign.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\Softsign ; $activationFunction = new Softsign ();","title":"Example"},{"location":"neural-network/activation-functions/softsign.html#references","text":"X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks. \u21a9","title":"References"},{"location":"neural-network/activation-functions/thresholded-relu.html","text":"[source] Thresholded ReLU # A version of the ReLU function that activates only if the input is above some user-specified threshold level. \\[ {\\displaystyle ThresholdedReLU = {\\begin{aligned}&{\\begin{cases}0&{\\text{if }}x\\leq \\theta \\\\x&{\\text{if }}x>\\theta\\end{cases}}\\end{aligned}}} \\] Parameters # # Name Default Type Description 1 threshold 1.0 float The threshold at which the neuron is activated. Example # use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ThresholdedReLU ; $activationFunction = new ThresholdedReLU ( 0.5 ); References # K. Konda et al. (2015). Zero-bias autoencoders and the benefits of co-adapting features. \u21a9","title":"Thresholded ReLU"},{"location":"neural-network/activation-functions/thresholded-relu.html#thresholded-relu","text":"A version of the ReLU function that activates only if the input is above some user-specified threshold level. \\[ {\\displaystyle ThresholdedReLU = {\\begin{aligned}&{\\begin{cases}0&{\\text{if }}x\\leq \\theta \\\\x&{\\text{if }}x>\\theta\\end{cases}}\\end{aligned}}} \\]","title":"Thresholded ReLU"},{"location":"neural-network/activation-functions/thresholded-relu.html#parameters","text":"# Name Default Type Description 1 threshold 1.0 float The threshold at which the neuron is activated.","title":"Parameters"},{"location":"neural-network/activation-functions/thresholded-relu.html#example","text":"use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ThresholdedReLU ; $activationFunction = new ThresholdedReLU ( 0.5 );","title":"Example"},{"location":"neural-network/activation-functions/thresholded-relu.html#references","text":"K. Konda et al. (2015). Zero-bias autoencoders and the benefits of co-adapting features. \u21a9","title":"References"},{"location":"neural-network/cost-functions/cross-entropy.html","text":"[source] Cross Entropy # Cross Entropy (or log loss ) measures the performance of a classification model whose output is a joint probability distribution over the possible classes. Entropy increases as the predicted probability distribution diverges from the actual distribution. \\[ Cross Entropy = -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}) \\] Parameters # This cost function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; $costFunction = new CrossEntropy ();","title":"Cross Entropy"},{"location":"neural-network/cost-functions/cross-entropy.html#cross-entropy","text":"Cross Entropy (or log loss ) measures the performance of a classification model whose output is a joint probability distribution over the possible classes. Entropy increases as the predicted probability distribution diverges from the actual distribution. \\[ Cross Entropy = -\\sum_{c=1}^My_{o,c}\\log(p_{o,c}) \\]","title":"Cross Entropy"},{"location":"neural-network/cost-functions/cross-entropy.html#parameters","text":"This cost function does not have any parameters.","title":"Parameters"},{"location":"neural-network/cost-functions/cross-entropy.html#example","text":"use Rubix\\ML\\NeuralNet\\CostFunctions\\CrossEntropy ; $costFunction = new CrossEntropy ();","title":"Example"},{"location":"neural-network/cost-functions/huber-loss.html","text":"[source] Huber Loss # The pseudo Huber Loss function transitions between L1 and L2 loss at a given pivot point (defined by delta ) such that the function becomes more quadratic as the loss decreases. The combination of L1 and L2 losses make Huber more robust to outliers while maintaining smoothness near the minimum. \\[ L_{\\delta}= \\left\\{\\begin{matrix} \\frac{1}{2}(y - \\hat{y})^{2} & if \\left | (y - \\hat{y}) \\right | < \\delta\\\\ \\delta ((y - \\hat{y}) - \\frac1 2 \\delta) & otherwise \\end{matrix}\\right. \\] Parameters # # Name Default Type Description 1 delta 1.0 float The pivot point i.e the point where numbers larger will be evaluated with an L1 loss while number smaller will be evaluated with an L2 loss. Example # use Rubix\\ML\\NeuralNet\\CostFunctions\\HuberLoss ; $costFunction = new HuberLoss ( 0.5 );","title":"Huber Loss"},{"location":"neural-network/cost-functions/huber-loss.html#huber-loss","text":"The pseudo Huber Loss function transitions between L1 and L2 loss at a given pivot point (defined by delta ) such that the function becomes more quadratic as the loss decreases. The combination of L1 and L2 losses make Huber more robust to outliers while maintaining smoothness near the minimum. \\[ L_{\\delta}= \\left\\{\\begin{matrix} \\frac{1}{2}(y - \\hat{y})^{2} & if \\left | (y - \\hat{y}) \\right | < \\delta\\\\ \\delta ((y - \\hat{y}) - \\frac1 2 \\delta) & otherwise \\end{matrix}\\right. \\]","title":"Huber Loss"},{"location":"neural-network/cost-functions/huber-loss.html#parameters","text":"# Name Default Type Description 1 delta 1.0 float The pivot point i.e the point where numbers larger will be evaluated with an L1 loss while number smaller will be evaluated with an L2 loss.","title":"Parameters"},{"location":"neural-network/cost-functions/huber-loss.html#example","text":"use Rubix\\ML\\NeuralNet\\CostFunctions\\HuberLoss ; $costFunction = new HuberLoss ( 0.5 );","title":"Example"},{"location":"neural-network/cost-functions/least-squares.html","text":"[source] Least Squares # Least Squares (or quadratic loss) is a function that computes the average squared error (MSE) between the target output given by the labels and the actual output of the network. It produces a smooth bowl-shaped gradient that is highly-influenced by large errors. \\[ Least Squares = \\sum_{i=1}^{D}(y_i-\\hat{y}_i)^2 \\] Parameters # This cost function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\CostFunctions\\LeastSquares ; $costFunction = new LeastSquares ();","title":"Least Squares"},{"location":"neural-network/cost-functions/least-squares.html#least-squares","text":"Least Squares (or quadratic loss) is a function that computes the average squared error (MSE) between the target output given by the labels and the actual output of the network. It produces a smooth bowl-shaped gradient that is highly-influenced by large errors. \\[ Least Squares = \\sum_{i=1}^{D}(y_i-\\hat{y}_i)^2 \\]","title":"Least Squares"},{"location":"neural-network/cost-functions/least-squares.html#parameters","text":"This cost function does not have any parameters.","title":"Parameters"},{"location":"neural-network/cost-functions/least-squares.html#example","text":"use Rubix\\ML\\NeuralNet\\CostFunctions\\LeastSquares ; $costFunction = new LeastSquares ();","title":"Example"},{"location":"neural-network/cost-functions/relative-entropy.html","text":"[source] Relative Entropy # Relative Entropy (or Kullback-Leibler divergence ) is a measure of how the expectation and activation of the network diverge. It is different from Cross Entropy in that it is asymmetric and thus does not qualify as a statistical measure of error. \\[ KL(\\hat{y} || y) = \\sum_{c=1}^{M}\\hat{y}_c \\log{\\frac{\\hat{y}_c}{y_c}} \\] Parameters # This cost function does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\CostFunctions\\RelativeEntropy ; $costFunction = new RelativeEntropy ();","title":"Relative Entropy"},{"location":"neural-network/cost-functions/relative-entropy.html#relative-entropy","text":"Relative Entropy (or Kullback-Leibler divergence ) is a measure of how the expectation and activation of the network diverge. It is different from Cross Entropy in that it is asymmetric and thus does not qualify as a statistical measure of error. \\[ KL(\\hat{y} || y) = \\sum_{c=1}^{M}\\hat{y}_c \\log{\\frac{\\hat{y}_c}{y_c}} \\]","title":"Relative Entropy"},{"location":"neural-network/cost-functions/relative-entropy.html#parameters","text":"This cost function does not have any parameters.","title":"Parameters"},{"location":"neural-network/cost-functions/relative-entropy.html#example","text":"use Rubix\\ML\\NeuralNet\\CostFunctions\\RelativeEntropy ; $costFunction = new RelativeEntropy ();","title":"Example"},{"location":"neural-network/hidden-layers/activation.html","text":"[source] Activation # Activation layers apply a user-defined non-linear activation function to their inputs. They often work in conjunction with Dense layers as a way to transform their output. Parameters # # Name Default Type Description 1 activationFn ActivationFunction The function that computes the output of the layer. Example # use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; $layer = new Activation ( new ReLU ());","title":"Activation"},{"location":"neural-network/hidden-layers/activation.html#activation","text":"Activation layers apply a user-defined non-linear activation function to their inputs. They often work in conjunction with Dense layers as a way to transform their output.","title":"Activation"},{"location":"neural-network/hidden-layers/activation.html#parameters","text":"# Name Default Type Description 1 activationFn ActivationFunction The function that computes the output of the layer.","title":"Parameters"},{"location":"neural-network/hidden-layers/activation.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; $layer = new Activation ( new ReLU ());","title":"Example"},{"location":"neural-network/hidden-layers/batch-norm.html","text":"[source] Batch Norm # Batch Norm layers normalize the activations of the previous layer such that the mean activation is close to 0 and the standard deviation is close to 1. Adding Batch Norm reduces the amount of covariate shift within the network which makes it possible to use higher learning rates and thus converge faster under some circumstances. Parameters # # Name Default Type Description 1 decay 0.9 float The decay rate of the previous running averages of the global mean and variance. 2 betaInitializer Constant Initializer The initializer of the beta parameter. 3 gammaInitializer Constant Initializer The initializer of the gamma parameter. Example # use Rubix\\ML\\NeuralNet\\Layers\\BatchNorm ; use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; use Rubix\\ML\\NeuralNet\\Initializers\\Normal ; $layer = new BatchNorm ( 0.7 , new Constant ( 0. ), new Normal ( 1. )); References # S. Ioffe et al. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. \u21a9","title":"Batch Norm"},{"location":"neural-network/hidden-layers/batch-norm.html#batch-norm","text":"Batch Norm layers normalize the activations of the previous layer such that the mean activation is close to 0 and the standard deviation is close to 1. Adding Batch Norm reduces the amount of covariate shift within the network which makes it possible to use higher learning rates and thus converge faster under some circumstances.","title":"Batch Norm"},{"location":"neural-network/hidden-layers/batch-norm.html#parameters","text":"# Name Default Type Description 1 decay 0.9 float The decay rate of the previous running averages of the global mean and variance. 2 betaInitializer Constant Initializer The initializer of the beta parameter. 3 gammaInitializer Constant Initializer The initializer of the gamma parameter.","title":"Parameters"},{"location":"neural-network/hidden-layers/batch-norm.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\BatchNorm ; use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; use Rubix\\ML\\NeuralNet\\Initializers\\Normal ; $layer = new BatchNorm ( 0.7 , new Constant ( 0. ), new Normal ( 1. ));","title":"Example"},{"location":"neural-network/hidden-layers/batch-norm.html#references","text":"S. Ioffe et al. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. \u21a9","title":"References"},{"location":"neural-network/hidden-layers/dense.html","text":"[source] Dense # Dense (or fully connected ) hidden layers are layers of neurons that connect to each node in the previous layer by a parameterized synapse. They perform a linear transformation on their input and are usually followed by an Activation layer. The majority of the trainable parameters in a standard feed forward neural network are contained within Dense hidden layers. Parameters # # Name Default Type Description 1 neurons int The number of nodes in the layer. 2 l2Penalty 0.0 float The amount of L2 regularization applied to the weights. 3 bias true bool Should the layer include a bias parameter? 4 weightInitializer He Initializer The initializer of the weight parameter. 5 biasInitializer Constant Initializer The initializer of the bias parameter. Example # use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Initializers\\He ; use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; $layer = new Dense ( 100 , 1e-4 , true , new He (), new Constant ( 0.0 ));","title":"Dense"},{"location":"neural-network/hidden-layers/dense.html#dense","text":"Dense (or fully connected ) hidden layers are layers of neurons that connect to each node in the previous layer by a parameterized synapse. They perform a linear transformation on their input and are usually followed by an Activation layer. The majority of the trainable parameters in a standard feed forward neural network are contained within Dense hidden layers.","title":"Dense"},{"location":"neural-network/hidden-layers/dense.html#parameters","text":"# Name Default Type Description 1 neurons int The number of nodes in the layer. 2 l2Penalty 0.0 float The amount of L2 regularization applied to the weights. 3 bias true bool Should the layer include a bias parameter? 4 weightInitializer He Initializer The initializer of the weight parameter. 5 biasInitializer Constant Initializer The initializer of the bias parameter.","title":"Parameters"},{"location":"neural-network/hidden-layers/dense.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Initializers\\He ; use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; $layer = new Dense ( 100 , 1e-4 , true , new He (), new Constant ( 0.0 ));","title":"Example"},{"location":"neural-network/hidden-layers/dropout.html","text":"[source] Dropout # Dropout is a regularization technique to reduce overfitting in neural networks by preventing complex co-adaptations on training data. It works by temporarily disabling output nodes during each training pass. It also acts as an efficient way of performing model averaging with the parameters of neural networks. Parameters # # Name Default Type Description 1 ratio 0.5 float The ratio of nodes that are dropped during each training pass. Example # use Rubix\\ML\\NeuralNet\\Layers\\Dropout ; $layer = new Dropout ( 0.2 ); References # N. Srivastava et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. \u21a9","title":"Dropout"},{"location":"neural-network/hidden-layers/dropout.html#dropout","text":"Dropout is a regularization technique to reduce overfitting in neural networks by preventing complex co-adaptations on training data. It works by temporarily disabling output nodes during each training pass. It also acts as an efficient way of performing model averaging with the parameters of neural networks.","title":"Dropout"},{"location":"neural-network/hidden-layers/dropout.html#parameters","text":"# Name Default Type Description 1 ratio 0.5 float The ratio of nodes that are dropped during each training pass.","title":"Parameters"},{"location":"neural-network/hidden-layers/dropout.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\Dropout ; $layer = new Dropout ( 0.2 );","title":"Example"},{"location":"neural-network/hidden-layers/dropout.html#references","text":"N. Srivastava et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. \u21a9","title":"References"},{"location":"neural-network/hidden-layers/noise.html","text":"[source] Noise # This layer adds random Gaussian noise to the inputs with a user-defined standard deviation. Noise added to neural network activations acts as a regularizer by indirectly adding a penalty to the weights through the cost function in the output layer. Parameters # # Name Default Type Description 1 stddev 0.1 float The standard deviation of the Gaussian noise added to the inputs. Example # use Rubix\\ML\\NeuralNet\\Layers\\Noise ; $layer = new Noise ( 1e-3 ); References # C. Gulcehre et al. (2016). Noisy Activation Functions. \u21a9","title":"Noise"},{"location":"neural-network/hidden-layers/noise.html#noise","text":"This layer adds random Gaussian noise to the inputs with a user-defined standard deviation. Noise added to neural network activations acts as a regularizer by indirectly adding a penalty to the weights through the cost function in the output layer.","title":"Noise"},{"location":"neural-network/hidden-layers/noise.html#parameters","text":"# Name Default Type Description 1 stddev 0.1 float The standard deviation of the Gaussian noise added to the inputs.","title":"Parameters"},{"location":"neural-network/hidden-layers/noise.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\Noise ; $layer = new Noise ( 1e-3 );","title":"Example"},{"location":"neural-network/hidden-layers/noise.html#references","text":"C. Gulcehre et al. (2016). Noisy Activation Functions. \u21a9","title":"References"},{"location":"neural-network/hidden-layers/prelu.html","text":"[source] PReLU # Parametric Rectified Linear Units are leaky rectifiers whose leakage coefficient is learned during training. Unlike standard Leaky ReLUs whose leakage remains constant, PReLU layers can adjust the leakage to better suite the model on a per node basis. \\[ {\\displaystyle PReLU = {\\begin{cases}\\alpha x&{\\text{if }}x<0\\\\x&{\\text{if }}x\\geq 0\\end{cases}}} \\] Parameters # # Name Default Type Description 1 initializer Constant Initializer The initializer of the leakage parameter. Example # use Rubix\\ML\\NeuralNet\\Layers\\PReLU ; use Rubix\\ML\\NeuralNet\\Initializers\\Normal ; $layer = new PReLU ( new Normal ( 0.5 )); References # K. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. \u21a9","title":"PReLU"},{"location":"neural-network/hidden-layers/prelu.html#prelu","text":"Parametric Rectified Linear Units are leaky rectifiers whose leakage coefficient is learned during training. Unlike standard Leaky ReLUs whose leakage remains constant, PReLU layers can adjust the leakage to better suite the model on a per node basis. \\[ {\\displaystyle PReLU = {\\begin{cases}\\alpha x&{\\text{if }}x<0\\\\x&{\\text{if }}x\\geq 0\\end{cases}}} \\]","title":"PReLU"},{"location":"neural-network/hidden-layers/prelu.html#parameters","text":"# Name Default Type Description 1 initializer Constant Initializer The initializer of the leakage parameter.","title":"Parameters"},{"location":"neural-network/hidden-layers/prelu.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\PReLU ; use Rubix\\ML\\NeuralNet\\Initializers\\Normal ; $layer = new PReLU ( new Normal ( 0.5 ));","title":"Example"},{"location":"neural-network/hidden-layers/prelu.html#references","text":"K. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. \u21a9","title":"References"},{"location":"neural-network/hidden-layers/swish.html","text":"[source] Swish # Swish is a parametric activation layer that utilizes smooth rectified activation functions. The trainable beta parameter allows each activation function in the layer to tailor its output to the training set by interpolating between the linear function and ReLU. Parameters # # Name Default Type Description 1 initializer Constant Initializer The initializer of the beta parameter. Example # use Rubix\\ML\\NeuralNet\\Layers\\Swish ; use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; $layer = new Swish ( new Constant ( 1.0 )); References # P. Ramachandran er al. (2017). Swish: A Self-gated Activation Function. \u21a9 P. Ramachandran et al. (2017). Searching for Activation Functions. \u21a9","title":"Swish"},{"location":"neural-network/hidden-layers/swish.html#swish","text":"Swish is a parametric activation layer that utilizes smooth rectified activation functions. The trainable beta parameter allows each activation function in the layer to tailor its output to the training set by interpolating between the linear function and ReLU.","title":"Swish"},{"location":"neural-network/hidden-layers/swish.html#parameters","text":"# Name Default Type Description 1 initializer Constant Initializer The initializer of the beta parameter.","title":"Parameters"},{"location":"neural-network/hidden-layers/swish.html#example","text":"use Rubix\\ML\\NeuralNet\\Layers\\Swish ; use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; $layer = new Swish ( new Constant ( 1.0 ));","title":"Example"},{"location":"neural-network/hidden-layers/swish.html#references","text":"P. Ramachandran er al. (2017). Swish: A Self-gated Activation Function. \u21a9 P. Ramachandran et al. (2017). Searching for Activation Functions. \u21a9","title":"References"},{"location":"neural-network/initializers/constant.html","text":"[source] Constant # Initialize the parameter to a user-specified constant value. Parameters # # Name Default Type Description 1 value 0.0 float The value to initialize the parameter to. Example # use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; $initializer = new Constant ( 1.0 );","title":"Constant"},{"location":"neural-network/initializers/constant.html#constant","text":"Initialize the parameter to a user-specified constant value.","title":"Constant"},{"location":"neural-network/initializers/constant.html#parameters","text":"# Name Default Type Description 1 value 0.0 float The value to initialize the parameter to.","title":"Parameters"},{"location":"neural-network/initializers/constant.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\Constant ; $initializer = new Constant ( 1.0 );","title":"Example"},{"location":"neural-network/initializers/he.html","text":"[source] He # The He initializer was designed to initialize parameters that feed into rectified Activation layers such as those employing ReLU , Leaky ReLU , or ELU . It draws values from a uniform distribution with limits defined as +/- (6 / (fanIn + fanOut)) ** (1. / sqrt(2)). Parameters # This initializer does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\Initializers\\He ; $initializer = new He (); References # K. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. \u21a9","title":"He"},{"location":"neural-network/initializers/he.html#he","text":"The He initializer was designed to initialize parameters that feed into rectified Activation layers such as those employing ReLU , Leaky ReLU , or ELU . It draws values from a uniform distribution with limits defined as +/- (6 / (fanIn + fanOut)) ** (1. / sqrt(2)).","title":"He"},{"location":"neural-network/initializers/he.html#parameters","text":"This initializer does not have any parameters.","title":"Parameters"},{"location":"neural-network/initializers/he.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\He ; $initializer = new He ();","title":"Example"},{"location":"neural-network/initializers/he.html#references","text":"K. He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. \u21a9","title":"References"},{"location":"neural-network/initializers/lecun.html","text":"[source] Le Cun # Proposed by Yan Le Cun in a paper in 1998, this initializer was one of the first published attempts to control the variance of activations between layers through weight initialization. It remains a good default choice for many hidden layer configurations. Parameters # This initializer does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\Initializers\\LeCun ; $initializer = new LeCun (); References # Y. Le Cun et al. (1998). Efficient Backprop. \u21a9","title":"LeCun"},{"location":"neural-network/initializers/lecun.html#le-cun","text":"Proposed by Yan Le Cun in a paper in 1998, this initializer was one of the first published attempts to control the variance of activations between layers through weight initialization. It remains a good default choice for many hidden layer configurations.","title":"Le Cun"},{"location":"neural-network/initializers/lecun.html#parameters","text":"This initializer does not have any parameters.","title":"Parameters"},{"location":"neural-network/initializers/lecun.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\LeCun ; $initializer = new LeCun ();","title":"Example"},{"location":"neural-network/initializers/lecun.html#references","text":"Y. Le Cun et al. (1998). Efficient Backprop. \u21a9","title":"References"},{"location":"neural-network/initializers/normal.html","text":"[source] Normal # Generates a random weight matrix from a Gaussian distribution with user-specified standard deviation. Parameters # # Name Default Type Description 1 stddev 0.05 float The standard deviation of the distribution to sample from. Example # use Rubix\\ML\\NeuralNet\\Initializers\\Normal ; $initializer = new Normal ( 0.1 );","title":"Normal"},{"location":"neural-network/initializers/normal.html#normal","text":"Generates a random weight matrix from a Gaussian distribution with user-specified standard deviation.","title":"Normal"},{"location":"neural-network/initializers/normal.html#parameters","text":"# Name Default Type Description 1 stddev 0.05 float The standard deviation of the distribution to sample from.","title":"Parameters"},{"location":"neural-network/initializers/normal.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\Normal ; $initializer = new Normal ( 0.1 );","title":"Example"},{"location":"neural-network/initializers/uniform.html","text":"[source] Uniform # Generates a random uniform distribution centered at 0 and bounded at both ends by the parameter beta. Parameters # # Name Default Type Description 1 beta 0.05 float The upper and lower bound of the distribution. Example # use Rubix\\ML\\NeuralNet\\Initializers\\Uniform ; $initializer = new Uniform ( 1e-3 );","title":"Uniform"},{"location":"neural-network/initializers/uniform.html#uniform","text":"Generates a random uniform distribution centered at 0 and bounded at both ends by the parameter beta.","title":"Uniform"},{"location":"neural-network/initializers/uniform.html#parameters","text":"# Name Default Type Description 1 beta 0.05 float The upper and lower bound of the distribution.","title":"Parameters"},{"location":"neural-network/initializers/uniform.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\Uniform ; $initializer = new Uniform ( 1e-3 );","title":"Example"},{"location":"neural-network/initializers/xavier-1.html","text":"[source] Xavier 1 # The Xavier 1 initializer draws from a uniform distribution [-limit, limit] where limit is equal to sqrt(6 / (fanIn + fanOut)). This initializer is best suited for layers that feed into an activation layer that outputs a value between 0 and 1 such as Softmax or Sigmoid . Parameters # This initializer does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\Initializers\\Xavier1 ; $initializer = new Xavier1 (); References # X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks. \u21a9","title":"Xavier 1"},{"location":"neural-network/initializers/xavier-1.html#xavier-1","text":"The Xavier 1 initializer draws from a uniform distribution [-limit, limit] where limit is equal to sqrt(6 / (fanIn + fanOut)). This initializer is best suited for layers that feed into an activation layer that outputs a value between 0 and 1 such as Softmax or Sigmoid .","title":"Xavier 1"},{"location":"neural-network/initializers/xavier-1.html#parameters","text":"This initializer does not have any parameters.","title":"Parameters"},{"location":"neural-network/initializers/xavier-1.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\Xavier1 ; $initializer = new Xavier1 ();","title":"Example"},{"location":"neural-network/initializers/xavier-1.html#references","text":"X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks. \u21a9","title":"References"},{"location":"neural-network/initializers/xavier-2.html","text":"[source] Xavier 2 # The Xavier 2 initializer draws from a uniform distribution [-limit, limit] where limit is equal to (6 / (fanIn + fanOut)) ** 0.25. This initializer is best suited for layers that feed into an activation layer that outputs values between -1 and 1 such as Hyperbolic Tangent and Softsign . Parameters # This initializer does not have any parameters. Example # use Rubix\\ML\\NeuralNet\\Initializers\\Xavier2 ; $initializer = new Xavier2 (); References # X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks. \u21a9","title":"Xavier 2"},{"location":"neural-network/initializers/xavier-2.html#xavier-2","text":"The Xavier 2 initializer draws from a uniform distribution [-limit, limit] where limit is equal to (6 / (fanIn + fanOut)) ** 0.25. This initializer is best suited for layers that feed into an activation layer that outputs values between -1 and 1 such as Hyperbolic Tangent and Softsign .","title":"Xavier 2"},{"location":"neural-network/initializers/xavier-2.html#parameters","text":"This initializer does not have any parameters.","title":"Parameters"},{"location":"neural-network/initializers/xavier-2.html#example","text":"use Rubix\\ML\\NeuralNet\\Initializers\\Xavier2 ; $initializer = new Xavier2 ();","title":"Example"},{"location":"neural-network/initializers/xavier-2.html#references","text":"X. Glorot et al. (2010). Understanding the Difficulty of Training Deep Feedforward Neural Networks. \u21a9","title":"References"},{"location":"neural-network/optimizers/adagrad.html","text":"[source] AdaGrad # Short for Adaptive Gradient , the AdaGrad Optimizer speeds up the learning of parameters that do not change often and slows down the learning of parameters that do enjoy heavy activity. Due to AdaGrad's infinitely decaying step size, training may be slow or fail to converge using a low learning rate. Parameters # # Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size. Example # use Rubix\\ML\\NeuralNet\\Optimizers\\AdaGrad ; $optimizer = new AdaGrad ( 0.125 ); References # J. Duchi et al. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. \u21a9","title":"AdaGrad"},{"location":"neural-network/optimizers/adagrad.html#adagrad","text":"Short for Adaptive Gradient , the AdaGrad Optimizer speeds up the learning of parameters that do not change often and slows down the learning of parameters that do enjoy heavy activity. Due to AdaGrad's infinitely decaying step size, training may be slow or fail to converge using a low learning rate.","title":"AdaGrad"},{"location":"neural-network/optimizers/adagrad.html#parameters","text":"# Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size.","title":"Parameters"},{"location":"neural-network/optimizers/adagrad.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\AdaGrad ; $optimizer = new AdaGrad ( 0.125 );","title":"Example"},{"location":"neural-network/optimizers/adagrad.html#references","text":"J. Duchi et al. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. \u21a9","title":"References"},{"location":"neural-network/optimizers/adam.html","text":"[source] Adam # Short for Adaptive Moment Estimation , the Adam Optimizer combines both Momentum and RMS properties. In addition to storing an exponentially decaying average of past squared gradients like RMSprop , Adam also keeps an exponentially decaying average of past gradients, similar to Momentum . Whereas Momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction. Parameters # # Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 momentumDecay 0.1 float The decay rate of the accumulated velocity. 3 normDecay 0.001 float The decay rate of the rms property. Example # use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; $optimizer = new Adam ( 0.0001 , 0.1 , 0.001 ); References # D. P. Kingma et al. (2014). Adam: A Method for Stochastic Optimization. \u21a9","title":"Adam"},{"location":"neural-network/optimizers/adam.html#adam","text":"Short for Adaptive Moment Estimation , the Adam Optimizer combines both Momentum and RMS properties. In addition to storing an exponentially decaying average of past squared gradients like RMSprop , Adam also keeps an exponentially decaying average of past gradients, similar to Momentum . Whereas Momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction.","title":"Adam"},{"location":"neural-network/optimizers/adam.html#parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 momentumDecay 0.1 float The decay rate of the accumulated velocity. 3 normDecay 0.001 float The decay rate of the rms property.","title":"Parameters"},{"location":"neural-network/optimizers/adam.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; $optimizer = new Adam ( 0.0001 , 0.1 , 0.001 );","title":"Example"},{"location":"neural-network/optimizers/adam.html#references","text":"D. P. Kingma et al. (2014). Adam: A Method for Stochastic Optimization. \u21a9","title":"References"},{"location":"neural-network/optimizers/adamax.html","text":"[source] AdaMax # A version of the Adam optimizer that replaces the RMS property with the infinity norm of the past gradients. As such, AdaMax is generally more suitable for sparse parameter updates and noisy gradients. Parameters # # Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 momentumDecay 0.1 float The decay rate of the accumulated velocity. 3 normDecay 0.001 float The decay rate of the infinity norm. Example # use Rubix\\ML\\NeuralNet\\Optimizers\\AdaMax ; $optimizer = new AdaMax ( 0.0001 , 0.1 , 0.001 ); References # D. P. Kingma et al. (2014). Adam: A Method for Stochastic Optimization. \u21a9","title":"AdaMax"},{"location":"neural-network/optimizers/adamax.html#adamax","text":"A version of the Adam optimizer that replaces the RMS property with the infinity norm of the past gradients. As such, AdaMax is generally more suitable for sparse parameter updates and noisy gradients.","title":"AdaMax"},{"location":"neural-network/optimizers/adamax.html#parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 momentumDecay 0.1 float The decay rate of the accumulated velocity. 3 normDecay 0.001 float The decay rate of the infinity norm.","title":"Parameters"},{"location":"neural-network/optimizers/adamax.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\AdaMax ; $optimizer = new AdaMax ( 0.0001 , 0.1 , 0.001 );","title":"Example"},{"location":"neural-network/optimizers/adamax.html#references","text":"D. P. Kingma et al. (2014). Adam: A Method for Stochastic Optimization. \u21a9","title":"References"},{"location":"neural-network/optimizers/cyclical.html","text":"[source] Cyclical # The Cyclical optimizer uses a global learning rate that cycles between the lower and upper bound over a designated period while also decaying the upper bound by a factor at each step. Cyclical learning rates have been shown to help escape bad local minima and saddle points of the gradient. Parameters # # Name Default Type Description 1 lower 0.001 float The lower bound on the learning rate. 2 upper 0.006 float The upper bound on the learning rate. 3 steps 100 int The number of steps in every half cycle. 4 decay 0.99994 float The exponential decay factor to decrease the learning rate by every step. Example # use Rubix\\ML\\NeuralNet\\Optimizers\\Cyclical ; $optimizer = new Cyclical ( 0.001 , 0.005 , 1000 ); References # L. N. Smith. (2017). Cyclical Learning Rates for Training Neural Networks. \u21a9","title":"Cyclical"},{"location":"neural-network/optimizers/cyclical.html#cyclical","text":"The Cyclical optimizer uses a global learning rate that cycles between the lower and upper bound over a designated period while also decaying the upper bound by a factor at each step. Cyclical learning rates have been shown to help escape bad local minima and saddle points of the gradient.","title":"Cyclical"},{"location":"neural-network/optimizers/cyclical.html#parameters","text":"# Name Default Type Description 1 lower 0.001 float The lower bound on the learning rate. 2 upper 0.006 float The upper bound on the learning rate. 3 steps 100 int The number of steps in every half cycle. 4 decay 0.99994 float The exponential decay factor to decrease the learning rate by every step.","title":"Parameters"},{"location":"neural-network/optimizers/cyclical.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\Cyclical ; $optimizer = new Cyclical ( 0.001 , 0.005 , 1000 );","title":"Example"},{"location":"neural-network/optimizers/cyclical.html#references","text":"L. N. Smith. (2017). Cyclical Learning Rates for Training Neural Networks. \u21a9","title":"References"},{"location":"neural-network/optimizers/momentum.html","text":"[source] Momentum # Momentum accelerates each update step by accumulating velocity from past updates and adding a factor of the previous velocity to the current step. Momentum can help speed up training and escape bad local minima when compared with Stochastic Gradient Descent. Parameters # # Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 decay 0.1 float The decay rate of the accumulated velocity. 3 lookahead false bool Should we employ Nesterov's lookahead (NAG) when updating the parameters? Example # use Rubix\\ML\\NeuralNet\\Optimizers\\Momentum ; $optimizer = new Momentum ( 0.01 , 0.1 , true ); References # D. E. Rumelhart et al. (1988). Learning representations by back-propagating errors. \u21a9 I. Sutskever et al. (2013). On the importance of initialization and momentum in deep learning. \u21a9","title":"Momentum"},{"location":"neural-network/optimizers/momentum.html#momentum","text":"Momentum accelerates each update step by accumulating velocity from past updates and adding a factor of the previous velocity to the current step. Momentum can help speed up training and escape bad local minima when compared with Stochastic Gradient Descent.","title":"Momentum"},{"location":"neural-network/optimizers/momentum.html#parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 decay 0.1 float The decay rate of the accumulated velocity. 3 lookahead false bool Should we employ Nesterov's lookahead (NAG) when updating the parameters?","title":"Parameters"},{"location":"neural-network/optimizers/momentum.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\Momentum ; $optimizer = new Momentum ( 0.01 , 0.1 , true );","title":"Example"},{"location":"neural-network/optimizers/momentum.html#references","text":"D. E. Rumelhart et al. (1988). Learning representations by back-propagating errors. \u21a9 I. Sutskever et al. (2013). On the importance of initialization and momentum in deep learning. \u21a9","title":"References"},{"location":"neural-network/optimizers/rms-prop.html","text":"[source] RMS Prop # An adaptive gradient technique that divides the current gradient over a rolling window of the magnitudes of recent gradients. Unlike AdaGrad , RMS Prop does not suffer from an infinitely decaying step size. Parameters # # Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 decay 0.1 float The decay rate of the rms property. Example # use Rubix\\ML\\NeuralNet\\Optimizers\\RMSProp ; $optimizer = new RMSProp ( 0.01 , 0.1 ); References # T. Tieleman et al. (2012). Lecture 6e rmsprop: Divide the gradient by a running average of its recent magnitude. \u21a9","title":"RMS Prop"},{"location":"neural-network/optimizers/rms-prop.html#rms-prop","text":"An adaptive gradient technique that divides the current gradient over a rolling window of the magnitudes of recent gradients. Unlike AdaGrad , RMS Prop does not suffer from an infinitely decaying step size.","title":"RMS Prop"},{"location":"neural-network/optimizers/rms-prop.html#parameters","text":"# Name Default Type Description 1 rate 0.001 float The learning rate that controls the global step size. 2 decay 0.1 float The decay rate of the rms property.","title":"Parameters"},{"location":"neural-network/optimizers/rms-prop.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\RMSProp ; $optimizer = new RMSProp ( 0.01 , 0.1 );","title":"Example"},{"location":"neural-network/optimizers/rms-prop.html#references","text":"T. Tieleman et al. (2012). Lecture 6e rmsprop: Divide the gradient by a running average of its recent magnitude. \u21a9","title":"References"},{"location":"neural-network/optimizers/step-decay.html","text":"[source] Step Decay # A learning rate decay optimizer that reduces the global learning rate by a factor whenever it reaches a new floor . The number of steps needed to reach a new floor is defined by the steps hyper-parameter. Parameters # # Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size. 2 steps 100 int The size of every floor in steps. i.e. the number of steps to take before applying another factor of decay. 3 decay 1e-3 float The factor to decrease the learning rate at each floor . Example # use Rubix\\ML\\NeuralNet\\Optimizers\\StepDecay ; $optimizer = new StepDecay ( 0.1 , 50 , 1e-3 );","title":"Step Decay"},{"location":"neural-network/optimizers/step-decay.html#step-decay","text":"A learning rate decay optimizer that reduces the global learning rate by a factor whenever it reaches a new floor . The number of steps needed to reach a new floor is defined by the steps hyper-parameter.","title":"Step Decay"},{"location":"neural-network/optimizers/step-decay.html#parameters","text":"# Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size. 2 steps 100 int The size of every floor in steps. i.e. the number of steps to take before applying another factor of decay. 3 decay 1e-3 float The factor to decrease the learning rate at each floor .","title":"Parameters"},{"location":"neural-network/optimizers/step-decay.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\StepDecay ; $optimizer = new StepDecay ( 0.1 , 50 , 1e-3 );","title":"Example"},{"location":"neural-network/optimizers/stochastic.html","text":"[source] Stochastic # A constant learning rate optimizer based on vanilla Stochastic Gradient Descent. Parameters # # Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size. Example # use Rubix\\ML\\NeuralNet\\Optimizers\\Stochastic ; $optimizer = new Stochastic ( 0.01 );","title":"Stochastic"},{"location":"neural-network/optimizers/stochastic.html#stochastic","text":"A constant learning rate optimizer based on vanilla Stochastic Gradient Descent.","title":"Stochastic"},{"location":"neural-network/optimizers/stochastic.html#parameters","text":"# Name Default Type Description 1 rate 0.01 float The learning rate that controls the global step size.","title":"Parameters"},{"location":"neural-network/optimizers/stochastic.html#example","text":"use Rubix\\ML\\NeuralNet\\Optimizers\\Stochastic ; $optimizer = new Stochastic ( 0.01 );","title":"Example"},{"location":"persisters/api.html","text":"Persisters # Persisters are responsible for persisting Encoding objects to storage and are also used by the Persistent Model meta-estimator to save and restore models that have been serialized. Save # To save an encoding: public save ( Encoding $encoding ) : void $persister -> save ( $encoding ); Load # To load an encoding from persistence: public load () : Encoding $encoding = $persister -> load ();","title":"API Reference"},{"location":"persisters/api.html#persisters","text":"Persisters are responsible for persisting Encoding objects to storage and are also used by the Persistent Model meta-estimator to save and restore models that have been serialized.","title":"Persisters"},{"location":"persisters/api.html#save","text":"To save an encoding: public save ( Encoding $encoding ) : void $persister -> save ( $encoding );","title":"Save"},{"location":"persisters/api.html#load","text":"To load an encoding from persistence: public load () : Encoding $encoding = $persister -> load ();","title":"Load"},{"location":"persisters/filesystem.html","text":"[source] Filesystem # Filesystems are local or remote storage drives that are organized by files and folders. The Filesystem persister saves models to a file at a given path and can automatically keep a history of past saved models. Parameters # # Name Default Type Description 1 path string The path to the model file on the filesystem. 2 history false bool Should we keep a history of past saves? 3 serializer RBX Serializer The serializer used to convert to and from storage format. Example # use Rubix\\ML\\Persisters\\Filesystem ; use Rubix\\ML\\Serializers\\RBX ; $persister = new Filesystem ( '/path/to/example.model' , true , new RBX ()); Additional Methods # This persister does not have any additional methods.","title":"Filesystem"},{"location":"persisters/filesystem.html#filesystem","text":"Filesystems are local or remote storage drives that are organized by files and folders. The Filesystem persister saves models to a file at a given path and can automatically keep a history of past saved models.","title":"Filesystem"},{"location":"persisters/filesystem.html#parameters","text":"# Name Default Type Description 1 path string The path to the model file on the filesystem. 2 history false bool Should we keep a history of past saves? 3 serializer RBX Serializer The serializer used to convert to and from storage format.","title":"Parameters"},{"location":"persisters/filesystem.html#example","text":"use Rubix\\ML\\Persisters\\Filesystem ; use Rubix\\ML\\Serializers\\RBX ; $persister = new Filesystem ( '/path/to/example.model' , true , new RBX ());","title":"Example"},{"location":"persisters/filesystem.html#additional-methods","text":"This persister does not have any additional methods.","title":"Additional Methods"},{"location":"regressors/adaline.html","text":"[source] Adaline # Adaptive Linear Neuron is a single layer feed-forward neural network with a continuous linear output neuron suitable for regression tasks. Training is equivalent to solving L2 regularized linear regression ( Ridge ) online using Mini Batch Gradient Descent. Interfaces: Estimator , Learner , Online , Ranks Features , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 batchSize 128 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn LeastSquares RegressionLoss The function that computes the loss associated with an erroneous activation during training. Example # use Rubix\\ML\\Regressors\\Adaline ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\NeuralNet\\CostFunctions\\HuberLoss ; $estimator = new Adaline ( 256 , new Adam ( 0.001 ), 1e-4 , 500 , 1e-6 , 5 , new HuberLoss ( 2.5 )); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the underlying neural network instance or null if untrained: public network () : Network | null References # B. Widrow. (1960). An Adaptive \"Adaline\" Neuron Using Chemical \"Memistors\". \u21a9","title":"Adaline"},{"location":"regressors/adaline.html#adaline","text":"Adaptive Linear Neuron is a single layer feed-forward neural network with a continuous linear output neuron suitable for regression tasks. Training is equivalent to solving L2 regularized linear regression ( Ridge ) online using Mini Batch Gradient Descent. Interfaces: Estimator , Learner , Online , Ranks Features , Verbose , Persistable Data Type Compatibility: Continuous","title":"Adaline"},{"location":"regressors/adaline.html#parameters","text":"# Name Default Type Description 1 batchSize 128 int The number of training samples to process at a time. 2 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 3 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the training loss to wait before considering an early stop. 7 costFn LeastSquares RegressionLoss The function that computes the loss associated with an erroneous activation during training.","title":"Parameters"},{"location":"regressors/adaline.html#example","text":"use Rubix\\ML\\Regressors\\Adaline ; use Rubix\\ML\\NeuralNet\\Optimizers\\Adam ; use Rubix\\ML\\NeuralNet\\CostFunctions\\HuberLoss ; $estimator = new Adaline ( 256 , new Adam ( 0.001 ), 1e-4 , 500 , 1e-6 , 5 , new HuberLoss ( 2.5 ));","title":"Example"},{"location":"regressors/adaline.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the loss for each epoch from the last training session: public losses () : float [] | null Return the underlying neural network instance or null if untrained: public network () : Network | null","title":"Additional Methods"},{"location":"regressors/adaline.html#references","text":"B. Widrow. (1960). An Adaptive \"Adaline\" Neuron Using Chemical \"Memistors\". \u21a9","title":"References"},{"location":"regressors/extra-tree-regressor.html","text":"[source] Extra Tree Regressor # Extremely Randomized Regression Trees differ from standard Regression Trees in that they choose candidate splits at random rather than searching the entire feature column for the best value to split on. Extra Trees are also faster to build and their predictions have higher variance than a regular decision tree regressor. Interfaces: Estimator , Learner , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. Example # use Rubix\\ML\\Regressors\\ExtraTreeRegressor ; $estimator = new ExtraTreeRegressor ( 30 , 5 , 0.05 , null ); Additional Methods # Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int References # P. Geurts et al. (2005). Extremely Randomized Trees. \u21a9","title":"Extra Tree Regressor"},{"location":"regressors/extra-tree-regressor.html#extra-tree-regressor","text":"Extremely Randomized Regression Trees differ from standard Regression Trees in that they choose candidate splits at random rather than searching the entire feature column for the best value to split on. Extra Trees are also faster to build and their predictions have higher variance than a regular decision tree regressor. Interfaces: Estimator , Learner , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous","title":"Extra Tree Regressor"},{"location":"regressors/extra-tree-regressor.html#parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split.","title":"Parameters"},{"location":"regressors/extra-tree-regressor.html#example","text":"use Rubix\\ML\\Regressors\\ExtraTreeRegressor ; $estimator = new ExtraTreeRegressor ( 30 , 5 , 0.05 , null );","title":"Example"},{"location":"regressors/extra-tree-regressor.html#additional-methods","text":"Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int","title":"Additional Methods"},{"location":"regressors/extra-tree-regressor.html#references","text":"P. Geurts et al. (2005). Extremely Randomized Trees. \u21a9","title":"References"},{"location":"regressors/gradient-boost.html","text":"[source] Gradient Boost # Gradient Boost (GBM) is a stage-wise additive ensemble that uses a Gradient Descent boosting scheme for training boosters (Decision Trees) to correct the error residuals of a base learner. Note The default booster is a Regression Tree with a max height of 3. Note Gradient Boost utilizes progress monitoring via an internal validation set for snapshotting and early stopping. If there are not enough training samples to build an internal validation set given the user-specified holdout ratio then training will proceed with progress monitoring disabled. Interfaces: Estimator , Learner , Verbose , Ranks Features , Persistable Data Type Compatibility: Categorical and Continuous Parameters # # Name Default Type Description 1 booster RegressionTree Learner The regressor used to up the error residuals of the base learner. 2 rate 0.1 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.5 float The ratio of samples to subsample from the training set to train each booster. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 7 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 8 metric RMSE Metric The metric used to score the generalization performance of the model during training. Example # use Rubix\\ML\\Regressors\\GradientBoost ; use Rubix\\ML\\Regressors\\RegressionTree ; use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE ; $estimator = new GradientBoost ( new RegressionTree ( 3 ), 0.1 , 0.8 , 1000 , 1e-4 , 10 , 0.1 , new SMAPE ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the validation score for each epoch from the last training session: public scores () : float [] | null Return the loss for each epoch from the last training session: public losses () : float [] | null References # J. H. Friedman. (2001). Greedy Function Approximation: A Gradient Boosting Machine. \u21a9 J. H. Friedman. (1999). Stochastic Gradient Boosting. \u21a9 Y. Wei. et al. (2017). Early stopping for kernel boosting algorithms: A general analysis with localized complexities. \u21a9 G. Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. \u21a9","title":"Gradient Boost"},{"location":"regressors/gradient-boost.html#gradient-boost","text":"Gradient Boost (GBM) is a stage-wise additive ensemble that uses a Gradient Descent boosting scheme for training boosters (Decision Trees) to correct the error residuals of a base learner. Note The default booster is a Regression Tree with a max height of 3. Note Gradient Boost utilizes progress monitoring via an internal validation set for snapshotting and early stopping. If there are not enough training samples to build an internal validation set given the user-specified holdout ratio then training will proceed with progress monitoring disabled. Interfaces: Estimator , Learner , Verbose , Ranks Features , Persistable Data Type Compatibility: Categorical and Continuous","title":"Gradient Boost"},{"location":"regressors/gradient-boost.html#parameters","text":"# Name Default Type Description 1 booster RegressionTree Learner The regressor used to up the error residuals of the base learner. 2 rate 0.1 float The learning rate of the ensemble i.e. the shrinkage applied to each step. 3 ratio 0.5 float The ratio of samples to subsample from the training set to train each booster. 4 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate before terminating. 5 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 6 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 7 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 8 metric RMSE Metric The metric used to score the generalization performance of the model during training.","title":"Parameters"},{"location":"regressors/gradient-boost.html#example","text":"use Rubix\\ML\\Regressors\\GradientBoost ; use Rubix\\ML\\Regressors\\RegressionTree ; use Rubix\\ML\\CrossValidation\\Metrics\\SMAPE ; $estimator = new GradientBoost ( new RegressionTree ( 3 ), 0.1 , 0.8 , 1000 , 1e-4 , 10 , 0.1 , new SMAPE ());","title":"Example"},{"location":"regressors/gradient-boost.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the validation score for each epoch from the last training session: public scores () : float [] | null Return the loss for each epoch from the last training session: public losses () : float [] | null","title":"Additional Methods"},{"location":"regressors/gradient-boost.html#references","text":"J. H. Friedman. (2001). Greedy Function Approximation: A Gradient Boosting Machine. \u21a9 J. H. Friedman. (1999). Stochastic Gradient Boosting. \u21a9 Y. Wei. et al. (2017). Early stopping for kernel boosting algorithms: A general analysis with localized complexities. \u21a9 G. Ke et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. \u21a9","title":"References"},{"location":"regressors/kd-neighbors-regressor.html","text":"[source] K-d Neighbors Regressor # A fast implementation of KNN Regressor using a spatially-aware binary tree for nearest neighbors search. K-d Neighbors Regressor works by locating the neighborhood of a sample via binary search and then does a brute force search only on the samples close to or within the neighborhood of the unknown sample. The main advantage of K-d Neighbors over brute force KNN is inference speed, however, it cannot be partially trained. Interfaces: Estimator , Learner , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches. Example # use Rubix\\ML\\Regressors\\KDNeighborsRegressor ; use Rubix\\ML\\Graph\\Trees\\BallTree ; $estimator = new KDNeighborsRegressor ( 20 , true , new BallTree ( 50 )); Additional Methods # Return the base spatial tree instance: public tree () : Spatial","title":"K-d Neighbors Regressor"},{"location":"regressors/kd-neighbors-regressor.html#k-d-neighbors-regressor","text":"A fast implementation of KNN Regressor using a spatially-aware binary tree for nearest neighbors search. K-d Neighbors Regressor works by locating the neighborhood of a sample via binary search and then does a brute force search only on the samples close to or within the neighborhood of the unknown sample. The main advantage of K-d Neighbors over brute force KNN is inference speed, however, it cannot be partially trained. Interfaces: Estimator , Learner , Persistable Data Type Compatibility: Depends on distance kernel","title":"K-d Neighbors Regressor"},{"location":"regressors/kd-neighbors-regressor.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree KDTree Spatial The spatial tree used to run nearest neighbor searches.","title":"Parameters"},{"location":"regressors/kd-neighbors-regressor.html#example","text":"use Rubix\\ML\\Regressors\\KDNeighborsRegressor ; use Rubix\\ML\\Graph\\Trees\\BallTree ; $estimator = new KDNeighborsRegressor ( 20 , true , new BallTree ( 50 ));","title":"Example"},{"location":"regressors/kd-neighbors-regressor.html#additional-methods","text":"Return the base spatial tree instance: public tree () : Spatial","title":"Additional Methods"},{"location":"regressors/knn-regressor.html","text":"[source] KNN Regressor # K Nearest Neighbors (KNN) is a brute-force distance-based learner that locates the k nearest training samples from the training set and averages their labels to make a prediction. K Nearest Neighbors (KNN) is considered a lazy learner because it performs most of its computation at inference time. Note For a faster spatial tree-accelerated version of KNN, see KD Neighbors Regressor . Interfaces: Estimator , Learner , Online , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 kernel Euclidean Distance The distance kernel used to compute the distance between sample points. Example # use Rubix\\ML\\Regressors\\KNNRegressor ; use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $estimator = new KNNRegressor ( 5 , false , new SafeEuclidean ()); Additional Methods # This estimator does not have any additional methods.","title":"KNN Regressor"},{"location":"regressors/knn-regressor.html#knn-regressor","text":"K Nearest Neighbors (KNN) is a brute-force distance-based learner that locates the k nearest training samples from the training set and averages their labels to make a prediction. K Nearest Neighbors (KNN) is considered a lazy learner because it performs most of its computation at inference time. Note For a faster spatial tree-accelerated version of KNN, see KD Neighbors Regressor . Interfaces: Estimator , Learner , Online , Persistable Data Type Compatibility: Depends on distance kernel","title":"KNN Regressor"},{"location":"regressors/knn-regressor.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbors to consider when making a prediction. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 kernel Euclidean Distance The distance kernel used to compute the distance between sample points.","title":"Parameters"},{"location":"regressors/knn-regressor.html#example","text":"use Rubix\\ML\\Regressors\\KNNRegressor ; use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $estimator = new KNNRegressor ( 5 , false , new SafeEuclidean ());","title":"Example"},{"location":"regressors/knn-regressor.html#additional-methods","text":"This estimator does not have any additional methods.","title":"Additional Methods"},{"location":"regressors/mlp-regressor.html","text":"[source] MLP Regressor # A multilayer feed-forward neural network with a continuous output layer suitable for regression problems. The Multilayer Perceptron regressor is able to handle complex non-linear regression problems by forming higher-order representations of the input features using intermediate user-defined hidden layers. The MLP also has network snapshotting and progress monitoring to ensure that the model achieves the highest validation score per a given training time budget. Note If there are not enough training samples to build an internal validation set with the user-specified holdout ratio then progress monitoring will be disabled. Interfaces: Estimator , Learner , Online , Verbose , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 hidden array An array composing the user-specified hidden layers of the network in order. 2 batchSize 128 int The number of training samples to process at a time. 3 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 4 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 5 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 6 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 7 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 8 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 9 costFn LeastSquares RegressionLoss The function that computes the loss associated with an erroneous activation during training. 10 metric RMSE Metric The metric used to score the generalization performance of the model during training. Example # use Rubix\\ML\\Regressors\\MLPRegressor ; use Rubix\\ML\\NeuralNet\\CostFunctions\\LeastSquares ; use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; use Rubix\\ML\\NeuralNet\\Optimizers\\RMSProp ; use Rubix\\ML\\CrossValidation\\Metrics\\RSquared ; $estimator = new MLPRegressor ([ new Dense ( 100 ), new Activation ( new ReLU ()), new Dense ( 100 ), new Activation ( new ReLU ()), new Dense ( 50 ), new Activation ( new ReLU ()), new Dense ( 50 ), new Activation ( new ReLU ()), ], 128 , new RMSProp ( 0.001 ), 1e-3 , 100 , 1e-5 , 3 , 0.1 , new LeastSquares (), new RSquared ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the validation score for each epoch from the last training session: public scores () : float [] | null Return the loss for each epoch from the last training session: public losses () : float [] | null Returns the underlying neural network instance or null if untrained: public network () : Network | null References # G. E. Hinton. (1989). Connectionist learning procedures. \u21a9 L. Prechelt. (1997). Early Stopping - but when? \u21a9","title":"MLP Regressor"},{"location":"regressors/mlp-regressor.html#mlp-regressor","text":"A multilayer feed-forward neural network with a continuous output layer suitable for regression problems. The Multilayer Perceptron regressor is able to handle complex non-linear regression problems by forming higher-order representations of the input features using intermediate user-defined hidden layers. The MLP also has network snapshotting and progress monitoring to ensure that the model achieves the highest validation score per a given training time budget. Note If there are not enough training samples to build an internal validation set with the user-specified holdout ratio then progress monitoring will be disabled. Interfaces: Estimator , Learner , Online , Verbose , Persistable Data Type Compatibility: Continuous","title":"MLP Regressor"},{"location":"regressors/mlp-regressor.html#parameters","text":"# Name Default Type Description 1 hidden array An array composing the user-specified hidden layers of the network in order. 2 batchSize 128 int The number of training samples to process at a time. 3 optimizer Adam Optimizer The gradient descent optimizer used to update the network parameters. 4 l2Penalty 1e-4 float The amount of L2 regularization applied to the weights of the output layer. 5 epochs 1000 int The maximum number of training epochs. i.e. the number of times to iterate over the entire training set before terminating. 6 minChange 1e-4 float The minimum change in the training loss necessary to continue training. 7 window 5 int The number of epochs without improvement in the validation score to wait before considering an early stop. 8 holdOut 0.1 float The proportion of training samples to use for internal validation. Set to 0 to disable. 9 costFn LeastSquares RegressionLoss The function that computes the loss associated with an erroneous activation during training. 10 metric RMSE Metric The metric used to score the generalization performance of the model during training.","title":"Parameters"},{"location":"regressors/mlp-regressor.html#example","text":"use Rubix\\ML\\Regressors\\MLPRegressor ; use Rubix\\ML\\NeuralNet\\CostFunctions\\LeastSquares ; use Rubix\\ML\\NeuralNet\\Layers\\Dense ; use Rubix\\ML\\NeuralNet\\Layers\\Activation ; use Rubix\\ML\\NeuralNet\\ActivationFunctions\\ReLU ; use Rubix\\ML\\NeuralNet\\Optimizers\\RMSProp ; use Rubix\\ML\\CrossValidation\\Metrics\\RSquared ; $estimator = new MLPRegressor ([ new Dense ( 100 ), new Activation ( new ReLU ()), new Dense ( 100 ), new Activation ( new ReLU ()), new Dense ( 50 ), new Activation ( new ReLU ()), new Dense ( 50 ), new Activation ( new ReLU ()), ], 128 , new RMSProp ( 0.001 ), 1e-3 , 100 , 1e-5 , 3 , 0.1 , new LeastSquares (), new RSquared ());","title":"Example"},{"location":"regressors/mlp-regressor.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $estimator -> steps ()); Return the validation score for each epoch from the last training session: public scores () : float [] | null Return the loss for each epoch from the last training session: public losses () : float [] | null Returns the underlying neural network instance or null if untrained: public network () : Network | null","title":"Additional Methods"},{"location":"regressors/mlp-regressor.html#references","text":"G. E. Hinton. (1989). Connectionist learning procedures. \u21a9 L. Prechelt. (1997). Early Stopping - but when? \u21a9","title":"References"},{"location":"regressors/radius-neighbors-regressor.html","text":"[source] Radius Neighbors Regressor # This is the regressor version of Radius Neighbors implementing a binary spatial tree under the hood for fast radius queries. The prediction is a weighted average of each label from the training set that is within a fixed user-defined radius. Note : Samples with 0 neighbors within radius will be predicted NaN . Interfaces: Estimator , Learner , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 radius 1.0 float The radius within which points are considered neighbors. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree BallTree Spatial The spatial tree used to run range searches. Example # use Rubix\\ML\\Regressors\\RadiusNeighborsRegressor ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Diagonal ; $estimator = new RadiusNeighborsRegressor ( 0.5 , false , new BallTree ( 30 , new Diagonal ())); Additional Methods # Return the base spatial tree instance: public tree () : Spatial","title":"Radius Neighbors Regressor"},{"location":"regressors/radius-neighbors-regressor.html#radius-neighbors-regressor","text":"This is the regressor version of Radius Neighbors implementing a binary spatial tree under the hood for fast radius queries. The prediction is a weighted average of each label from the training set that is within a fixed user-defined radius. Note : Samples with 0 neighbors within radius will be predicted NaN . Interfaces: Estimator , Learner , Persistable Data Type Compatibility: Depends on distance kernel","title":"Radius Neighbors Regressor"},{"location":"regressors/radius-neighbors-regressor.html#parameters","text":"# Name Default Type Description 1 radius 1.0 float The radius within which points are considered neighbors. 2 weighted false bool Should we consider the distances of our nearest neighbors when making predictions? 3 tree BallTree Spatial The spatial tree used to run range searches.","title":"Parameters"},{"location":"regressors/radius-neighbors-regressor.html#example","text":"use Rubix\\ML\\Regressors\\RadiusNeighborsRegressor ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\Diagonal ; $estimator = new RadiusNeighborsRegressor ( 0.5 , false , new BallTree ( 30 , new Diagonal ()));","title":"Example"},{"location":"regressors/radius-neighbors-regressor.html#additional-methods","text":"Return the base spatial tree instance: public tree () : Spatial","title":"Additional Methods"},{"location":"regressors/regression-tree.html","text":"[source] Regression Tree # A decision tree based on the CART ( Classification and Regression Tree ) learning algorithm that performs greedy splitting by minimizing the variance of the labels at each node split. Regression Trees can be used on their own or as the booster in algorithms such as Gradient Boost . Interfaces: Estimator , Learner , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. 5 maxBins Auto int The maximum number of bins to consider when determining a split with a continuous feature as the split point. Example # use Rubix\\ML\\Regressors\\RegressionTree ; $estimator = new RegressionTree ( 20 , 2 , 1e-3 , 10 , null ); Additional Methods # Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int References: # W. Y. Loh. (2011). Classification and Regression Trees. \u21a9 K. Alsabti. et al. (1998). CLOUDS: A Decision Tree Classifier for Large Datasets. \u21a9","title":"Regression Tree"},{"location":"regressors/regression-tree.html#regression-tree","text":"A decision tree based on the CART ( Classification and Regression Tree ) learning algorithm that performs greedy splitting by minimizing the variance of the labels at each node split. Regression Trees can be used on their own or as the booster in algorithms such as Gradient Boost . Interfaces: Estimator , Learner , Ranks Features , Persistable Data Type Compatibility: Categorical, Continuous","title":"Regression Tree"},{"location":"regressors/regression-tree.html#parameters","text":"# Name Default Type Description 1 maxHeight PHP_INT_MAX int The maximum height of the tree. 2 maxLeafSize 3 int The max number of samples that a leaf node can contain. 3 minPurityIncrease 1e-7 float The minimum increase in purity necessary to continue splitting a subtree. 4 maxFeatures Auto int The max number of feature columns to consider when determining a best split. 5 maxBins Auto int The maximum number of bins to consider when determining a split with a continuous feature as the split point.","title":"Parameters"},{"location":"regressors/regression-tree.html#example","text":"use Rubix\\ML\\Regressors\\RegressionTree ; $estimator = new RegressionTree ( 20 , 2 , 1e-3 , 10 , null );","title":"Example"},{"location":"regressors/regression-tree.html#additional-methods","text":"Return the number of levels in the tree. public height () : ? int Return a factor that quantifies the skewness of the distribution of nodes in the tree. public balance () : ? int","title":"Additional Methods"},{"location":"regressors/regression-tree.html#references","text":"W. Y. Loh. (2011). Classification and Regression Trees. \u21a9 K. Alsabti. et al. (1998). CLOUDS: A Decision Tree Classifier for Large Datasets. \u21a9","title":"References:"},{"location":"regressors/ridge.html","text":"[source] Ridge # L2 regularized linear regression solved using a closed-form solution. The addition of regularization, controlled by the alpha hyper-parameter, makes Ridge less likely to overfit the training data than ordinary least squares (OLS). Interfaces: Estimator , Learner , Ranks Features , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 l2Penalty 1.0 float The strength of the L2 regularization penalty. Example # use Rubix\\ML\\Regressors\\Ridge ; $estimator = new Ridge ( 2.0 ); Additional Methods # Return the weights of features in the decision function. public coefficients () : array | null Return the bias added to the decision function. public bias () : float | null","title":"Ridge"},{"location":"regressors/ridge.html#ridge","text":"L2 regularized linear regression solved using a closed-form solution. The addition of regularization, controlled by the alpha hyper-parameter, makes Ridge less likely to overfit the training data than ordinary least squares (OLS). Interfaces: Estimator , Learner , Ranks Features , Persistable Data Type Compatibility: Continuous","title":"Ridge"},{"location":"regressors/ridge.html#parameters","text":"# Name Default Type Description 1 l2Penalty 1.0 float The strength of the L2 regularization penalty.","title":"Parameters"},{"location":"regressors/ridge.html#example","text":"use Rubix\\ML\\Regressors\\Ridge ; $estimator = new Ridge ( 2.0 );","title":"Example"},{"location":"regressors/ridge.html#additional-methods","text":"Return the weights of features in the decision function. public coefficients () : array | null Return the bias added to the decision function. public bias () : float | null","title":"Additional Methods"},{"location":"regressors/svr.html","text":"[source] SVR # The Support Vector Machine Regressor (SVR) is a maximum margin algorithm for the purposes of regression. Similarly to the SVC , the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction given by parameter epsilon . Thus, the value of epsilon defines a margin of tolerance where no penalty is given to errors. Note This estimator requires the SVM extension which uses the libsvm engine under the hood. Interfaces: Estimator , Learner Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 c 1.0 float The parameter that defines the width of the margin used to separate the classes. 2 epsilon 0.1 float Specifies the margin within which no penalty is associated in the training loss. 3 kernel RBF Kernel The kernel function used to operate in higher dimensions. 4 shrinking true bool Should we use the shrinking heuristic? 5 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 6 cache size 100.0 float The size of the kernel cache in MB. Additional Methods # Save the model data to the filesystem: public save ( string $path ) : void Load the model data from the filesystem: public load ( string $path ) : void Example # use Rubix\\ML\\Regressors\\SVR ; use Rubix\\ML\\Kernels\\SVM\\RBF ; $estimator = new SVR ( 1.0 , 0.03 , new RBF (), true , 1e-3 , 256.0 ); References # C. Chang et al. (2011). LIBSVM: A library for support vector machines. \u21a9 A. Smola et al. (2003). A Tutorial on Support Vector Regression. \u21a9","title":"SVR"},{"location":"regressors/svr.html#svr","text":"The Support Vector Machine Regressor (SVR) is a maximum margin algorithm for the purposes of regression. Similarly to the SVC , the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction given by parameter epsilon . Thus, the value of epsilon defines a margin of tolerance where no penalty is given to errors. Note This estimator requires the SVM extension which uses the libsvm engine under the hood. Interfaces: Estimator , Learner Data Type Compatibility: Continuous","title":"SVR"},{"location":"regressors/svr.html#parameters","text":"# Name Default Type Description 1 c 1.0 float The parameter that defines the width of the margin used to separate the classes. 2 epsilon 0.1 float Specifies the margin within which no penalty is associated in the training loss. 3 kernel RBF Kernel The kernel function used to operate in higher dimensions. 4 shrinking true bool Should we use the shrinking heuristic? 5 tolerance 1e-3 float The minimum change in the cost function necessary to continue training. 6 cache size 100.0 float The size of the kernel cache in MB.","title":"Parameters"},{"location":"regressors/svr.html#additional-methods","text":"Save the model data to the filesystem: public save ( string $path ) : void Load the model data from the filesystem: public load ( string $path ) : void","title":"Additional Methods"},{"location":"regressors/svr.html#example","text":"use Rubix\\ML\\Regressors\\SVR ; use Rubix\\ML\\Kernels\\SVM\\RBF ; $estimator = new SVR ( 1.0 , 0.03 , new RBF (), true , 1e-3 , 256.0 );","title":"Example"},{"location":"regressors/svr.html#references","text":"C. Chang et al. (2011). LIBSVM: A library for support vector machines. \u21a9 A. Smola et al. (2003). A Tutorial on Support Vector Regression. \u21a9","title":"References"},{"location":"serializers/api.html","text":"Serializers # Serializers take objects that implement the Persistable interface and convert them into blobs of data called encodings . Encodings can then be used to either store an object or to reinstantiate an object from storage. Serialize # To serialize a persistable object into an encoding: public serialize ( Persistable $persistable ) : Encoding $encoding = $serializer -> serialize ( $persistable ); Deserialize # To deserialize a persistable object from an encoding: public deserialize ( Encoding $encoding ) : Persistable $persistable = $serializer -> deserialize ( $encoding );","title":"API Reference"},{"location":"serializers/api.html#serializers","text":"Serializers take objects that implement the Persistable interface and convert them into blobs of data called encodings . Encodings can then be used to either store an object or to reinstantiate an object from storage.","title":"Serializers"},{"location":"serializers/api.html#serialize","text":"To serialize a persistable object into an encoding: public serialize ( Persistable $persistable ) : Encoding $encoding = $serializer -> serialize ( $persistable );","title":"Serialize"},{"location":"serializers/api.html#deserialize","text":"To deserialize a persistable object from an encoding: public deserialize ( Encoding $encoding ) : Persistable $persistable = $serializer -> deserialize ( $encoding );","title":"Deserialize"},{"location":"serializers/gzip-native.html","text":"[source] Gzip Native # Gzip Native wraps the native PHP serialization format in an outer compression layer based on the DEFLATE algorithm with a header and CRC32 checksum. Parameters # # Name Default Type Description 1 level 6 int The compression level between 0 and 9, 0 meaning no compression. Example # use Rubix\\ML\\Serializers\\GzipNative ; $serializer = new GzipNative ( 1 ); References # P. Deutsch. (1996). RFC 1951 - DEFLATE Compressed Data Format Specification version. \u21a9","title":"Gzip Native"},{"location":"serializers/gzip-native.html#gzip-native","text":"Gzip Native wraps the native PHP serialization format in an outer compression layer based on the DEFLATE algorithm with a header and CRC32 checksum.","title":"Gzip Native"},{"location":"serializers/gzip-native.html#parameters","text":"# Name Default Type Description 1 level 6 int The compression level between 0 and 9, 0 meaning no compression.","title":"Parameters"},{"location":"serializers/gzip-native.html#example","text":"use Rubix\\ML\\Serializers\\GzipNative ; $serializer = new GzipNative ( 1 );","title":"Example"},{"location":"serializers/gzip-native.html#references","text":"P. Deutsch. (1996). RFC 1951 - DEFLATE Compressed Data Format Specification version. \u21a9","title":"References"},{"location":"serializers/native.html","text":"[source] Native # The native bytecode format that comes bundled with PHP core. Parameters # This serializer does not have any parameters. Example # use Rubix\\ML\\Serializers\\Native ; $serializer = new Native ();","title":"Native"},{"location":"serializers/native.html#native","text":"The native bytecode format that comes bundled with PHP core.","title":"Native"},{"location":"serializers/native.html#parameters","text":"This serializer does not have any parameters.","title":"Parameters"},{"location":"serializers/native.html#example","text":"use Rubix\\ML\\Serializers\\Native ; $serializer = new Native ();","title":"Example"},{"location":"serializers/rbx.html","text":"[source] RBX # Rubix Object File format (RBX) is a format designed to reliably store and share serialized PHP objects. Based on PHP's native serialization format, RBX adds additional layers of compression, data integrity checks, and class compatibility detection all in one robust format. Note We recommend to use the .rbx file extension when storing RBX-serialized PHP objects. Parameters # Parameters # # Name Default Type Description 1 level 6 int The compression level between 0 and 9, 0 meaning no compression. Example # use Rubix\\ML\\Serializers\\RBX ; $serializer = new RBX ( 6 );","title":"RBX"},{"location":"serializers/rbx.html#rbx","text":"Rubix Object File format (RBX) is a format designed to reliably store and share serialized PHP objects. Based on PHP's native serialization format, RBX adds additional layers of compression, data integrity checks, and class compatibility detection all in one robust format. Note We recommend to use the .rbx file extension when storing RBX-serialized PHP objects.","title":"RBX"},{"location":"serializers/rbx.html#parameters","text":"","title":"Parameters"},{"location":"serializers/rbx.html#parameters_1","text":"# Name Default Type Description 1 level 6 int The compression level between 0 and 9, 0 meaning no compression.","title":"Parameters"},{"location":"serializers/rbx.html#example","text":"use Rubix\\ML\\Serializers\\RBX ; $serializer = new RBX ( 6 );","title":"Example"},{"location":"strategies/constant.html","text":"[source] ' Constant # Always guess the same value. Data Type: Continuous Parameters # # Name Default Type Description 1 value 0.0 float The value to constantly guess. Example # use Rubix\\ML\\Strategies\\Constant ; $strategy = new Constant ( 0.0 );","title":"Constant"},{"location":"strategies/constant.html#constant","text":"Always guess the same value. Data Type: Continuous","title":"Constant"},{"location":"strategies/constant.html#parameters","text":"# Name Default Type Description 1 value 0.0 float The value to constantly guess.","title":"Parameters"},{"location":"strategies/constant.html#example","text":"use Rubix\\ML\\Strategies\\Constant ; $strategy = new Constant ( 0.0 );","title":"Example"},{"location":"strategies/k-most-frequent.html","text":"[source] K Most Frequent # This Strategy outputs one of k most frequently occurring classes at random with equal probability. Data Type: Categorical Parameters # # Name Default Type Description 1 k 1 int The number of most frequent classes to consider. Example # use Rubix\\ML\\Strategies\\KMostFrequent ; $strategy = new KMostFrequent ( 5 );","title":"K Most Frequent"},{"location":"strategies/k-most-frequent.html#k-most-frequent","text":"This Strategy outputs one of k most frequently occurring classes at random with equal probability. Data Type: Categorical","title":"K Most Frequent"},{"location":"strategies/k-most-frequent.html#parameters","text":"# Name Default Type Description 1 k 1 int The number of most frequent classes to consider.","title":"Parameters"},{"location":"strategies/k-most-frequent.html#example","text":"use Rubix\\ML\\Strategies\\KMostFrequent ; $strategy = new KMostFrequent ( 5 );","title":"Example"},{"location":"strategies/mean.html","text":"[source] Mean # This strategy always predicts the mean of the fitted data. Data Type: Continuous Parameters # This strategy does not have any parameters. Example # use Rubix\\ML\\Strategies\\Mean ; $strategy = new Mean ();","title":"Mean"},{"location":"strategies/mean.html#mean","text":"This strategy always predicts the mean of the fitted data. Data Type: Continuous","title":"Mean"},{"location":"strategies/mean.html#parameters","text":"This strategy does not have any parameters.","title":"Parameters"},{"location":"strategies/mean.html#example","text":"use Rubix\\ML\\Strategies\\Mean ; $strategy = new Mean ();","title":"Example"},{"location":"strategies/percentile.html","text":"[source] Blurry Percentile # A strategy that always guesses the p-th percentile of the fitted data. Data Type: Continuous Parameters # # Name Default Type Description 1 p 50.0 float The percentile of the fitted data to use as a guess. Example # use Rubix\\ML\\Strategies\\Percentile ; $strategy = new Percentile ( 90.0 );","title":"Percentile"},{"location":"strategies/percentile.html#blurry-percentile","text":"A strategy that always guesses the p-th percentile of the fitted data. Data Type: Continuous","title":"Blurry Percentile"},{"location":"strategies/percentile.html#parameters","text":"# Name Default Type Description 1 p 50.0 float The percentile of the fitted data to use as a guess.","title":"Parameters"},{"location":"strategies/percentile.html#example","text":"use Rubix\\ML\\Strategies\\Percentile ; $strategy = new Percentile ( 90.0 );","title":"Example"},{"location":"strategies/prior.html","text":"[source] Prior # A strategy where the probability of guessing a class is equal to the class's prior probability. Data Type: Categorical Parameters # This strategy does not have any parameters. Example # use Rubix\\ML\\Strategies\\Prior ; $strategy = new Prior ();","title":"Prior"},{"location":"strategies/prior.html#prior","text":"A strategy where the probability of guessing a class is equal to the class's prior probability. Data Type: Categorical","title":"Prior"},{"location":"strategies/prior.html#parameters","text":"This strategy does not have any parameters.","title":"Parameters"},{"location":"strategies/prior.html#example","text":"use Rubix\\ML\\Strategies\\Prior ; $strategy = new Prior ();","title":"Example"},{"location":"strategies/wild-guess.html","text":"[source] Wild Guess # Guess a random number somewhere between the minimum and maximum computed by fitting a collection of values. Data Type: Continuous Parameters # This strategy does not have any parameters. Example # use Rubix\\ML\\Strategies\\WildGuess ; $strategy = new WildGuess ();","title":"Wild Guess"},{"location":"strategies/wild-guess.html#wild-guess","text":"Guess a random number somewhere between the minimum and maximum computed by fitting a collection of values. Data Type: Continuous","title":"Wild Guess"},{"location":"strategies/wild-guess.html#parameters","text":"This strategy does not have any parameters.","title":"Parameters"},{"location":"strategies/wild-guess.html#example","text":"use Rubix\\ML\\Strategies\\WildGuess ; $strategy = new WildGuess ();","title":"Example"},{"location":"tokenizers/k-skip-n-gram.html","text":"[source] K-Skip-N-Gram # K-skip-n-grams are a technique similar to n-grams, whereby n-grams are formed but in addition to allowing adjacent sequences of words, the next k words will be skipped forming n-grams of the new forward looking sequences. The tokenizer outputs tokens ranging from min to max number of words per token. Parameters # # Name Default Type Description 1 min 2 int The minimum number of words in a single token. 2 max 2 int The maximum number of words in a single token. 3 skip 2 int The number of words to skip over to form new sequences. Example # use Rubix\\ML\\Tokenizers\\KSkipNGram ; $tokenizer = new KSkipNGram ( 2 , 3 , 2 );","title":"K-Skip-N-Gram"},{"location":"tokenizers/k-skip-n-gram.html#k-skip-n-gram","text":"K-skip-n-grams are a technique similar to n-grams, whereby n-grams are formed but in addition to allowing adjacent sequences of words, the next k words will be skipped forming n-grams of the new forward looking sequences. The tokenizer outputs tokens ranging from min to max number of words per token.","title":"K-Skip-N-Gram"},{"location":"tokenizers/k-skip-n-gram.html#parameters","text":"# Name Default Type Description 1 min 2 int The minimum number of words in a single token. 2 max 2 int The maximum number of words in a single token. 3 skip 2 int The number of words to skip over to form new sequences.","title":"Parameters"},{"location":"tokenizers/k-skip-n-gram.html#example","text":"use Rubix\\ML\\Tokenizers\\KSkipNGram ; $tokenizer = new KSkipNGram ( 2 , 3 , 2 );","title":"Example"},{"location":"tokenizers/n-gram.html","text":"[source] N-gram # N-grams are sequences of n-words of a given string. The N-gram tokenizer outputs tokens of contiguous words ranging from min to max number of words per token. Parameters # # Name Default Type Description 1 min 2 int The minimum number of contiguous words to a token. 2 max 2 int The maximum number of contiguous words to a token. Example # use Rubix\\ML\\Tokenizers\\NGram ; $tokenizer = new NGram ( 1 , 3 );","title":"N-Gram"},{"location":"tokenizers/n-gram.html#n-gram","text":"N-grams are sequences of n-words of a given string. The N-gram tokenizer outputs tokens of contiguous words ranging from min to max number of words per token.","title":"N-gram"},{"location":"tokenizers/n-gram.html#parameters","text":"# Name Default Type Description 1 min 2 int The minimum number of contiguous words to a token. 2 max 2 int The maximum number of contiguous words to a token.","title":"Parameters"},{"location":"tokenizers/n-gram.html#example","text":"use Rubix\\ML\\Tokenizers\\NGram ; $tokenizer = new NGram ( 1 , 3 );","title":"Example"},{"location":"tokenizers/sentence.html","text":"[source] Word Tokenizer # This tokenizer matches sentences starting with a letter and ending with a punctuation mark. Parameters # This tokenizer does not have any parameters. Example # use Rubix\\ML\\Tokenizers\\Sentence ; $tokenizer = new Sentence ();","title":"Sentence"},{"location":"tokenizers/sentence.html#word-tokenizer","text":"This tokenizer matches sentences starting with a letter and ending with a punctuation mark.","title":"Word Tokenizer"},{"location":"tokenizers/sentence.html#parameters","text":"This tokenizer does not have any parameters.","title":"Parameters"},{"location":"tokenizers/sentence.html#example","text":"use Rubix\\ML\\Tokenizers\\Sentence ; $tokenizer = new Sentence ();","title":"Example"},{"location":"tokenizers/whitespace.html","text":"[source] Whitespace # Tokens are delimited by a user-specified whitespace character. Parameters # # Name Default Type Description 1 delimiter ' ' string The whitespace character that delimits each token. Example # use Rubix\\ML\\Tokenizers\\Whitespace ; $tokenizer = new Whitespace ( ',' );","title":"Whitespace"},{"location":"tokenizers/whitespace.html#whitespace","text":"Tokens are delimited by a user-specified whitespace character.","title":"Whitespace"},{"location":"tokenizers/whitespace.html#parameters","text":"# Name Default Type Description 1 delimiter ' ' string The whitespace character that delimits each token.","title":"Parameters"},{"location":"tokenizers/whitespace.html#example","text":"use Rubix\\ML\\Tokenizers\\Whitespace ; $tokenizer = new Whitespace ( ',' );","title":"Example"},{"location":"tokenizers/word-stemmer.html","text":"[source] Word Stemmer # Word Stemmer reduces inflected and derived words to their root form using the Snowball method. For example, the sentence \"Majority voting is likely foolish\" stems to \"Major vote is like foolish.\" Note For a complete list of supported languages you can visit the PHP Stemmer documentation. Parameters # # Name Default Type Description 1 language string The minimum number of contiguous words to a token. Example # use Rubix\\ML\\Tokenizers\\WordStemmer ; $tokenizer = new WordStemmer ( 'english' );","title":"Word Stemmer"},{"location":"tokenizers/word-stemmer.html#word-stemmer","text":"Word Stemmer reduces inflected and derived words to their root form using the Snowball method. For example, the sentence \"Majority voting is likely foolish\" stems to \"Major vote is like foolish.\" Note For a complete list of supported languages you can visit the PHP Stemmer documentation.","title":"Word Stemmer"},{"location":"tokenizers/word-stemmer.html#parameters","text":"# Name Default Type Description 1 language string The minimum number of contiguous words to a token.","title":"Parameters"},{"location":"tokenizers/word-stemmer.html#example","text":"use Rubix\\ML\\Tokenizers\\WordStemmer ; $tokenizer = new WordStemmer ( 'english' );","title":"Example"},{"location":"tokenizers/word.html","text":"[source] Word Tokenizer # The Word tokenizer uses a regular expression to tokenize the words in a blob of text. Parameters # This tokenizer does not have any parameters. Example # use Rubix\\ML\\Tokenizers\\Word ; $tokenizer = new Word ();","title":"Word"},{"location":"tokenizers/word.html#word-tokenizer","text":"The Word tokenizer uses a regular expression to tokenize the words in a blob of text.","title":"Word Tokenizer"},{"location":"tokenizers/word.html#parameters","text":"This tokenizer does not have any parameters.","title":"Parameters"},{"location":"tokenizers/word.html#example","text":"use Rubix\\ML\\Tokenizers\\Word ; $tokenizer = new Word ();","title":"Example"},{"location":"transformers/api.html","text":"Transformer # Transformers take Dataset objects and modify the features contained within. They are often used as part of a transformer Pipeline or they can be used on their own. Transform a Dataset # To transform a dataset, pass a transformer object to the apply() method on a Dataset object like in the example below. use Rubix\\ML\\Transformers\\MinMaxNormalizer ; $dataset -> apply ( new MinMaxNormalizer ()); The transformer can directly transform the samples in place via the transform() method given a samples array: public transform ( array & $samples ) : void $transformer -> transform ( $samples ); Stateful # Stateful transformers are those that require fitting before they can transform. The fit() method takes a dataset as input and pre-computes any necessary information in order to carry out future transformations. You can think of fitting a transformer like training a learner. Fit a Dataset # To fit the transformer to a training set: public fit ( Dataset $dataset ) : void Check if the transformer has been fitted: public fitted () : bool use Rubix\\ML\\Transformers\\OneHotEncoder ; $transformer = new OneHotEncoder (); $transformer -> fit ( $dataset ); var_dump ( $transformer -> fitted ()); bool ( true ) To apply a Stateful transformer to a dataset object, pass the transformer instance to the apply() method like you normally would. The transformer will automatically be fitted with the dataset before transforming the samples. use Rubix\\ML\\Transformers\\OneHotEncoder ; $dataset -> apply ( new OneHotEncoder ()); Elastic # Some transformers are able to adapt to new training data. The update() method provided by the Elastic interface can be used to modify the fitting of the transformer with new data even after being previously fitted. Updating is the transformer equivalent to partially training an Online learner. Update a Fitting # public update ( Dataset $dataset ) : void use Rubix\\ML\\Transformers\\ZScaleStandardizer ; $transformer = new ZScaleStandardizer (); $folds = $dataset -> fold ( 3 ); $transformer -> fit ( $folds [ 0 ]); $transformer -> update ( $folds [ 1 ]); $transformer -> update ( $folds [ 2 ]); Reversible # Transformers that implement the Reversible interface can reverse the transformation applied to a dataset. To apply the reverse transform to a dataset call the reverseApply() method on the dataset object and pass it the reversible transformer. $transformer = new ZScaleStandardizer (); $dataset -> apply ( $transformer ); // Do something $dataset -> reverseApply ( $transformer );","title":"API Reference"},{"location":"transformers/api.html#transformer","text":"Transformers take Dataset objects and modify the features contained within. They are often used as part of a transformer Pipeline or they can be used on their own.","title":"Transformer"},{"location":"transformers/api.html#transform-a-dataset","text":"To transform a dataset, pass a transformer object to the apply() method on a Dataset object like in the example below. use Rubix\\ML\\Transformers\\MinMaxNormalizer ; $dataset -> apply ( new MinMaxNormalizer ()); The transformer can directly transform the samples in place via the transform() method given a samples array: public transform ( array & $samples ) : void $transformer -> transform ( $samples );","title":"Transform a Dataset"},{"location":"transformers/api.html#stateful","text":"Stateful transformers are those that require fitting before they can transform. The fit() method takes a dataset as input and pre-computes any necessary information in order to carry out future transformations. You can think of fitting a transformer like training a learner.","title":"Stateful"},{"location":"transformers/api.html#fit-a-dataset","text":"To fit the transformer to a training set: public fit ( Dataset $dataset ) : void Check if the transformer has been fitted: public fitted () : bool use Rubix\\ML\\Transformers\\OneHotEncoder ; $transformer = new OneHotEncoder (); $transformer -> fit ( $dataset ); var_dump ( $transformer -> fitted ()); bool ( true ) To apply a Stateful transformer to a dataset object, pass the transformer instance to the apply() method like you normally would. The transformer will automatically be fitted with the dataset before transforming the samples. use Rubix\\ML\\Transformers\\OneHotEncoder ; $dataset -> apply ( new OneHotEncoder ());","title":"Fit a Dataset"},{"location":"transformers/api.html#elastic","text":"Some transformers are able to adapt to new training data. The update() method provided by the Elastic interface can be used to modify the fitting of the transformer with new data even after being previously fitted. Updating is the transformer equivalent to partially training an Online learner.","title":"Elastic"},{"location":"transformers/api.html#update-a-fitting","text":"public update ( Dataset $dataset ) : void use Rubix\\ML\\Transformers\\ZScaleStandardizer ; $transformer = new ZScaleStandardizer (); $folds = $dataset -> fold ( 3 ); $transformer -> fit ( $folds [ 0 ]); $transformer -> update ( $folds [ 1 ]); $transformer -> update ( $folds [ 2 ]);","title":"Update a Fitting"},{"location":"transformers/api.html#reversible","text":"Transformers that implement the Reversible interface can reverse the transformation applied to a dataset. To apply the reverse transform to a dataset call the reverseApply() method on the dataset object and pass it the reversible transformer. $transformer = new ZScaleStandardizer (); $dataset -> apply ( $transformer ); // Do something $dataset -> reverseApply ( $transformer );","title":"Reversible"},{"location":"transformers/boolean-converter.html","text":"[source] Boolean Converter # This transformer is used to convert boolean values to a compatible continuous or categorical datatype. Strings should be used when the boolean should be treated as a categorical value. Ints or floats when the boolean should be treated as a continuous value. Interfaces: Transformer Data Type Compatibility: Categorical, Continuous Parameters # # Name Default Type Description 1 trueValue 'true' string, int, float The value to convert true to. 2 falseValue 'false' string, int, float The value to convert false to. Example # use Rubix\\ML\\Transformers\\BooleanConverter ; $transformer = new BooleanConverter ( 'true' , 'false); $transformer = new BooleanConverter(' tall ', ' not tall ' ); $transformer = new BooleanConverter ( 1 , 0 ); Additional Methods # This transformer does not have any additional methods.","title":"Boolean Converter"},{"location":"transformers/boolean-converter.html#boolean-converter","text":"This transformer is used to convert boolean values to a compatible continuous or categorical datatype. Strings should be used when the boolean should be treated as a categorical value. Ints or floats when the boolean should be treated as a continuous value. Interfaces: Transformer Data Type Compatibility: Categorical, Continuous","title":"Boolean Converter"},{"location":"transformers/boolean-converter.html#parameters","text":"# Name Default Type Description 1 trueValue 'true' string, int, float The value to convert true to. 2 falseValue 'false' string, int, float The value to convert false to.","title":"Parameters"},{"location":"transformers/boolean-converter.html#example","text":"use Rubix\\ML\\Transformers\\BooleanConverter ; $transformer = new BooleanConverter ( 'true' , 'false); $transformer = new BooleanConverter(' tall ', ' not tall ' ); $transformer = new BooleanConverter ( 1 , 0 );","title":"Example"},{"location":"transformers/boolean-converter.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/gaussian-random-projector.html","text":"[source] Gaussian Random Projector # Random Projection is a dimensionality reduction technique based on the Johnson-Lindenstrauss lemma. It uses random matrices to project feature vectors onto a target number of dimensions. The Gaussian Random Projector utilizes a random matrix sampled from a smooth Gaussian distribution which projects samples onto a spherically random hyperplane through the origin. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 dimensions int The number of target dimensions to project onto. Example # use Rubix\\ML\\Transformers\\GaussianRandomProjector ; $transformer = new GaussianRandomProjector ( 100 ); Additional Methods # Estimate the minimum dimensionality needed to satisfy a max distortion constraint with n samples using the Johnson-Lindenstrauss lemma: public static minDimensions ( int $n , float $maxDistortion = 0.5 ) : int use Rubix\\ML\\Transformers\\GaussianRandomProjector ; $dimensions = GaussianRandomProjector :: minDimensions ( 5000 , 0.2 );","title":"Gaussian Random Projector"},{"location":"transformers/gaussian-random-projector.html#gaussian-random-projector","text":"Random Projection is a dimensionality reduction technique based on the Johnson-Lindenstrauss lemma. It uses random matrices to project feature vectors onto a target number of dimensions. The Gaussian Random Projector utilizes a random matrix sampled from a smooth Gaussian distribution which projects samples onto a spherically random hyperplane through the origin. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only","title":"Gaussian Random Projector"},{"location":"transformers/gaussian-random-projector.html#parameters","text":"# Name Default Type Description 1 dimensions int The number of target dimensions to project onto.","title":"Parameters"},{"location":"transformers/gaussian-random-projector.html#example","text":"use Rubix\\ML\\Transformers\\GaussianRandomProjector ; $transformer = new GaussianRandomProjector ( 100 );","title":"Example"},{"location":"transformers/gaussian-random-projector.html#additional-methods","text":"Estimate the minimum dimensionality needed to satisfy a max distortion constraint with n samples using the Johnson-Lindenstrauss lemma: public static minDimensions ( int $n , float $maxDistortion = 0.5 ) : int use Rubix\\ML\\Transformers\\GaussianRandomProjector ; $dimensions = GaussianRandomProjector :: minDimensions ( 5000 , 0.2 );","title":"Additional Methods"},{"location":"transformers/hot-deck-imputer.html","text":"[source] Hot Deck Imputer # A hot deck is a set of complete donor samples that may be referenced when imputing a value for a missing feature value. Hot Deck Imputer first finds the k nearest donors to a sample that contains a missing value and then chooses a value at random from the neighborhood. Note Requires a NaN safe distance kernel such as Safe Euclidean for continuous features. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 5 int The number of nearest neighbor donors to consider when imputing a value. 2 weighted false bool Should we use distances as weights when selecting a donor sample? 3 categoricalPlaceholder '?' string The categorical placeholder denoting the category that contains missing values. 4 tree BallTree Spatial The spatial tree used to run nearest neighbor searches. Example # use Rubix\\ML\\Transformers\\HotDeckImputer ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $transformer = new HotDeckImputer ( 20 , false , '?' , new BallTree ( 50 , new SafeEuclidean ())); Additional Methods # This transformer does not have any additional methods. References # C. Hasler et al. (2015). Balanced k-Nearest Neighbor Imputation. \u21a9","title":"Hot Deck Imputer"},{"location":"transformers/hot-deck-imputer.html#hot-deck-imputer","text":"A hot deck is a set of complete donor samples that may be referenced when imputing a value for a missing feature value. Hot Deck Imputer first finds the k nearest donors to a sample that contains a missing value and then chooses a value at random from the neighborhood. Note Requires a NaN safe distance kernel such as Safe Euclidean for continuous features. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Depends on distance kernel","title":"Hot Deck Imputer"},{"location":"transformers/hot-deck-imputer.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbor donors to consider when imputing a value. 2 weighted false bool Should we use distances as weights when selecting a donor sample? 3 categoricalPlaceholder '?' string The categorical placeholder denoting the category that contains missing values. 4 tree BallTree Spatial The spatial tree used to run nearest neighbor searches.","title":"Parameters"},{"location":"transformers/hot-deck-imputer.html#example","text":"use Rubix\\ML\\Transformers\\HotDeckImputer ; use Rubix\\ML\\Graph\\Trees\\BallTree ; use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $transformer = new HotDeckImputer ( 20 , false , '?' , new BallTree ( 50 , new SafeEuclidean ()));","title":"Example"},{"location":"transformers/hot-deck-imputer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/hot-deck-imputer.html#references","text":"C. Hasler et al. (2015). Balanced k-Nearest Neighbor Imputation. \u21a9","title":"References"},{"location":"transformers/image-resizer.html","text":"[source] Image Resizer # Image Resizer fits (scales and crops) images to a user-specified width and height that preserves aspect ratio. Note The GD extension is required to use this transformer. Interfaces: Transformer Data Type Compatibility: Image Parameters # # Name Default Type Description 1 width 32 int The width of the resized image. 2 heights 32 int The height of the resized image. Example # use Rubix\\ML\\Transformers\\ImageResizer ; $transformer = new ImageResizer ( 28 , 28 ); Additional Methods # This transformer does not have any additional methods.","title":"Image Resizer"},{"location":"transformers/image-resizer.html#image-resizer","text":"Image Resizer fits (scales and crops) images to a user-specified width and height that preserves aspect ratio. Note The GD extension is required to use this transformer. Interfaces: Transformer Data Type Compatibility: Image","title":"Image Resizer"},{"location":"transformers/image-resizer.html#parameters","text":"# Name Default Type Description 1 width 32 int The width of the resized image. 2 heights 32 int The height of the resized image.","title":"Parameters"},{"location":"transformers/image-resizer.html#example","text":"use Rubix\\ML\\Transformers\\ImageResizer ; $transformer = new ImageResizer ( 28 , 28 );","title":"Example"},{"location":"transformers/image-resizer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/image-vectorizer.html","text":"[source] Image Vectorizer # Image Vectorizer takes images of the same size and converts them into flat feature vectors of raw color channel intensities. Intensities range from 0 to 255 and can either be read from 1 channel (grayscale) or 3 channels (RGB color) per pixel. Note Note that the GD extension is required to use this transformer. Interfaces: Transformer , Stateful Data Type Compatibility: Image Parameters # # Name Default Type Description 1 grayscale false bool Should we encode the image in grayscale instead of color? Example # use Rubix\\ML\\Transformers\\ImageVectorizer ; $transformer = new ImageVectorizer ( false ); Additional Methods # This transformer does not have any additional methods.","title":"Image Vectorizer"},{"location":"transformers/image-vectorizer.html#image-vectorizer","text":"Image Vectorizer takes images of the same size and converts them into flat feature vectors of raw color channel intensities. Intensities range from 0 to 255 and can either be read from 1 channel (grayscale) or 3 channels (RGB color) per pixel. Note Note that the GD extension is required to use this transformer. Interfaces: Transformer , Stateful Data Type Compatibility: Image","title":"Image Vectorizer"},{"location":"transformers/image-vectorizer.html#parameters","text":"# Name Default Type Description 1 grayscale false bool Should we encode the image in grayscale instead of color?","title":"Parameters"},{"location":"transformers/image-vectorizer.html#example","text":"use Rubix\\ML\\Transformers\\ImageVectorizer ; $transformer = new ImageVectorizer ( false );","title":"Example"},{"location":"transformers/image-vectorizer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/interval-discretizer.html","text":"[source] Interval Discretizer # Assigns continuous features to ordered categories using variable width per-feature histograms with a fixed user-specified number of bins. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 bins 5 int The number of bins per histogram. 2 equiWidth false bool Should the bins be equal width? Example # use Rubix\\ML\\Transformers\\IntervalDiscretizer ; $transformer = new IntervalDiscretizer ( 8 , false ); Additional Methods # Return the bin intervals of the fitted data: public intervals () : array","title":"Interval Discretizer"},{"location":"transformers/interval-discretizer.html#interval-discretizer","text":"Assigns continuous features to ordered categories using variable width per-feature histograms with a fixed user-specified number of bins. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous","title":"Interval Discretizer"},{"location":"transformers/interval-discretizer.html#parameters","text":"# Name Default Type Description 1 bins 5 int The number of bins per histogram. 2 equiWidth false bool Should the bins be equal width?","title":"Parameters"},{"location":"transformers/interval-discretizer.html#example","text":"use Rubix\\ML\\Transformers\\IntervalDiscretizer ; $transformer = new IntervalDiscretizer ( 8 , false );","title":"Example"},{"location":"transformers/interval-discretizer.html#additional-methods","text":"Return the bin intervals of the fitted data: public intervals () : array","title":"Additional Methods"},{"location":"transformers/knn-imputer.html","text":"[source] KNN Imputer # An unsupervised imputer that replaces missing values in a dataset with the distance-weighted average of the samples' k nearest neighbors' values. The average for a continuous feature column is defined as the mean of the values of each donor. Similarly, average is defined as the most frequent value for categorical features. Note Requires a NaN safe distance kernel such as Safe Euclidean for continuous features. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 k 5 int The number of nearest neighbor donors to consider when imputing a value. 2 weighted false bool Should we use distances as weights when selecting a donor sample? 3 categoricalPlaceholder '?' string The categorical placeholder denoting the category that contains missing values. 4 tree BallTree Spatial The spatial tree used to run nearest neighbor searches. Example # use Rubix\\ML\\Transformers\\KNNImputer ; use Rubix\\ML\\Graph\\Trees\\BallTee ; use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $transformer = new KNNImputer ( 10 , false , '?' , new BallTree ( 30 , new SafeEuclidean ())); Additional Methods # This transformer does not have any additional methods. References # O. Troyanskaya et al. (2001). Missing value estimation methods for DNA microarrays. \u21a9","title":"KNN Imputer"},{"location":"transformers/knn-imputer.html#knn-imputer","text":"An unsupervised imputer that replaces missing values in a dataset with the distance-weighted average of the samples' k nearest neighbors' values. The average for a continuous feature column is defined as the mean of the values of each donor. Similarly, average is defined as the most frequent value for categorical features. Note Requires a NaN safe distance kernel such as Safe Euclidean for continuous features. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Depends on distance kernel","title":"KNN Imputer"},{"location":"transformers/knn-imputer.html#parameters","text":"# Name Default Type Description 1 k 5 int The number of nearest neighbor donors to consider when imputing a value. 2 weighted false bool Should we use distances as weights when selecting a donor sample? 3 categoricalPlaceholder '?' string The categorical placeholder denoting the category that contains missing values. 4 tree BallTree Spatial The spatial tree used to run nearest neighbor searches.","title":"Parameters"},{"location":"transformers/knn-imputer.html#example","text":"use Rubix\\ML\\Transformers\\KNNImputer ; use Rubix\\ML\\Graph\\Trees\\BallTee ; use Rubix\\ML\\Kernels\\Distance\\SafeEuclidean ; $transformer = new KNNImputer ( 10 , false , '?' , new BallTree ( 30 , new SafeEuclidean ()));","title":"Example"},{"location":"transformers/knn-imputer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/knn-imputer.html#references","text":"O. Troyanskaya et al. (2001). Missing value estimation methods for DNA microarrays. \u21a9","title":"References"},{"location":"transformers/l1-normalizer.html","text":"[source] L1 Normalizer # Transform each sample (row) vector in the sample matrix such that each feature is divided by the L1 norm (or magnitude ) of that vector. Interfaces: Transformer Data Type Compatibility: Continuous only Parameters # This transformer does not have any parameters. Example # use Rubix\\ML\\Transformers\\L1Normalizer ; $transformer = new L1Normalizer (); Additional Methods # This transformer does not have any additional methods.","title":"L1 Normalizer"},{"location":"transformers/l1-normalizer.html#l1-normalizer","text":"Transform each sample (row) vector in the sample matrix such that each feature is divided by the L1 norm (or magnitude ) of that vector. Interfaces: Transformer Data Type Compatibility: Continuous only","title":"L1 Normalizer"},{"location":"transformers/l1-normalizer.html#parameters","text":"This transformer does not have any parameters.","title":"Parameters"},{"location":"transformers/l1-normalizer.html#example","text":"use Rubix\\ML\\Transformers\\L1Normalizer ; $transformer = new L1Normalizer ();","title":"Example"},{"location":"transformers/l1-normalizer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/l2-normalizer.html","text":"[source] L2 Normalizer # Transform each sample (row) vector in the sample matrix such that each feature is divided by the L2 norm (or magnitude ) of that vector. Interfaces: Transformer Data Type Compatibility: Continuous only Parameters # This transformer does not have any parameters. Example # use Rubix\\ML\\Transformers\\L2Normalizer ; $transformer = new L2Normalizer (); Additional Methods # This transformer does not have any additional methods.","title":"L2 Normalizer"},{"location":"transformers/l2-normalizer.html#l2-normalizer","text":"Transform each sample (row) vector in the sample matrix such that each feature is divided by the L2 norm (or magnitude ) of that vector. Interfaces: Transformer Data Type Compatibility: Continuous only","title":"L2 Normalizer"},{"location":"transformers/l2-normalizer.html#parameters","text":"This transformer does not have any parameters.","title":"Parameters"},{"location":"transformers/l2-normalizer.html#example","text":"use Rubix\\ML\\Transformers\\L2Normalizer ; $transformer = new L2Normalizer ();","title":"Example"},{"location":"transformers/l2-normalizer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/lambda-function.html","text":"[source] Lambda Function # Run a stateless lambda function over the samples in a dataset. The function receives three arguments - the sample to be transformed, its row offset in the dataset, and a user-defined outside context variable that can be used to hold state. Note: If the transformation results in a change in dimensionality, the change must be consistent for each sample. Interfaces: Transformer Compatibility Depends on callback function Parameters # # Param Default Type Description 1 callback callable The function to call over the samples in the dataset. 2 context null mixed The outside context that gets injected into the callback function on each call. Example # use Rubix\\ML\\Transformers\\LambdaFunction ; $callback = function ( & $sample , $offset , $context ) { $sample [] = log1p ( $sample [ 3 ]); }; $transformer = new LambdaFunction ( $callback , 'example context' ); Additional Methods # This transformer does not have any additional methods.","title":"Lambda Function"},{"location":"transformers/lambda-function.html#lambda-function","text":"Run a stateless lambda function over the samples in a dataset. The function receives three arguments - the sample to be transformed, its row offset in the dataset, and a user-defined outside context variable that can be used to hold state. Note: If the transformation results in a change in dimensionality, the change must be consistent for each sample. Interfaces: Transformer Compatibility Depends on callback function","title":"Lambda Function"},{"location":"transformers/lambda-function.html#parameters","text":"# Param Default Type Description 1 callback callable The function to call over the samples in the dataset. 2 context null mixed The outside context that gets injected into the callback function on each call.","title":"Parameters"},{"location":"transformers/lambda-function.html#example","text":"use Rubix\\ML\\Transformers\\LambdaFunction ; $callback = function ( & $sample , $offset , $context ) { $sample [] = log1p ( $sample [ 3 ]); }; $transformer = new LambdaFunction ( $callback , 'example context' );","title":"Example"},{"location":"transformers/lambda-function.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/linear-discriminant-analysis.html","text":"[source] Linear Discriminant Analysis # Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that selects the most informative features using information in the class labels. More formally, LDA finds a linear combination of features that characterizes or best discriminates two or more classes. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 dimensions int The target number of dimensions to project onto. Example # use Rubix\\ML\\Transformers\\LinearDiscriminantAnalysis ; $transformer = new LinearDiscriminantAnalysis ( 20 ); Additional Methods # Return the proportion of information lost due to the transformation: public lossiness () : ? float","title":"Linear Discriminant Analysis"},{"location":"transformers/linear-discriminant-analysis.html#linear-discriminant-analysis","text":"Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction technique that selects the most informative features using information in the class labels. More formally, LDA finds a linear combination of features that characterizes or best discriminates two or more classes. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only","title":"Linear Discriminant Analysis"},{"location":"transformers/linear-discriminant-analysis.html#parameters","text":"# Name Default Type Description 1 dimensions int The target number of dimensions to project onto.","title":"Parameters"},{"location":"transformers/linear-discriminant-analysis.html#example","text":"use Rubix\\ML\\Transformers\\LinearDiscriminantAnalysis ; $transformer = new LinearDiscriminantAnalysis ( 20 );","title":"Example"},{"location":"transformers/linear-discriminant-analysis.html#additional-methods","text":"Return the proportion of information lost due to the transformation: public lossiness () : ? float","title":"Additional Methods"},{"location":"transformers/max-absolute-scaler.html","text":"[source] Max Absolute Scaler # Scale the sample matrix by the maximum absolute value of each feature column independently such that the feature value is between -1 and 1. Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous Parameters # This transformer does not have any parameters. Example # use Rubix\\ML\\Transformers\\MaxAbsoluteScaler ; $transformer = new MaxAbsoluteScaler (); Additional Methods # Return the maximum absolute values for each feature column: public maxabs () : array","title":"Max Absolute Scaler"},{"location":"transformers/max-absolute-scaler.html#max-absolute-scaler","text":"Scale the sample matrix by the maximum absolute value of each feature column independently such that the feature value is between -1 and 1. Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous","title":"Max Absolute Scaler"},{"location":"transformers/max-absolute-scaler.html#parameters","text":"This transformer does not have any parameters.","title":"Parameters"},{"location":"transformers/max-absolute-scaler.html#example","text":"use Rubix\\ML\\Transformers\\MaxAbsoluteScaler ; $transformer = new MaxAbsoluteScaler ();","title":"Example"},{"location":"transformers/max-absolute-scaler.html#additional-methods","text":"Return the maximum absolute values for each feature column: public maxabs () : array","title":"Additional Methods"},{"location":"transformers/min-max-normalizer.html","text":"[source] Min Max Normalizer # The Min Max Normalizer scales the input features to a value between a user-specified range ( default 0 to 1). Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 min 0.0 float The minimum value of the transformed features. 2 max 1.0 float The maximum value of the transformed features. Example # use Rubix\\ML\\Transformers\\MinMaxNormalizer ; $transformer = new MinMaxNormalizer ( - 5.0 , 5.0 ); Additional Methods # Return the minimum values for each fitted feature column: public minimums () : ? array Return the maximum values for each fitted feature column: public maximums () : ? array","title":"Min Max Normalizer"},{"location":"transformers/min-max-normalizer.html#min-max-normalizer","text":"The Min Max Normalizer scales the input features to a value between a user-specified range ( default 0 to 1). Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous","title":"Min Max Normalizer"},{"location":"transformers/min-max-normalizer.html#parameters","text":"# Name Default Type Description 1 min 0.0 float The minimum value of the transformed features. 2 max 1.0 float The maximum value of the transformed features.","title":"Parameters"},{"location":"transformers/min-max-normalizer.html#example","text":"use Rubix\\ML\\Transformers\\MinMaxNormalizer ; $transformer = new MinMaxNormalizer ( - 5.0 , 5.0 );","title":"Example"},{"location":"transformers/min-max-normalizer.html#additional-methods","text":"Return the minimum values for each fitted feature column: public minimums () : ? array Return the maximum values for each fitted feature column: public maximums () : ? array","title":"Additional Methods"},{"location":"transformers/missing-data-imputer.html","text":"[source] Missing Data Imputer # Missing Data Imputer replaces missing continuous (denoted by NaN ) or categorical values (denoted by special placeholder category such as '?' ) with a guess based on user-defined Strategy. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Categorical and Continuous Parameters # # Name Default Type Description 1 continuous Mean Strategy The guessing strategy to employ for continuous feature columns. 2 categorical K Most Frequent Strategy The guessing strategy to employ for categorical feature columns. 3 categoricalPlaceholder '?' string The special placeholder category that denotes missing values. Example # use Rubix\\ML\\Transformers\\MissingDataImputer ; use Rubix\\ML\\Strategies\\Percentile ; use Rubix\\ML\\Strategies\\Prior ; $transformer = new MissingDataImputer ( new Percentile ( 0.55 ), new Prior (), '?' ); Additional Methods # This transformer does not have any additional methods.","title":"Missing Data Imputer"},{"location":"transformers/missing-data-imputer.html#missing-data-imputer","text":"Missing Data Imputer replaces missing continuous (denoted by NaN ) or categorical values (denoted by special placeholder category such as '?' ) with a guess based on user-defined Strategy. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Categorical and Continuous","title":"Missing Data Imputer"},{"location":"transformers/missing-data-imputer.html#parameters","text":"# Name Default Type Description 1 continuous Mean Strategy The guessing strategy to employ for continuous feature columns. 2 categorical K Most Frequent Strategy The guessing strategy to employ for categorical feature columns. 3 categoricalPlaceholder '?' string The special placeholder category that denotes missing values.","title":"Parameters"},{"location":"transformers/missing-data-imputer.html#example","text":"use Rubix\\ML\\Transformers\\MissingDataImputer ; use Rubix\\ML\\Strategies\\Percentile ; use Rubix\\ML\\Strategies\\Prior ; $transformer = new MissingDataImputer ( new Percentile ( 0.55 ), new Prior (), '?' );","title":"Example"},{"location":"transformers/missing-data-imputer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/multibyte-text-normalizer.html","text":"[source] Multibyte Text Normalizer # This transformer converts the characters in all multibyte strings to the same case. Multibyte strings contain characters such as accents (\u00e9, \u00e8, \u00e0), emojis (\ud83d\ude00, \ud83d\ude09) or characters of non roman alphabets such as Chinese and Cyrillic. Note \u26a0\ufe0f We recommend you install the mbstring extension for best performance. Interfaces: Transformer Data Type Compatibility: Categorical Parameters # # Name Default Type Description 1 uppercase false bool Should the text be converted to uppercase? Example # use Rubix\\ML\\Transformers\\MultibyteTextNormalizer ; $transformer = new MultibyteTextNormalizer ( false ); Additional Methods # This transformer does not have any additional methods.","title":"Multibyte Text Normalizer"},{"location":"transformers/multibyte-text-normalizer.html#multibyte-text-normalizer","text":"This transformer converts the characters in all multibyte strings to the same case. Multibyte strings contain characters such as accents (\u00e9, \u00e8, \u00e0), emojis (\ud83d\ude00, \ud83d\ude09) or characters of non roman alphabets such as Chinese and Cyrillic. Note \u26a0\ufe0f We recommend you install the mbstring extension for best performance. Interfaces: Transformer Data Type Compatibility: Categorical","title":"Multibyte Text Normalizer"},{"location":"transformers/multibyte-text-normalizer.html#parameters","text":"# Name Default Type Description 1 uppercase false bool Should the text be converted to uppercase?","title":"Parameters"},{"location":"transformers/multibyte-text-normalizer.html#example","text":"use Rubix\\ML\\Transformers\\MultibyteTextNormalizer ; $transformer = new MultibyteTextNormalizer ( false );","title":"Example"},{"location":"transformers/multibyte-text-normalizer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/numeric-string-converter.html","text":"[source] Numeric String Converter # Convert all numeric strings to their equivalent integer and floating point types. Useful for when extracting from a source that only recognizes data as string types such as CSV. Note The string representations of the PHP constants NAN and INF are the string literals 'NAN' and 'INF' respectively. Interfaces: Transformer , Reversible Data Type Compatibility: Categorical Parameters # This transformer does not have any parameters. Example # use Rubix\\ML\\Transformers\\NumericStringConverter ; $transformer = new NumericStringConverter (); Additional Methods # This transformer does not have any additional methods.","title":"Numeric String Converter"},{"location":"transformers/numeric-string-converter.html#numeric-string-converter","text":"Convert all numeric strings to their equivalent integer and floating point types. Useful for when extracting from a source that only recognizes data as string types such as CSV. Note The string representations of the PHP constants NAN and INF are the string literals 'NAN' and 'INF' respectively. Interfaces: Transformer , Reversible Data Type Compatibility: Categorical","title":"Numeric String Converter"},{"location":"transformers/numeric-string-converter.html#parameters","text":"This transformer does not have any parameters.","title":"Parameters"},{"location":"transformers/numeric-string-converter.html#example","text":"use Rubix\\ML\\Transformers\\NumericStringConverter ; $transformer = new NumericStringConverter ();","title":"Example"},{"location":"transformers/numeric-string-converter.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/one-hot-encoder.html","text":"[source] One Hot Encoder # The One Hot Encoder takes a categorical feature column and produces an n-dimensional continuous representation where n is equal to the number of unique categories present in that column. A 0 in any location indicates that the category represented by that column is not present in the sample, whereas a 1 indicates that a category is present. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Categorical Parameters # This transformer does not have any parameters. Example # use Rubix\\ML\\Transformers\\OneHotEncoder ; $transformer = new OneHotEncoder (); Additional Methods # Return the categories computed during fitting indexed by feature column: public categories () : ? array","title":"One Hot Encoder"},{"location":"transformers/one-hot-encoder.html#one-hot-encoder","text":"The One Hot Encoder takes a categorical feature column and produces an n-dimensional continuous representation where n is equal to the number of unique categories present in that column. A 0 in any location indicates that the category represented by that column is not present in the sample, whereas a 1 indicates that a category is present. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Categorical","title":"One Hot Encoder"},{"location":"transformers/one-hot-encoder.html#parameters","text":"This transformer does not have any parameters.","title":"Parameters"},{"location":"transformers/one-hot-encoder.html#example","text":"use Rubix\\ML\\Transformers\\OneHotEncoder ; $transformer = new OneHotEncoder ();","title":"Example"},{"location":"transformers/one-hot-encoder.html#additional-methods","text":"Return the categories computed during fitting indexed by feature column: public categories () : ? array","title":"Additional Methods"},{"location":"transformers/polynomial-expander.html","text":"[source] Polynomial Expander # This transformer will generate polynomials up to and including the specified degree of each continuous feature. Polynomial expansion is sometimes used to fit data that is non-linear using a linear estimator such as Ridge , Logistic Regression , or Softmax Classifier . Interfaces: Transformer Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 degree 2 int The degree of the polynomials to generate for each feature. Example # use Rubix\\ML\\Transformers\\PolynomialExpander ; $transformer = new PolynomialExpander ( 3 ); Additional Methods # This transformer does not have any additional methods.","title":"Polynomial Expander"},{"location":"transformers/polynomial-expander.html#polynomial-expander","text":"This transformer will generate polynomials up to and including the specified degree of each continuous feature. Polynomial expansion is sometimes used to fit data that is non-linear using a linear estimator such as Ridge , Logistic Regression , or Softmax Classifier . Interfaces: Transformer Data Type Compatibility: Continuous only","title":"Polynomial Expander"},{"location":"transformers/polynomial-expander.html#parameters","text":"# Name Default Type Description 1 degree 2 int The degree of the polynomials to generate for each feature.","title":"Parameters"},{"location":"transformers/polynomial-expander.html#example","text":"use Rubix\\ML\\Transformers\\PolynomialExpander ; $transformer = new PolynomialExpander ( 3 );","title":"Example"},{"location":"transformers/polynomial-expander.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/principal-component-analysis.html","text":"[source] Principal Component Analysis # Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to transform the feature space by the k principal components that explain the most variance. PCA is used to compress high-dimensional samples down to lower dimensions such that they would retain as much information as possible. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 dimensions int The target number of dimensions to project onto. Example # use Rubix\\ML\\Transformers\\PrincipalComponentAnalysis ; $transformer = new PrincipalComponentAnalysis ( 15 ); Additional Methods # Return the proportion of information lost due to the transformation: public lossiness () : ? float References # H. Abdi et al. (2010). Principal Component Analysis. \u21a9","title":"Principal Component Analysis"},{"location":"transformers/principal-component-analysis.html#principal-component-analysis","text":"Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to transform the feature space by the k principal components that explain the most variance. PCA is used to compress high-dimensional samples down to lower dimensions such that they would retain as much information as possible. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only","title":"Principal Component Analysis"},{"location":"transformers/principal-component-analysis.html#parameters","text":"# Name Default Type Description 1 dimensions int The target number of dimensions to project onto.","title":"Parameters"},{"location":"transformers/principal-component-analysis.html#example","text":"use Rubix\\ML\\Transformers\\PrincipalComponentAnalysis ; $transformer = new PrincipalComponentAnalysis ( 15 );","title":"Example"},{"location":"transformers/principal-component-analysis.html#additional-methods","text":"Return the proportion of information lost due to the transformation: public lossiness () : ? float","title":"Additional Methods"},{"location":"transformers/principal-component-analysis.html#references","text":"H. Abdi et al. (2010). Principal Component Analysis. \u21a9","title":"References"},{"location":"transformers/regex-filter.html","text":"[source] Regex Filter # Filters the text features of a dataset by matching and removing patterns from a list of regular expressions. Note Patterns are filtered in the same sequence as they are given in the constructor. Interfaces: Transformer Data Type Compatibility: Categorical Parameters # # Name Default Type Description 1 patterns array A list of regular expression patterns used to filter the text columns of the dataset. Example # use Rubix\\ML\\Transformers\\RegexFilter ; $transformer = new RegexFilter ([ RegexFilter :: URL , RegexFilter :: MENTION , '/(?<me>.+)/' , RegexFilter :: EXTRA_CHARACTERS , ]); Predefined Regex Patterns # Class Constant Description EMAIL A pattern to match any email address. URL An alias for the default URL matching pattern. GRUBER_1 The original Gruber URL matching pattern. GRUBER_2 The improved Gruber URL matching pattern. EXTRA_CHARACTERS Matches consecutively repeated non word or number characters such as punctuation and special characters. EXTRA_WORDS Matches consecutively repeated words. EXTRA_WHITESPACE Matches consecutively repeated whitespace characters. MENTION A pattern that matches Twitter-style mentions (@example). HASHTAG Matches Twitter-style hashtags (#example). Additional Methods # This transformer does not have any additional methods. References: # J. Gruber. (2009). A Liberal, Accurate Regex Pattern for Matching URLs. \u21a9 J. Gruber. (2010). An Improved Liberal, Accurate Regex Pattern for Matching URLs. \u21a9","title":"Regex Filter"},{"location":"transformers/regex-filter.html#regex-filter","text":"Filters the text features of a dataset by matching and removing patterns from a list of regular expressions. Note Patterns are filtered in the same sequence as they are given in the constructor. Interfaces: Transformer Data Type Compatibility: Categorical","title":"Regex Filter"},{"location":"transformers/regex-filter.html#parameters","text":"# Name Default Type Description 1 patterns array A list of regular expression patterns used to filter the text columns of the dataset.","title":"Parameters"},{"location":"transformers/regex-filter.html#example","text":"use Rubix\\ML\\Transformers\\RegexFilter ; $transformer = new RegexFilter ([ RegexFilter :: URL , RegexFilter :: MENTION , '/(?<me>.+)/' , RegexFilter :: EXTRA_CHARACTERS , ]);","title":"Example"},{"location":"transformers/regex-filter.html#predefined-regex-patterns","text":"Class Constant Description EMAIL A pattern to match any email address. URL An alias for the default URL matching pattern. GRUBER_1 The original Gruber URL matching pattern. GRUBER_2 The improved Gruber URL matching pattern. EXTRA_CHARACTERS Matches consecutively repeated non word or number characters such as punctuation and special characters. EXTRA_WORDS Matches consecutively repeated words. EXTRA_WHITESPACE Matches consecutively repeated whitespace characters. MENTION A pattern that matches Twitter-style mentions (@example). HASHTAG Matches Twitter-style hashtags (#example).","title":"Predefined Regex Patterns"},{"location":"transformers/regex-filter.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/regex-filter.html#references","text":"J. Gruber. (2009). A Liberal, Accurate Regex Pattern for Matching URLs. \u21a9 J. Gruber. (2010). An Improved Liberal, Accurate Regex Pattern for Matching URLs. \u21a9","title":"References:"},{"location":"transformers/robust-standardizer.html","text":"[source] Robust Standardizer # This standardizer transforms continuous features by centering them around the median and scaling by the median absolute deviation (MAD) referred to as a robust or modified Z-Score. The use of robust statistics make this standardizer more immune to outliers than Z Scale Standardizer . \\[ {\\displaystyle z^\\prime = {x - \\operatorname {median}(X) \\over MAD }} \\] Interfaces: Transformer , Stateful , Reversible , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 center true bool Should we center the data at 0? Example # use Rubix\\ML\\Transformers\\RobustStandardizer ; $transformer = new RobustStandardizer ( true ); Additional Methods # Return the medians calculated by fitting the training set: public medians () : array Return the median absolute deviations calculated during fitting: public mads () : array","title":"Robust Standardizer"},{"location":"transformers/robust-standardizer.html#robust-standardizer","text":"This standardizer transforms continuous features by centering them around the median and scaling by the median absolute deviation (MAD) referred to as a robust or modified Z-Score. The use of robust statistics make this standardizer more immune to outliers than Z Scale Standardizer . \\[ {\\displaystyle z^\\prime = {x - \\operatorname {median}(X) \\over MAD }} \\] Interfaces: Transformer , Stateful , Reversible , Persistable Data Type Compatibility: Continuous","title":"Robust Standardizer"},{"location":"transformers/robust-standardizer.html#parameters","text":"# Name Default Type Description 1 center true bool Should we center the data at 0?","title":"Parameters"},{"location":"transformers/robust-standardizer.html#example","text":"use Rubix\\ML\\Transformers\\RobustStandardizer ; $transformer = new RobustStandardizer ( true );","title":"Example"},{"location":"transformers/robust-standardizer.html#additional-methods","text":"Return the medians calculated by fitting the training set: public medians () : array Return the median absolute deviations calculated during fitting: public mads () : array","title":"Additional Methods"},{"location":"transformers/sparse-random-projector.html","text":"[source] Sparse Random Projector # A database-friendly random projector that samples its random projection matrix from a sparse probabilistic approximation of the Gaussian distribution. The term database-friendly refers to the fact that the number of non-zero operations required to transform the input matrix is reduced by the sparsity factor. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 dimensions int The number of target dimensions to project onto. 2 sparsity 0.66 float The proportion of zero to non-zero elements in the random projection matrix. If null, sparsity factor will be chosen automatically. Example # use Rubix\\ML\\Transformers\\SparseRandomProjector ; $transformer = new SparseRandomProjector ( 30 , null ); Additional Methods # Estimate the minimum dimensionality needed to satisfy a max distortion constraint with n samples using the Johnson-Lindenstrauss lemma: public static minDimensions ( int $n , float $maxDistortion = 0.5 ) : int use Rubix\\ML\\Transformers\\SparseRandomProjector ; $dimensions = SparseRandomProjector :: minDimensions ( 10000 , 0.5 ); References # D. Achlioptas. (2003). Database-friendly random projections: Johnson-Lindenstrauss with binary coins. \u21a9 P. Li at al. (2006). Very Sparse Random Projections. \u21a9","title":"Sparse Random Projector"},{"location":"transformers/sparse-random-projector.html#sparse-random-projector","text":"A database-friendly random projector that samples its random projection matrix from a sparse probabilistic approximation of the Gaussian distribution. The term database-friendly refers to the fact that the number of non-zero operations required to transform the input matrix is reduced by the sparsity factor. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only","title":"Sparse Random Projector"},{"location":"transformers/sparse-random-projector.html#parameters","text":"# Name Default Type Description 1 dimensions int The number of target dimensions to project onto. 2 sparsity 0.66 float The proportion of zero to non-zero elements in the random projection matrix. If null, sparsity factor will be chosen automatically.","title":"Parameters"},{"location":"transformers/sparse-random-projector.html#example","text":"use Rubix\\ML\\Transformers\\SparseRandomProjector ; $transformer = new SparseRandomProjector ( 30 , null );","title":"Example"},{"location":"transformers/sparse-random-projector.html#additional-methods","text":"Estimate the minimum dimensionality needed to satisfy a max distortion constraint with n samples using the Johnson-Lindenstrauss lemma: public static minDimensions ( int $n , float $maxDistortion = 0.5 ) : int use Rubix\\ML\\Transformers\\SparseRandomProjector ; $dimensions = SparseRandomProjector :: minDimensions ( 10000 , 0.5 );","title":"Additional Methods"},{"location":"transformers/sparse-random-projector.html#references","text":"D. Achlioptas. (2003). Database-friendly random projections: Johnson-Lindenstrauss with binary coins. \u21a9 P. Li at al. (2006). Very Sparse Random Projections. \u21a9","title":"References"},{"location":"transformers/stop-word-filter.html","text":"[source] Stop Word Filter # Removes user-specified words from any categorical feature columns including blobs of text. Interfaces: Transformer Data Type Compatibility: Categorical Parameters # # Name Default Type Description 1 stopWords array A list of stop words to filter out of each text feature. Example # use Rubix\\ML\\Transformers\\StopWordFilter ; $transformer = new StopWordFilter ([ 'i' , 'me' , 'my' , ... ]); Additional Methods # This transformer does not have any additional methods.","title":"Stop Word Filter"},{"location":"transformers/stop-word-filter.html#stop-word-filter","text":"Removes user-specified words from any categorical feature columns including blobs of text. Interfaces: Transformer Data Type Compatibility: Categorical","title":"Stop Word Filter"},{"location":"transformers/stop-word-filter.html#parameters","text":"# Name Default Type Description 1 stopWords array A list of stop words to filter out of each text feature.","title":"Parameters"},{"location":"transformers/stop-word-filter.html#example","text":"use Rubix\\ML\\Transformers\\StopWordFilter ; $transformer = new StopWordFilter ([ 'i' , 'me' , 'my' , ... ]);","title":"Example"},{"location":"transformers/stop-word-filter.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/t-sne.html","text":"[source] t-SNE # T-distributed Stochastic Neighbor Embedding is a two-stage non-linear manifold learning algorithm based on Batch Gradient Descent that seeks to maintain the distances between samples in low-dimensional space. During the first stage ( early stage ) the distances are exaggerated to encourage more pronounced clusters. Since the t-SNE cost function (KL Divergence) has a rough gradient, momentum is employed to help escape bad local minima. Note T-SNE is implemented using the exact method which scales quadratically in the number of samples. Therefore, it is recommended to subsample datasets larger than a few thousand samples. Interfaces: Transformer , Verbose Data Type Compatibility: Depends on distance kernel Parameters # # Name Default Type Description 1 dimensions 2 int The number of dimensions of the target embedding. 2 rate 100.0 float The learning rate that controls the global step size. 3 perplexity 30 int The number of effective nearest neighbors to refer to when computing the variance of the distribution over that sample. 4 exaggeration 12.0 float The factor to exaggerate the distances between samples during the early stage of embedding. 5 epochs 1000 int The maximum number of times to iterate over the embedding. 6 minGradient 1e-7 float The minimum norm of the gradient necessary to continue embedding. 7 window 10 int The number of epochs without improvement in the training loss to wait before considering an early stop. 8 kernel Euclidean Distance The distance kernel to use when measuring distances between samples. Example # use Rubix\\ML\\Transformers\\TSNE ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $transformer = new TSNE ( 3 , 10.0 , 30 , 12.0 , 500 , 1e-6 , 10 , new Manhattan ()); Additional Methods # Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $transformer -> steps ()); Return the magnitudes of the gradient at each epoch from the last embedding: public losses () : float [] | null References # L. van der Maaten et al. (2008). Visualizing Data using t-SNE. \u21a9 L. van der Maaten. (2009). Learning a Parametric Embedding by Preserving Local Structure. \u21a9","title":"t-SNE"},{"location":"transformers/t-sne.html#t-sne","text":"T-distributed Stochastic Neighbor Embedding is a two-stage non-linear manifold learning algorithm based on Batch Gradient Descent that seeks to maintain the distances between samples in low-dimensional space. During the first stage ( early stage ) the distances are exaggerated to encourage more pronounced clusters. Since the t-SNE cost function (KL Divergence) has a rough gradient, momentum is employed to help escape bad local minima. Note T-SNE is implemented using the exact method which scales quadratically in the number of samples. Therefore, it is recommended to subsample datasets larger than a few thousand samples. Interfaces: Transformer , Verbose Data Type Compatibility: Depends on distance kernel","title":"t-SNE"},{"location":"transformers/t-sne.html#parameters","text":"# Name Default Type Description 1 dimensions 2 int The number of dimensions of the target embedding. 2 rate 100.0 float The learning rate that controls the global step size. 3 perplexity 30 int The number of effective nearest neighbors to refer to when computing the variance of the distribution over that sample. 4 exaggeration 12.0 float The factor to exaggerate the distances between samples during the early stage of embedding. 5 epochs 1000 int The maximum number of times to iterate over the embedding. 6 minGradient 1e-7 float The minimum norm of the gradient necessary to continue embedding. 7 window 10 int The number of epochs without improvement in the training loss to wait before considering an early stop. 8 kernel Euclidean Distance The distance kernel to use when measuring distances between samples.","title":"Parameters"},{"location":"transformers/t-sne.html#example","text":"use Rubix\\ML\\Transformers\\TSNE ; use Rubix\\ML\\Kernels\\Distance\\Manhattan ; $transformer = new TSNE ( 3 , 10.0 , 30 , 12.0 , 500 , 1e-6 , 10 , new Manhattan ());","title":"Example"},{"location":"transformers/t-sne.html#additional-methods","text":"Return an iterable progress table with the steps from the last training session: public steps () : iterable use Rubix\\ML\\Extractors\\CSV ; $extractor = new CSV ( 'progress.csv' , true ); $extractor -> export ( $transformer -> steps ()); Return the magnitudes of the gradient at each epoch from the last embedding: public losses () : float [] | null","title":"Additional Methods"},{"location":"transformers/t-sne.html#references","text":"L. van der Maaten et al. (2008). Visualizing Data using t-SNE. \u21a9 L. van der Maaten. (2009). Learning a Parametric Embedding by Preserving Local Structure. \u21a9","title":"References"},{"location":"transformers/text-normalizer.html","text":"[source] Text Normalizer # Converts all the characters in a blob of text to the same case. Interfaces: Transformer Data Type Compatibility: Categorical Note This transformer does not handle multibyte strings. For multibyte support, see MultibyteTextNormalizer . Parameters # # Name Default Type Description 1 uppercase false bool Should the text be converted to uppercase? Example # use Rubix\\ML\\Transformers\\TextNormalizer ; $transformer = new TextNormalizer ( false ); Additional Methods # This transformer does not have any additional methods.","title":"Text Normalizer"},{"location":"transformers/text-normalizer.html#text-normalizer","text":"Converts all the characters in a blob of text to the same case. Interfaces: Transformer Data Type Compatibility: Categorical Note This transformer does not handle multibyte strings. For multibyte support, see MultibyteTextNormalizer .","title":"Text Normalizer"},{"location":"transformers/text-normalizer.html#parameters","text":"# Name Default Type Description 1 uppercase false bool Should the text be converted to uppercase?","title":"Parameters"},{"location":"transformers/text-normalizer.html#example","text":"use Rubix\\ML\\Transformers\\TextNormalizer ; $transformer = new TextNormalizer ( false );","title":"Example"},{"location":"transformers/text-normalizer.html#additional-methods","text":"This transformer does not have any additional methods.","title":"Additional Methods"},{"location":"transformers/tf-idf-transformer.html","text":"[source] TF-IDF Transformer # Term Frequency - Inverse Document Frequency is a measurement of how important a word is to a document. The TF-IDF value increases with the number of times a word appears in a document ( TF ) and is offset by the frequency of the word in the corpus ( IDF ). Note TF-IDF Transformer assumes that its inputs are token frequency vectors such as those created by Word Count Vectorizer . Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 smoothing 1.0 float The amount of additive (Laplace) smoothing to add to the IDFs. 2 dampening false bool Should we apply a sub-linear function to dampen the effect of recurring tokens? Example # use Rubix\\ML\\Transformers\\TfIdfTransformer ; $transformer = new TfIdfTransformer ( 2.0 , true ); Additional Methods # Return the document frequencies calculated during fitting: public dfs () : ? array References # S. Robertson. (2003). Understanding Inverse Document Frequency: On theoretical arguments for IDF. \u21a9 C. D. Manning et al. (2009). An Introduction to Information Retrieval. \u21a9","title":"TF-IDF Transformer"},{"location":"transformers/tf-idf-transformer.html#tf-idf-transformer","text":"Term Frequency - Inverse Document Frequency is a measurement of how important a word is to a document. The TF-IDF value increases with the number of times a word appears in a document ( TF ) and is offset by the frequency of the word in the corpus ( IDF ). Note TF-IDF Transformer assumes that its inputs are token frequency vectors such as those created by Word Count Vectorizer . Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous only","title":"TF-IDF Transformer"},{"location":"transformers/tf-idf-transformer.html#parameters","text":"# Name Default Type Description 1 smoothing 1.0 float The amount of additive (Laplace) smoothing to add to the IDFs. 2 dampening false bool Should we apply a sub-linear function to dampen the effect of recurring tokens?","title":"Parameters"},{"location":"transformers/tf-idf-transformer.html#example","text":"use Rubix\\ML\\Transformers\\TfIdfTransformer ; $transformer = new TfIdfTransformer ( 2.0 , true );","title":"Example"},{"location":"transformers/tf-idf-transformer.html#additional-methods","text":"Return the document frequencies calculated during fitting: public dfs () : ? array","title":"Additional Methods"},{"location":"transformers/tf-idf-transformer.html#references","text":"S. Robertson. (2003). Understanding Inverse Document Frequency: On theoretical arguments for IDF. \u21a9 C. D. Manning et al. (2009). An Introduction to Information Retrieval. \u21a9","title":"References"},{"location":"transformers/token-hashing-vectorizer.html","text":"[source] Token Hashing Vectorizer # Token Hashing Vectorizer builds token count vectors on the fly by employing a hashing trick . It is a stateless transformer that uses a hashing algorithm to assign token occurrences to a bucket in a vector of user-specified dimensionality. The advantage of hashing over storing a fixed vocabulary is that there is no memory footprint however there is a chance that certain tokens will collide with other tokens especially in lower-dimensional vector spaces. Note The default hashing function is CRC32 and is a good mix between speed and output space utilization. MurmurHash has even greater utilization but at the cost of some speed and it is only available on PHP 8.1 and above. FNV1 is comparable to CRC32 but with slightly more overhead. Interfaces: Transformer Data Type Compatibility: Categorical Parameters # # Param Default Type Description 1 dimensions int The dimensionality of the vector space. 2 tokenizer Word Tokenizer The tokenizer used to extract tokens from blobs of text. 3 hashFn callable 'crc32' The hash function that accepts a string token and returns an integer. Example # use Rubix\\ML\\Transformers\\TokenHashingVectorizer ; use Rubix\\ML\\Tokenizers\\Word (); $transformer = new TokenHashingVectorizer ( 10000 , new Word (), TokenHashingVectorizer :: MURMUR3 ); Additional Constants # The CRC32 callback function. public const CRC32 callable ( string ) : int The MurmurHash3 callback function. public const MURMUR3 callable ( string ) : int The FNV1 callback function. public const FNV1 callable ( string ) : int Additional Methods # The MurmurHash3 hashing function: public static murmur3 ( string $input ) : int Note MurmurHash3 is only available on PHP 8.1 or above. The FNV1a 32-bit hashing function: public static fnv1a32 ( string $input ) : int","title":"Token Hashing Vectorizer"},{"location":"transformers/token-hashing-vectorizer.html#token-hashing-vectorizer","text":"Token Hashing Vectorizer builds token count vectors on the fly by employing a hashing trick . It is a stateless transformer that uses a hashing algorithm to assign token occurrences to a bucket in a vector of user-specified dimensionality. The advantage of hashing over storing a fixed vocabulary is that there is no memory footprint however there is a chance that certain tokens will collide with other tokens especially in lower-dimensional vector spaces. Note The default hashing function is CRC32 and is a good mix between speed and output space utilization. MurmurHash has even greater utilization but at the cost of some speed and it is only available on PHP 8.1 and above. FNV1 is comparable to CRC32 but with slightly more overhead. Interfaces: Transformer Data Type Compatibility: Categorical","title":"Token Hashing Vectorizer"},{"location":"transformers/token-hashing-vectorizer.html#parameters","text":"# Param Default Type Description 1 dimensions int The dimensionality of the vector space. 2 tokenizer Word Tokenizer The tokenizer used to extract tokens from blobs of text. 3 hashFn callable 'crc32' The hash function that accepts a string token and returns an integer.","title":"Parameters"},{"location":"transformers/token-hashing-vectorizer.html#example","text":"use Rubix\\ML\\Transformers\\TokenHashingVectorizer ; use Rubix\\ML\\Tokenizers\\Word (); $transformer = new TokenHashingVectorizer ( 10000 , new Word (), TokenHashingVectorizer :: MURMUR3 );","title":"Example"},{"location":"transformers/token-hashing-vectorizer.html#additional-constants","text":"The CRC32 callback function. public const CRC32 callable ( string ) : int The MurmurHash3 callback function. public const MURMUR3 callable ( string ) : int The FNV1 callback function. public const FNV1 callable ( string ) : int","title":"Additional Constants"},{"location":"transformers/token-hashing-vectorizer.html#additional-methods","text":"The MurmurHash3 hashing function: public static murmur3 ( string $input ) : int Note MurmurHash3 is only available on PHP 8.1 or above. The FNV1a 32-bit hashing function: public static fnv1a32 ( string $input ) : int","title":"Additional Methods"},{"location":"transformers/truncated-svd.html","text":"[source] Truncated SVD # Truncated Singular Value Decomposition (SVD) is a matrix factorization and dimensionality reduction technique that generalizes eigendecomposition to general matrices. When applied to datasets of document term frequency vectors, the technique is called Latent Semantic Analysis (LSA) and computes a statistical model of relationships between words. Note Note that the Tensor extension is required to use this transformer. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only Parameters # # Name Default Type Description 1 dimensions int The target number of dimensions to project onto. Example # use Rubix\\ML\\Transformers\\TruncatedSVD ; $transformer = new TruncatedSVD ( 100 ); Additional Methods # Return the proportion of information lost due to the transformation: public lossiness () : ? float References # S. Deerwater et al. (1990). Indexing by Latent Semantic Analysis. \u21a9","title":"Truncated SVD"},{"location":"transformers/truncated-svd.html#truncated-svd","text":"Truncated Singular Value Decomposition (SVD) is a matrix factorization and dimensionality reduction technique that generalizes eigendecomposition to general matrices. When applied to datasets of document term frequency vectors, the technique is called Latent Semantic Analysis (LSA) and computes a statistical model of relationships between words. Note Note that the Tensor extension is required to use this transformer. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Continuous only","title":"Truncated SVD"},{"location":"transformers/truncated-svd.html#parameters","text":"# Name Default Type Description 1 dimensions int The target number of dimensions to project onto.","title":"Parameters"},{"location":"transformers/truncated-svd.html#example","text":"use Rubix\\ML\\Transformers\\TruncatedSVD ; $transformer = new TruncatedSVD ( 100 );","title":"Example"},{"location":"transformers/truncated-svd.html#additional-methods","text":"Return the proportion of information lost due to the transformation: public lossiness () : ? float","title":"Additional Methods"},{"location":"transformers/truncated-svd.html#references","text":"S. Deerwater et al. (1990). Indexing by Latent Semantic Analysis. \u21a9","title":"References"},{"location":"transformers/word-count-vectorizer.html","text":"[source] Word Count Vectorizer # The Word Count Vectorizer builds a vocabulary from the training samples and transforms text blobs into fixed length sparse feature vectors. Each feature column represents a word or token from the vocabulary and the value denotes the number of times that word appears in a given document. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Categorical Parameters # # Name Default Type Description 1 maxVocabularySize PHP_INT_MAX int The maximum number of unique tokens to embed into each document vector. 2 minDocumentCount 1 float The minimum number of documents a word must appear in to be added to the vocabulary. 3 maxDocumentRatio 0.8 float The maximum ratio of documents a word can appear in to be added to the vocabulary. 4 tokenizer Word Tokenizer The tokenizer used to extract features from blobs of text. Example # use Rubix\\ML\\Transformers\\WordCountVectorizer ; use Rubix\\ML\\Tokenizers\\NGram ; $transformer = new WordCountVectorizer ( 10000 , 5 , 0.5 , new NGram ( 1 , 2 )); Additional Methods # Return an array of words that comprise each of the vocabularies: public vocabularies () : array","title":"Word Count Vectorizer"},{"location":"transformers/word-count-vectorizer.html#word-count-vectorizer","text":"The Word Count Vectorizer builds a vocabulary from the training samples and transforms text blobs into fixed length sparse feature vectors. Each feature column represents a word or token from the vocabulary and the value denotes the number of times that word appears in a given document. Interfaces: Transformer , Stateful , Persistable Data Type Compatibility: Categorical","title":"Word Count Vectorizer"},{"location":"transformers/word-count-vectorizer.html#parameters","text":"# Name Default Type Description 1 maxVocabularySize PHP_INT_MAX int The maximum number of unique tokens to embed into each document vector. 2 minDocumentCount 1 float The minimum number of documents a word must appear in to be added to the vocabulary. 3 maxDocumentRatio 0.8 float The maximum ratio of documents a word can appear in to be added to the vocabulary. 4 tokenizer Word Tokenizer The tokenizer used to extract features from blobs of text.","title":"Parameters"},{"location":"transformers/word-count-vectorizer.html#example","text":"use Rubix\\ML\\Transformers\\WordCountVectorizer ; use Rubix\\ML\\Tokenizers\\NGram ; $transformer = new WordCountVectorizer ( 10000 , 5 , 0.5 , new NGram ( 1 , 2 ));","title":"Example"},{"location":"transformers/word-count-vectorizer.html#additional-methods","text":"Return an array of words that comprise each of the vocabularies: public vocabularies () : array","title":"Additional Methods"},{"location":"transformers/z-scale-standardizer.html","text":"[source] Z Scale Standardizer # A method of centering and scaling a dataset such that it has 0 mean and unit variance, also known as a Z-Score. Although Z-Scores are technically unbounded, in practice they mostly fall between -3 and 3 - that is, they are no more than 3 standard deviations away from the mean. \\[ {\\displaystyle z = {x - \\mu \\over \\sigma }} \\] Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous Parameters # # Name Default Type Description 1 center true bool Should we center the data at 0? Example # use Rubix\\ML\\Transformers\\ZScaleStandardizer ; $transformer = new ZScaleStandardizer ( true ); Additional Methods # Return the means calculated by fitting the training set: public means () : array Return the variances calculated during fitting: public variances () : array References # T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances. \u21a9","title":"Z Scale Standardizer"},{"location":"transformers/z-scale-standardizer.html#z-scale-standardizer","text":"A method of centering and scaling a dataset such that it has 0 mean and unit variance, also known as a Z-Score. Although Z-Scores are technically unbounded, in practice they mostly fall between -3 and 3 - that is, they are no more than 3 standard deviations away from the mean. \\[ {\\displaystyle z = {x - \\mu \\over \\sigma }} \\] Interfaces: Transformer , Stateful , Elastic , Reversible , Persistable Data Type Compatibility: Continuous","title":"Z Scale Standardizer"},{"location":"transformers/z-scale-standardizer.html#parameters","text":"# Name Default Type Description 1 center true bool Should we center the data at 0?","title":"Parameters"},{"location":"transformers/z-scale-standardizer.html#example","text":"use Rubix\\ML\\Transformers\\ZScaleStandardizer ; $transformer = new ZScaleStandardizer ( true );","title":"Example"},{"location":"transformers/z-scale-standardizer.html#additional-methods","text":"Return the means calculated by fitting the training set: public means () : array Return the variances calculated during fitting: public variances () : array","title":"Additional Methods"},{"location":"transformers/z-scale-standardizer.html#references","text":"T. F. Chan et al. (1979). Updating Formulae and a Pairwise Algorithm for Computing Sample Variances. \u21a9","title":"References"}]}